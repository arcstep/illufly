{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 智谱API测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 ChatZhipuAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic==1.10.13\n",
      "  Using cached pydantic-1.10.13-cp39-cp39-macosx_10_9_x86_64.whl.metadata (149 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-chinese-gSQlHcwW-py3.9/lib/python3.9/site-packages (from pydantic==1.10.13) (4.10.0)\n",
      "Using cached pydantic-1.10.13-cp39-cp39-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.6.2\n",
      "    Uninstalling pydantic-2.6.2:\n",
      "      Successfully uninstalled pydantic-2.6.2\n",
      "Successfully installed pydantic-1.10.13\n"
     ]
    }
   ],
   "source": [
    "# !pip install pydantic==1.10.13\n",
    "# !pip install pydantic==2.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_zhipu import ChatZhipuAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import transformers python package. This is needed in order to calculate get_token_ids. Please install it with `pip install transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages/langchain_core/language_models/base.py:40\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2TokenizerFast  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatZhipuAI()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages/langchain_core/language_models/base.py:283\u001b[0m, in \u001b[0;36mBaseLanguageModel.get_num_tokens\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_num_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the number of tokens present in the text.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    Useful for checking if an input will fit in a model's context window.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m        The integer number of tokens in the text.\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages/langchain_core/language_models/base.py:270\u001b[0m, in \u001b[0;36mBaseLanguageModel.get_token_ids\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_token_ids\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    261\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the ordered ids of the tokens in a text.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m            in the text.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_token_ids_default_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages/langchain_core/language_models/base.py:54\u001b[0m, in \u001b[0;36m_get_token_ids_default_method\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Encode the text into token IDs.\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# get the cached tokenizer\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# tokenize the text using the GPT-2 tokenizer\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mencode(text)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages/langchain_core/language_models/base.py:42\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2TokenizerFast  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is needed in order to calculate get_token_ids. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m     )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# create a GPT-2 tokenizer instance\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GPT2TokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import transformers python package. This is needed in order to calculate get_token_ids. Please install it with `pip install transformers`."
     ]
    }
   ],
   "source": [
    "llm = ChatZhipuAI()\n",
    "llm.get_num_tokens(text=\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import transformers python package. This is needed in order to calculate get_token_ids. Please install it with `pip install transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages/langchain_core/language_models/base.py:40\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2TokenizerFast  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages/langchain_core/language_models/base.py:270\u001b[0m, in \u001b[0;36mBaseLanguageModel.get_token_ids\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_token_ids\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    261\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the ordered ids of the tokens in a text.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m            in the text.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_token_ids_default_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages/langchain_core/language_models/base.py:54\u001b[0m, in \u001b[0;36m_get_token_ids_default_method\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Encode the text into token IDs.\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# get the cached tokenizer\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# tokenize the text using the GPT-2 tokenizer\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mencode(text)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages/langchain_core/language_models/base.py:42\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2TokenizerFast  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is needed in order to calculate get_token_ids. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m     )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# create a GPT-2 tokenizer instance\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GPT2TokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import transformers python package. This is needed in order to calculate get_token_ids. Please install it with `pip install transformers`."
     ]
    }
   ],
   "source": [
    "llm.get_token_ids(text=\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好👋！我是人工智能助手智谱清言（ChatGLM），很高兴见到你，欢迎问我任何问题。')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好👋！我是人工智能助手智谱清言（ChatGLM），很高兴见到你，欢迎问我任何问题。')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await llm.ainvoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello| |👋!| I|'|m| Chat|GL|M|（|智|谱|清|言|）,| the| artificial| intelligence| assistant|,| nice| to| meet| you|.| Feel| free| to| ask| me| any| questions|.||"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"hi\"):\n",
    "  print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello|!| How| can| I| assist| you| today|?||"
     ]
    }
   ],
   "source": [
    "async for chunk in llm.astream(\"hi\"):\n",
    "  print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chat_model_start', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'name': 'ChatZhipuAI', 'tags': [], 'metadata': {}, 'data': {'input': 'hi'}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='你好')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='👋！')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='我是')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='人工智能')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='助手')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='智')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='谱')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='清')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='言')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='（')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='C')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='hat')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='GL')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='M')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='），')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='很高兴')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='见到')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='你')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='，')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='欢迎')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='问我')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='任何')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='问题')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='。')}}\n",
      "{'event': 'on_chat_model_stream', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'name': 'ChatZhipuAI', 'data': {'chunk': AIMessageChunk(content='')}}\n",
      "{'event': 'on_chat_model_end', 'name': 'ChatZhipuAI', 'run_id': '4f202b64-1034-455b-9bc4-5fc32e027565', 'tags': [], 'metadata': {}, 'data': {'output': AIMessageChunk(content='你好👋！我是人工智能助手智谱清言（ChatGLM），很高兴见到你，欢迎问我任何问题。')}}\n"
     ]
    }
   ],
   "source": [
    "async for chunk in llm.astream_events(\"hi\", version=\"v1\"):\n",
    "  print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"query_train_info\",\n",
    "            \"description\": \"根据用户提供的信息，查询对应的车次\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"departure\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"出发城市或车站\",\n",
    "                    },\n",
    "                    \"destination\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"目的地城市或车站\",\n",
    "                    },\n",
    "                    \"date\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"要查询的车次日期\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"departure\", \"destination\", \"date\"],\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatZhipuAI().bind(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatZhipuAI(client=<zhipuai._client.ZhipuAI object at 0x110ca1820>), kwargs={'tools': [{'type': 'function', 'function': {'name': 'query_train_info', 'description': '根据用户提供的信息，查询对应的车次', 'parameters': {'type': 'object', 'properties': {'departure': {'type': 'string', 'description': '出发城市或车站'}, 'destination': {'type': 'string', 'description': '目的地城市或车站'}, 'date': {'type': 'string', 'description': '要查询的车次日期'}}, 'required': ['departure', 'destination', 'date']}}}]})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='一只羊说：“我每天都比一只狼跑得快，因为我比它更会绕圈子。”')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"给我讲个一句话笑话\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_8415022186389879829', 'function': {'arguments': '{\"date\":\"2024-01-01\",\"departure\":\"北京南站\",\"destination\":\"上海\"}', 'name': 'query_train_info'}, 'type': 'function'}]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"你能帮我查询2024年1月1日从北京南站到上海的火车票吗？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_8367745969533447732', 'function': {'arguments': '{\"date\":\"2024-01-01\",\"departure\":\"北京南站\",\"destination\":\"上海\"}', 'name': 'query_train_info'}, 'type': 'function'}]}\n",
      "content=''\n"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"你能帮我查询2024年1月1日从北京南站到上海的火车票吗？\"):\n",
    "  print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='根据提供的信息，贾玲的最新作品是《热辣滚烫》，这部电影在春节档期上映，并且取得了相当的成功，票房超过23.6亿元，豆瓣评分达到了8.0分，是当时春节档期评分最高的电影。贾玲不仅是该片的导演，也通过该片获得了观众和市场的广泛认可。\\n\\n此外，贾玲还透露了她的下一部作品的一些信息。她的新片名为《转念花开》（又名《光明之战》），是一部关于反传销题材的电影。这部电影已经备案，并且贾玲表示已经拍摄了一部分，剩下的一部分还在拍摄计划中。贾玲还提到，她已经邀请杨紫参与新片的演出，这增加了观众对这部电影的期待。')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatZhipuAI()\n",
    "llm.invoke(\"贾玲最新的作品是什么？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='贾玲是中国内地的著名女演员、喜剧演员和制片人，她有许多知名的作品。其中，贾玲最著名的作品之一是《欢乐喜剧人》。这是一档喜剧竞技类节目，贾玲作为固定成员和队长，带领团队在节目中表演了许多精彩的喜剧作品，深受观众喜爱。通过这个节目，贾玲展示了自己出色的喜剧表演才华，赢得了广泛的赞誉和人气。')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"web_search\",\n",
    "        \"web_search\": {\n",
    "            \"search_query\": \"贾玲演过的相声\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "llm = ChatZhipuAI().bind(tools=tools)\n",
    "llm.invoke(\"列举一个贾玲的作品\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='很抱歉，您没有提供具体的背景信息或者说明，所以我无法直接指出贾玲的作品。贾玲是中国的一位著名女喜剧演员、导演和制片人。如果您能提供更多的上下文或细节，比如关于某个特定节目、事件或者作品的信息，我将能够更好地帮助您找出贾玲的相关作品。例如，贾玲因参与《欢乐喜剧人》等节目以及导演和参与电影《你好，李焕英》而广受欢迎。提供更多细节后，我可以为您提供更精确的答案。')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"web_search\",\n",
    "        \"web_search\": {\n",
    "            \"search_query\": \"贾玲演过的电影\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "llm = ChatZhipuAI().bind(tools=tools)\n",
    "llm.invoke(\"从背景信息中找出贾玲的作品\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-chinese-gSQlHcwW-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
