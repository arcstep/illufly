{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be809bfa-e1ae-48c1-9fc3-d4fad2832c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec74ac9f-503b-42d6-9431-8c144142cc38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## MemInsight: Autonomous Memory Augmentation for LLM Agents\n",
      "\n",
      "Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, Yassine Benajiba AWS AI\n",
      "\n",
      "{ranasal, cjinglun, miyuan, ancurrey, sunkaral, yizhngn, benajiy} @amazon.com\n",
      "\n",
      "## Abstract\n",
      "\n",
      "Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLMREDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.\n",
      "\n",
      "actions (Zhang et al., 2024). However, the advantages of an LLM agent's memory also introduce notable challenges (Wang et al., 2024b). (1) As data accumulates over time, retrieving relevant information becomes increasingly challenging, especially during extended interactions or complex tasks. (2) Processing large historical data, which can grow rapidly, as interactions accumulate, requires effective memory management strategies. (3) Storing data in its raw format can hinder efficient retrieval of pertinent knowledge, as distinguishing between relevant and irrelevant details becomes more challenging, potentially leading to noisy or imprecise information that compromises the agent's performance. Furthermore, (4) the integration of knowledge across tasks is constrained, limiting the agent's ability to effectively utilize data from diverse contexts. Consequently, effective knowledge representation and structuring of LLM agent memory are essential to accumulate relevant information and enhance understanding of past events. Improved memory management enables better retrieval and contextual awareness, making this a critical and evolving area of research.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "LLM agents have emerged as an advanced framework to extend the capabilities of LLMs to improve reasoning (Yao et al., 2023; Wang et al., 2024c), adaptability (Wang et al., 2024d), and selfevolution (Zhao et al., 2024a; Wang et al., 2024e; Tang et al., 2025). A key component of these agents is their memory module, which retains past interactions to allow more coherent, consistent, and personalized responses across various tasks. The memory of the LLM agent is designed to emulate human cognitive processes by simulating how knowledge is accumulated and historical experiences are leveraged to facilitate complex reasoning and the retrieval of relevant information to inform\n",
      "\n",
      "Hence, in this paper we introduce an autonomous memory augmentation approach, MemInsight, which empowers LLM agents to identify critical information within the data and proactively propose effective attributes for memory enhancements. This is analogous to the human processes of attentional control and cognitive updating, which involve selectively prioritizing relevant information, filtering out distractions, and continuously refreshing the mental workspace with new and pertinent data (Hu et al., 2024; Hou et al., 2024).\n",
      "\n",
      "MemInsight autonomously generates augmentations that encode both relevant semantic and contextual information for memory. These augmentations facilitate the identification of memory components pertinent to various tasks. Accordingly, MemInsight can improve memory re- trieval by leveraging relevant attributes of memory, thereby supporting autonomous LLM agent adaptability and self-evolution.\n",
      "\n",
      "Our contributions can be summarized as follows:\n",
      "\n",
      "- · We propose a structured autonomous approach that adapts LLM agents' memory representations while preserving context across extended conversations for various tasks.\n",
      "- · We design and apply memory retrieval methods that leverage the generated memory augmentations to filter out irrelevant memory while retaining key historical insights.\n",
      "- · Our promising empirical findings demonstrate the effectiveness of MemInsight on several tasks: conversational recommendation, question answering, and event summarization.\n",
      "\n",
      "## 2 Related Work\n",
      "\n",
      "Well-organized and semantically rich memory structures enable efficient storage and retrieval of information, allowing LLM agents to maintain contextual coherence and provide relevant responses. Developing an effective memory module in LLM agents typically involves two critical components: structural memory generation and memory retrieval methods (Zhang et al., 2024; Wang et al., 2024a).\n",
      "\n",
      "LLM Agents Memory Recent research in LLM agents memory focuses on developing methods for effectively storing previous interactions and feedback (Packer et al., 2024). Contemporary approaches emphasize memory structures that enhance the adaptability of agents and improve their ability to generalize to previously unseen environments (Zhao et al., 2024a; Zhang et al., 2024; Zhu et al., 2023). Common memory forms include summaries and abstract high-level information from raw observations to capture key points and reduce information redundancy (Maharana et al., 2024). Other approaches include structuring memory as summaries, temporal events, or reasoning chains (Zhao et al., 2024a; Zhang et al., 2024; Zhu et al., 2023; Maharana et al., 2024; Anokhin et al., 2024; Liu et al., 2023a). In addition, there are studies that enrich raw conversations with semantic representations like sequence of events and historical event summaries (Zhong et al., 2023; Maharana et al., 2024) or extract reusable workflows from canonical examples and integrate them into memory to assist test-time inference (Wang et al., 2024f). However, all aforementioned studies rely on either unstructured memory or human-designed attributes for memory representation, while MemInsight leverages the AI agent's autonomy to discover the ideal attributes for structured representation.\n",
      "\n",
      "LLMAgents Memory Retrieval Existing works have leveraged memory retrieval techniques for efficiency when tackling vast amounts of historical context (Hu et al., 2023a; Zhao et al., 2024b; Tack et al., 2024; Ge et al., 2025). Common approaches for memory retrieval include generative retrieval models, which encode memory as dense vectors and retrieve the topk relevant documents based on similarity search techniques (Zhong et al., 2023; Penha et al., 2024). Various similarity metrics, such as cosine similarity (Packer et al., 2024), are employed, alongside advanced techniques like dual-tower dense retrieval models, which encode each memory history into embeddings indexed by FAISS (Johnson et al., 2017) to enhance retrieval efficiency (Zhong et al., 2023). Additionally, methods such as Locality-Sensitive Hashing (LSH) are utilized to retrieve tuples containing related entries in memory (Hu et al., 2023b).\n",
      "\n",
      "## 3 Autonomous Memory Augmentation\n",
      "\n",
      "Our proposed MemInsight model is designed to enhance memory representation through a structured augmentation process that optimizes memory retrieval. Figure 1 presents an overview of the model, highlighting its main modules: attribute mining, annotation, and memory retriever.\n",
      "\n",
      "## 3.1 Attribute Mining and Annotation\n",
      "\n",
      "To ensure the effectiveness of these attributes in future interactions, they must be meaningful, accurate, and Attribute mining in our MemInsight model leverages a backbone LLM to autonomously identify and define key attributes that encapsulate semantic knowledge from user interactions. This entails selecting attributes most relevant to the task under consideration and employs them to annotate historical conversations. Effective attributes must be meaningful, accurate, and contextually relevant to enhance future interactions. To achieve this, the augmentation process follows a structured approach, defining the perspective from which the attributes are derived, determining the appropriate level of augmentation granularity, and establishing\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "MemInsight Augmentation\n",
      "\n",
      "Test-time Inference\n",
      "\n",
      "Figure 1: Main modules of MemInsight including, Attribute Mining, Memory Retrieval, and Annotation, triggered by different memory processes: Augment, Write, and Retrieve. In addition to the test-time inference evaluation downstream tasks, memory augmentation and adopted memory retrieval methods.\n",
      "\n",
      "a coherent sequence for annotation. The resulting attributes and values are then used to enrich memory, ensuring a well-organized and informative representation of past interactions.\n",
      "\n",
      "## 3.1.1 Attribute Perspective\n",
      "\n",
      "Attribute generation is guided by two primary orientations: entity-centric and conversation-centric. Entity-centric emphasizes a specific item stored in memory such as movies or books. Attributes generated for entity-centric augmentations should capture the main characteristics and features of this entity. For example, attributes for a movie entity might include the director, actors, and year of release, while attributes for a book entity would encompass the author, publisher, and number of pages. On the other hand, conversation-centric augmentations focus on annotating and characterizing the entire user interaction from the user's perspective. This approach ensures that the extracted attributes align with the user's intent, preferences, sentiment, emotions, motivations, and choices, thereby improving personalized responses and memory retrieval. An illustrative example is provided in Figure 4.\n",
      "\n",
      "## 3.1.2 Attribute Granularity\n",
      "\n",
      "While entity-centric augmentations focus on specific entities in memory, conversation-centric augmentations introduce an additional factor: attribute granularity, which determines the level of details captured in the augmentation process. The augmentation attributes can be analyzed at varying levels of abstraction, either at the level of individual turns within a user conversation (turn-level), or across the entire dialogue session (session-level), each offering distinct insights into the conversational context. At the turn level, each dialogue turn is indepen-\n",
      "\n",
      "Figure 2: An example for Turn level and Session level annotations for a sample dialogue conversation from the LoCoMo Dataset.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "dently augmented, focusing on the specific content of individual turns to generate more nuanced and contextual attributes. In contrast, session-level annotation considers the entire dialogue, generating generalized attributes that capture the broader conversational context. Due to its broader granularity, session-level augmentation emphasizes high-level attributes and conversational structures rather than the detailed features of individual turns. An example of both levels is illustrated in Figure 2 for a sample dialogue turns. As shown, turn-level annotations offer finer-grained details, while sessionlevel annotations provide a broader overview of the dialogue.\n",
      "\n",
      "## 3.1.3 Annotation and Attribute Prioritization\n",
      "\n",
      "Subsequently, the generated attributes and their corresponding values are used to annotate the agent's memory. Annotation is done by aggregating attributes and values in the relevant memory in the form:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where m i stands for relevant memory and a , v i i denote attributes and values respectively. The relevant memory may correspond to turn or session level. Attributes are typically aggregated using the Attribute Prioritization method, which can be classified into Basic and Priority. In Basic Augmentation, attributes are aggregated without a predefined order, resulting in an arbitrary sequence i 1 , .., i n . In contrast, Priority Augmentation sorts attribute-value pairs according to their relevance to the memory being augmented. This prioritization follows a structured order in which attribute i 1 holds the highest significance, ensuring that more relevant attributes are processed first.\n",
      "\n",
      "## 3.2 Memory Retrieval\n",
      "\n",
      "MemInsight augmentations are employed to enrich or retrieve relevant memory. For comprehensive retrieval, memory is retrieved along with all associated augmentations to generate a more contextaware response. Additionally, MemInsight can refine the retrieval process. Initially, the current context is augmented to identify task-specific and interaction-related attributes, which then guide the retrieval of the pertinent memory. Two primary retrieval methods are proposed: (1) Attribute-based Retrieval , leverages the current context to generate attributes tailored to the specific task at hand. These attributes serve as criteria for selecting and retrieving relevant memory that shares similar attributes in their augmentations. The retrieved memories, which align with the required attributes, are subsequently integrated into the current context to enrich the ongoing interaction. (2) Embedding-based Retrieval , utilizes memory augmentations to create a unique embedding representation for each memory instance, derived from its aggregated annotations. Simultaneously, the augmentations of the current context are embedded to form a query vector, which is then used in a similarity-based search to retrieve the topk most relevant memories. Finally, all retrieved memory are incorporated into the current context to enhance the relevance and coherence of the ongoing interaction. A detailed description of this method can be found in Appendix C.\n",
      "\n",
      "## 4 Evaluation\n",
      "\n",
      "## 4.1 Datasets\n",
      "\n",
      "We conduct a series of experiments on the datasets: LLM-REDIAL (Liang et al., 2024) and LoCoMo (Maharana et al., 2024). LLM-REDIAL\n",
      "\n",
      "is a dataset for evaluating movie Conversational Recommendation, containing approximately 10K dialogues covering 11K movies in memory. While LoCoMo is a dataset for evaluating Question Answering and Event Summarization, consisting of 30 long-term dialogues across up to 10 sessions between two speakers. LoCoMo includes five question categories: Single-hop, Multi-hop, Temporal reasoning, Open-domain knowledge, and Adversarial questions. Each question has a reference label that specifies the relevant dialogue turn in memory required to generate the answer. Additionally, LoCoMo provides event labels for each speaker in a session, which we use as ground truth for Event Summarization evaluation.\n",
      "\n",
      "## 4.2 Experimental Setup\n",
      "\n",
      "To evaluate our model, we begin by augmenting the datasets using a backbone LLM with zero-shot prompting to identify relevant attributes and their corresponding values. For augmentation generation and evaluation across various tasks, we utilize the following models for attribute generation: Claude Sonnet, 1 Llama 2 and Mistral. 3 For the Event Summarization task, we also use the Claude3-Haiku model. 4 For embedding-based retrieval tasks, we employ the Titan Text Embedding model 5 to generate embeddings. The augmented memory is then embedded and indexed using FAISS (Johnson et al., 2017) for vector indexing and search. To ensure consistency across all experiments, we use the same base model for the primary tasks: recommendation, answer generation, and summarization, while evaluating different models for augmentation. Claude Sonnet serves as the backbone LLM in baselines for all tasks.\n",
      "\n",
      "## 4.3 Evaluation Metrics\n",
      "\n",
      "The evaluation metrics used for assessing different tasks using MemInsight include, traditional metrics like F1-score metric for answer prediction and recall for accuracy in Question Answering. Recall@K and NDCG@K for Conversational Recommendation, along with LLM-based metrics for genre matching.\n",
      "\n",
      "We also evaluate using subjective metrics including Persuasiveness, used in Liang et al. (2024), to\n",
      "\n",
      "1 claude-3-sonnet-20240229-v1\n",
      "\n",
      "2 llama3-70b-instruct-v1\n",
      "\n",
      "3 mistral-7b-instruct-v0\n",
      "\n",
      "4 claude-3-Haiku-20240307-v1\n",
      "\n",
      "5 titan-embed-text-v2:0\n",
      "\n",
      "assess how persuasive the recommendations are relative to the ground truth. Additionally, we introduce a Relatedness metric, where we prompt an LLMto measure how comparable the recommendation attributes are to the ground truth, categorizing them as not comparable, comparable, or highly comparable. Finally, we assess Event Summarization using an LLM-based metric, G-Eval (Liu et al., 2023b), a summarization evaluation metric that measures the relevance, consistency, and coherence of generated summaries as opposed to reference labels. These metrics provide a comprehensive framework for evaluating both retrieval effectiveness and response quality.\n",
      "\n",
      "## 5 Experiments\n",
      "\n",
      "## 5.1 Questioning Answering\n",
      "\n",
      "Questioning Answering task experiments are conducted to evaluate the effectiveness of MemInsight in answer generation. We assess overall accuracy to measure the system's ability to retrieve and incorporate relevant information from augmentations. The base model, which incorporates all historical dialogues without augmentation, serves as the baseline. We additionally consider Dense Passage Retrieval (DPR) RAG model (Karpukhin et al., 2020) as a comparative baseline due to its speed and scalability.\n",
      "\n",
      "Memory Augmentation In this task, memory is constructed from historical conversational dialogues, which requires the generation of conversation-centric attributes for augmentation. Given that the ground truth labels consist of dialogue turns relevant to the question, the dialogues are annotated at the turn level. A backbone LLM is prompted to generate augmentation attributes for both conversation-centric and turn-level annotations.\n",
      "\n",
      "Memory Retrieval To answer a given question, the relevant dialogue turn must be retrieved from historical dialogues. In order to retrieve the relevant dialogue turn, the question is first augmented to identify relevant attributes and a memory retrieval method is applied. We evaluate different MemInsight memory retrieval methods to demonstrate the efficacy of our model. We employ attribute-based retrieval by selecting dialogue turns augmented with attributes that exactly match the question's attributes. Additionally, we evaluate the embedding-based retrieval, where the augmen- tations are embedded and indexed for retrieval. Hence, the question and attributes are transformed into an embedded query, which is used to perform a vector similarity search to retrieve the topk most similar dialogue turns. Once the relevant memory is retrieved, it is integrated into the current context to generate the final answer.\n",
      "\n",
      "Experimental Results We initiate our evaluation by assessing attribute-based memory retrieval using the Claude-3-Sonnet model. Table 1 presents the overall F1 score, measuring the accuracy of the generated answers. As shown in the table, attribute-based retrieval outperforms the baseline model by 3% in overall accuracy, with notable improvements in single-hop, temporal reasoning, and adversarial questions, which require advanced contextual understanding and reasoning. These results indicate that the augmented history enriched the context, leading to better reasoning and a significant increase in the F1 score for answer generation. Additionally, we perform a detailed analysis of embedding-based retrieval, where we consider evaluating basic and priority augmentation using the Claude-3-Sonnet model.\n",
      "\n",
      "Table 1 demonstrates that the priority augmentation consistently outperforms the basic model across all questions. This finding suggests that the priority relevance of augmentations enhances context representation for conversational data. Subsequently, we evaluate the priority augmentations using Llama, and Mistral models for Embeddingbased retrieval. As shown in the table, the Embedding-based retrieval outperforms the RAG baseline across all question categories, except for adversarial questions, yet the overall accuracy of MemInsight remains superior. Additionally, MemInsight demonstrates a significant improvement in performance on multi-hop questions, which require reasoning over multiple pieces of supporting evidence. This suggests that the generated augmentations provided a more robust understanding and a broader perspective of the historical dialogues. RECALL metrics in Table 2 revealed a more significant boost, with priority augmentations increasing accuracy across all categories and yielding a 35% overall improvement.\n",
      "\n",
      "## 5.2 Conversational Recommendation\n",
      "\n",
      "We simulate conversational recommendation by preparing dialogues for evaluation under the same conditions proposed by Liang et al. (2024). This\n",
      "\n",
      "| Model                                                  |   Single-hop |   Multi-hop |   Temporal |   Open-domain |   Adversarial |   Overall |\n",
      "|--------------------------------------------------------|--------------|-------------|------------|---------------|---------------|-----------|\n",
      "| Baseline (Claud-3-Sonnet)                              |         15   |        10   |        3.3 |          26   |          45.3 |      26.1 |\n",
      "| Attribute-based Retrieval MemInsight (Claude-3-Sonnet) |         18   |        10.3 |        7.5 |          27   |          58.3 |      29.1 |\n",
      "| Embedding-Based Retrieval RAG Baseline (DPR)           |         11.9 |         9   |        6.3 |          12   |          89.9 |      28.7 |\n",
      "| MemInsight (Llama v3 Priority )                        |         14.3 |        13.4 |        6   |          15.8 |          82.7 |      29.7 |\n",
      "| MemInsight (Mistral v1 Priority )                      |         16.1 |        14.1 |        6.1 |          16.7 |          81.2 |      30   |\n",
      "| MemInsight (Claude-3-Sonnet Basic )                    |         14.7 |        13.8 |        5.8 |          15.6 |          82.1 |      29.6 |\n",
      "| MemInsight (Claude-3-Sonnet Priority )                 |         15.8 |        15.8 |        6.7 |          19.1 |          75.3 |      30.1 |\n",
      "\n",
      "Table 1: Results for F1 Score (%) for answer generation accuracy for attribute-based and embedding-based memory retrieval methods. Baseline is Claude-3-Sonnet model to generate answers using all memory without augmentation, for Attribute-based retrieval. In addition to the Dense Passage Retrieval(DPR) for Embedding-based retrieval. Evaluation is done with k = 5 . Best results per question category over all methods are in bold.\n",
      "\n",
      "Table 2: Results for the RECALL@k=5 accuracy for Embedding-based retrieval for answer generation using LoCoMo dataset. Dense Passage Retrieval(DPR) RAG model is the baseline. Best results are in bold.\n",
      "\n",
      "| Model                                  |   Single-hop |   Multi-hop |   Temporal |   Open-domain |   Adversarial |   Overall |\n",
      "|----------------------------------------|--------------|-------------|------------|---------------|---------------|-----------|\n",
      "| RAG Baseline (DPR)                     |         15.7 |        31.4 |       15.4 |          15.4 |          34.9 |      26.5 |\n",
      "| MemInsight (Llama v3 Priority )        |         31.3 |        63.6 |       23.8 |          53.4 |          28.7 |      44.9 |\n",
      "| MemInsight (Mistral v1 Priority )      |         31.4 |        63.9 |       26.9 |          58.1 |          36.7 |      48.9 |\n",
      "| MemInsight (Claude-3-Sonnet Basic )    |         33.2 |        67.1 |       29.5 |          56.2 |          35.7 |      48.8 |\n",
      "| MemInsight (Claude-3-Sonnet Priority ) |         39.7 |        75.1 |       32.6 |          70.9 |          49.7 |      60.5 |\n",
      "\n",
      "Table 3: Statistics of attributes generated for the LLMREDIAL Movie dataset, which include total number of movies, average number of attributes per item, number of failed attributes, and the counts for the most frequent five attributes.\n",
      "\n",
      "| Statistic         |              | Count   |\n",
      "|-------------------|--------------|---------|\n",
      "| Total Movies      |              | 9687    |\n",
      "| Avg. Attributes   |              | 7.39    |\n",
      "| Failed Attributes |              | 0.10%   |\n",
      "| Top-5 Attributes  | Genre        | 9662    |\n",
      "| Top-5 Attributes  | Release year | 5998    |\n",
      "|                   | Director     | 5917    |\n",
      "|                   | Setting      | 4302    |\n",
      "|                   | Characters   | 3603    |\n",
      "\n",
      "Evaluation includes direct matches between recommended and ground truth movie titles using RECALL@[1,5,10] and NDCG@[1,5,10]. Furthermore, to address inconsistencies in movie titles generated by LLMs, we incorporate an LLM-based evaluation that assesses recommendations based on genre similarity. Specifically, a recommended movie is considered a valid match if it shares the same genre as the corresponding ground truth label.\n",
      "\n",
      "process involves masking the dialogue and randomly selecting n = 200 conversations for evaluation to ensure a fair comparison. Each conversational dialogue used is processed by masking the ground truth labels, followed by a turn cut-off, where all dialogue turns following the first masked turn are removed and retained as evaluation labels. Subsequently, the dialogues are augmented using a conversation-centric approach to identify relevant user interest attributes for retrieval. Finally, we prompt the LLM model to generate a movie recommendation that best aligns with the masked token, guided by the augmented movies retrieved based on the user's historical interactions.\n",
      "\n",
      "The baseline for this evaluation is the results presented in the LLM-REDIAL paper (Liang et al., 2024) which employs zero-shot prompting for recommendation using the ChatGPT model 6 . In addition to the baseline model that uses memory without augmentation.\n",
      "\n",
      "6 https://openai.com/blog/chatgpt\n",
      "\n",
      "Memory Augmentation We initially augment the dataset with relevant attributes, primarily employing entity-centric augmentations for memory annotation, as the memory consists of movies. In this context, we conduct a detailed evaluation of the generated attributes to provide an initial assessment of the effectiveness and relevance of MemInsight augmentations. To evaluate the quality of the generated attributes, Table 3 presents statistical data on the generated attributes, including the five most frequently occurring attributes across the entire dataset. As shown in the table, the generated attributes are generally relevant, with \"genre\" being the most significant attribute based on its cumulative frequency across all movies (also shown in Figure 5). However, the relevance of attributes vary, emphasizing the need for prioritization in augmentation. Additionally, the table reveals that augmentation was unsuccessful for 0.1% of the movies, primarily due to the LLM's inability to recognize certain movie titles or because the presence of some words in the movie titles conflicted with the LLM's policy.\n",
      "\n",
      "Table 4: Results for Movie Conversational Recommendation using (1) Attribute-based retrieval with Claude-3-Sonnet model (2) Embedding-based retrieval across models (Llama v3, Mistral v1, Claude-3-Haiku, and Claude-3-Sonnet) (3) Comprehensive setting using Claude-3-Sonnet that includes ALL augmentations. Evaluation metrics include RECALL, NDCG, and an LLMbased genre matching metric, with n = 200 and k = 10 . Baseline is Claude-3-Sonnet without augmentation. Best results are in bold.\n",
      "\n",
      "| Model                                                  | Avg. Items Retrieved   | Direct Match ( ↑ )   | Direct Match ( ↑ )   | Direct Match ( ↑ )   | Genre Match ( ↑ )   | Genre Match ( ↑ )   | Genre Match ( ↑ )   | NDCG ( ↑ )   | NDCG ( ↑ )   | NDCG ( ↑ )   |\n",
      "|--------------------------------------------------------|------------------------|----------------------|----------------------|----------------------|---------------------|---------------------|---------------------|--------------|--------------|--------------|\n",
      "|                                                        |                        | R@1                  | R@5                  | R@10                 | R@1                 | R@5                 | R@10                | N@1          | N@5          | N@10         |\n",
      "| Baseline (Claude-3-Sonnet) LLM-REDIAL Model            | 144 144                | 0.000 -              | 0.010 0.000          | 0.015 0.005          | 0.320 -             | 0.57 -              | 0.660 -             | 0.005 -      | 0.007 0.000  | 0.008 0.001  |\n",
      "| Attribute-Based Retrieval MemInsight (Claude-3-Sonnet) | 15                     | 0.005                | 0.015                | 0.015                | 0.270               | 0.540               | 0.640               | 0.005        | 0.007        | 0.007        |\n",
      "| Embedding-Based Retrieval MemInsight (Llama v3)        | 10                     | 0.000                | 0.005                | 0.028                | 0.380               | 0.580               | 0.670               | 0.000        | 0.002        | 0.001        |\n",
      "| MemInsight (Mistral v1)                                | 10                     | 0.005                | 0.010                | 0.010                | 0.380               | 0.550               | 0.630               | 0.005        | 0.007        | 0.007        |\n",
      "| MemInsight (Claude-3-Haiku)                            | 10                     | 0.005                | 0.010                | 0.010                | 0.360               | 0.610               | 0.650               | 0.005        | 0.007        | 0.007        |\n",
      "| MemInsight (Claude-3-Sonnet)                           | 10                     | 0.005                | 0.015                | 0.015                | 0.400               | 0.600               | 0.64                | 0.005        | 0.010        | 0.010        |\n",
      "| Comprehensive MemInsight (Claude-3-Sonnet)             | 144                    | 0.010                | 0.020                | 0.025                | 0.300               | 0.590               | 0.690               | 0.010        | 0.015        | 0.017        |\n",
      "\n",
      "Memory Retrieval For this task we evaluate attribute-based retrieval using the Claude-3-Sonnet model with both filtered and comprehensive settings. Additionally, we examine embedding-based retrieval using all other models. For embeddingbased retrieval, we set k = 10 , meaning that 10 memory instances are retrieved (as opposed to 144 in the baseline).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Experimental Results Table 4 shows the results for conversational recommendation evaluating comprehensive setting, attribute-based retrieval and embedding-based retrieval. As shown in the table, comprehensive memory augmentation tends to outperform the baseline and LLM-REDIAL model for recall and NDCG metrics. For genre match we find the results to be comparable when considering all attributes. However, attributed-based filtering retrieval still outperforms the LLM-REDIAL model and is comparable to the baseline with almost 90% less memory retrieved.\n",
      "\n",
      "Table 5 presents the results of subjective LLMbased evaluation for Persuasiveness and Relatedness. The findings indicate that memory augmentation enhances partial persuasiveness by 10-11% using both comprehensive and attribute-based retrieval, while also reducing unpersuasive recommendations and increasing highly persuasive ones by 4% in attribute-based retrieval. Furthermore, the results highlights the effectiveness of embeddingbased retrieval, which leads to a 12% increase in highly persuasive recommendations and enhances all relatedness metrics. This illustrates how MemInsight enriches the recommendation process by incorporating condensed, relevant knowledge, thereby producing more persuasive and related recommendations. However, these improvements\n",
      "\n",
      "Figure 3: Evaluation framework for event summarization with MemInsight, exploring augmentation at Turn and Session levels, considering attributes alone or both attributes and dialogues for richer summaries.\n",
      "\n",
      "were not reflected in recall and NDCG metrics.\n",
      "\n",
      "## 5.3 Event Summarization\n",
      "\n",
      "We evaluate the effectiveness of MemInsight in enriching raw dialogues with relevant insights for event summarization. We utilize the generated annotations to identify key events within conversations and hence use them for event summarization. We compare the generated summaries against LoCoMo's event labels as the baseline. Figure 3 illustrates the experimental framework, where the baseline is the raw dialogues sent to the LLM model to generate an event summary, then both event summaries, from raw dialogues and augmentation based summaries, are compared to the ground truth summaries in the LoCoMo dataset.\n",
      "\n",
      "Memory Augmentation In this experiment, we evaluate the effectiveness of augmentation granularity; turn-level dialogue augmentations as opposed to session-level dialogue annotations. We additionally, consider studying the effectiveness of using only the augmentations to generate the event summaries as opposed to using both the augmentations and their corresponding dialogue content.\n",
      "\n",
      "Table 5: Movie Recommendations results (with similar settings to Table 4) using LLM-based metrics; (1) Persuasiveness- % of Unpersuasive (lower is better), Partially, and Highly Persuasive cases. (2) Relatedness- % of Not Comparable (lower is better), Comparable, and Exactly Matching cases. Best results are in bold. Comprehensive setting includes ALL augmentations. Totals may NOT sum to 100% due to cases the LLM model could not evaluate.\n",
      "\n",
      "| Model                                                  | Avg. Items Retrieved   | LLM-Persuasiveness %   | LLM-Persuasiveness %   | LLM-Persuasiveness %   | LLM-Relatedness %   | LLM-Relatedness %   | LLM-Relatedness %   |\n",
      "|--------------------------------------------------------|------------------------|------------------------|------------------------|------------------------|---------------------|---------------------|---------------------|\n",
      "|                                                        |                        | Unpers*                | Partially Pers.        | Highly Pers.           | Not Comp*           | Comp                | Match               |\n",
      "| Baseline (Claude-3-Sonnet)                             | 144                    | 16.0                   | 64.0                   | 13.0                   | 57.0                | 41.0                | 2.0                 |\n",
      "| Attribute-Based Retrieval MemInsight (Claude-3-Sonnet) | 15                     | 2.0                    | 75.0                   | 17.0                   | 40.5                | 54.0                | 2.0                 |\n",
      "| Embedding-Based Retrieval                              |                        |                        |                        |                        |                     |                     |                     |\n",
      "| MemInsight (Llama v3)                                  | 10                     | 11.3                   | 63.0                   | 20.4                   | 19.3                | 80.1                | 0.5                 |\n",
      "| MemInsight (Mistral v1)                                | 10                     | 16.3                   | 61.2                   | 18.0                   | 16.3                | 82.5                | 5.0                 |\n",
      "| MemInsight (Claude-3-Haiku)                            | 10                     | 1.6                    | 53.0                   | 25.0                   | 23.3                | 74.4                | 2.2                 |\n",
      "| MemInsight (Claude-3-Sonnet)                           | 10                     | 2.0                    | 59.5                   | 20.0                   | 29.5                | 68.0                | 2.5                 |\n",
      "| Comprehensive MemInsight (Claude-3-Sonnet)             | 144                    | 2.0                    | 74.0                   | 12.0                   | 42.5                | 56.0                | 1.0                 |\n",
      "\n",
      "Table 6: Event Summarization results using G-Eval metrics (higher is better): Relevance, Coherence, and Consistency. Comparing summaries generated with augmentations only at Turn-Level (TL) and Session-Level (SL) and summaries generated using both augmentations and dialogues (MemInsight +Dialogues) at TL and SL. Best results are in bold.\n",
      "\n",
      "| Model                      | Claude-3-Sonnet   | Claude-3-Sonnet   | Claude-3-Sonnet   | Llama v3   | Llama v3   | Llama v3   | Mistral v1   | Mistral v1   | Mistral v1   | Claude-3-Haiku   | Claude-3-Haiku   | Claude-3-Haiku   |\n",
      "|----------------------------|-------------------|-------------------|-------------------|------------|------------|------------|--------------|--------------|--------------|------------------|------------------|------------------|\n",
      "|                            | Rel.              | Coh.              | Con.              | Rel.       | Coh.       | Con.       | Rel.         | Coh.         | Con.         | Rel.             | Coh.             | Con.             |\n",
      "| Baseline Summary           | 3.27              | 3.52              | 2.86              | 2.03       | 2.64       | 2.68       | 3.39         | 3.71         | 4.10         | 4.00             | 4.4              | 3.83             |\n",
      "| MemInsight (TL)            | 3.08              | 3.33              | 2.76              | 1.57       | 2.17       | 1.95       | 2.54         | 2.53         | 2.49         | 3.93             | 4.3              | 3.59             |\n",
      "| MemInsight (SL)            | 3.08              | 3.39              | 2.68              | 2.0        | 2.62       | 3.67       | 4.13         | 4.41         | 4.29         | 3.96             | 4.30             | 3.77             |\n",
      "| MemInsight +Dialogues (TL) | 3.29              | 3.46              | 2.92              | 2.45       | 2.19       | 2.87       | 4.30         | 4.53         | 4.60         | 4.23             | 4.52             | 4.16             |\n",
      "| MemInsight +Dialogues (SL) | 3.05              | 3.41              | 2.69              | 2.24       | 2.80       | 3.86       | 4.04         | 4.48         | 4.33         | 3.93             | 4.33             | 3.73             |\n",
      "\n",
      "Experimental Results As shown in Table 6, our MemInsight model achieves performance comparable to the baseline, despite relying only on dialogue turns or sessions containing the event label. Notably, turn-level augmentations provided more precise and detailed event information, leading to improved performance over both the baseline and session-level annotations.\n",
      "\n",
      "For Claude-3-Sonnet, all metrics remain comparable, indicating that memory augmentations effectively capture the semantics and knowledge within dialogues at both the turn and session levels. This proves that the augmentations sufficiently enhance context representation for generating event summaries.\n",
      "\n",
      "To further investigate how backbone LLMs impact augmentation quality, we employed Claude-3Sonnet as opposed to Llama v3 for augmentation while still using Llama for event summarization. As presented in Table 7, Sonnet augmentations resulted in improved performance for all metrics, providing empirical evidence for the effectiveness and stability of Sonnet in augmentation.\n",
      "\n",
      "## 6 Conclusion\n",
      "\n",
      "This paper introduced MemInsight, an autonomous memory augmentation method that enhances LLM agents memory through attribute-based annotations. While maintaining comparable performance\n",
      "\n",
      "Table 7: Results for Event Summarization using Llama v3, where the baseline is the model without augmentation as opposed to the augmentation model (turn-level) using Claude3-Sonnet vs Llama v3.\n",
      "\n",
      "| Model                      | G-Eval %( ↑ )   | G-Eval %( ↑ )   | G-Eval %( ↑ )   |\n",
      "|----------------------------|-----------------|-----------------|-----------------|\n",
      "|                            | Rel.            | Coh.            | Con.            |\n",
      "| Baseline(Llama v3 )        | 2.03            | 2.64            | 2.68            |\n",
      "| Llama v3 + Llama v3        | 2.45            | 2.19            | 2.87            |\n",
      "| Claude-3-Sonnet + Llama v3 | 3.15            | 3.59            | 3.17            |\n",
      "\n",
      "on standard metrics, MemInsight significantly improves LLM-based evaluation scores, highlighting its effectiveness in capturing semantics and boosting performance across tasks and datasets. Additionally, attribute-based filtering and embedding retrieval methods showed promising methods of utilizing the generated augmentations to improve the performance of various tasks. Priority augmentation enhancing similarity searches and retrieval. MemInsight also could be a complement to RAG models for customized retrievals, integrating LLM knowledge. Results confirm that attribute-based retrieval effectively enriches recommendation tasks, leading to more persuasive recommendations.\n",
      "\n",
      "## 7 Limitations\n",
      "\n",
      "While the proposed MemInsight model demonstrates significant potential in enhancing retrieval and contextual understanding, certain limitations must be acknowledged. MemInsight relies on the quality and granularity of annotations generated using LLMs, making it susceptible to issues such as hallucinations inherent to LLM outputs. Furthermore, although the current evaluation metrics provide valuable insights, they may not comprehensively capture all aspects of retrieval and generation quality, highlighting the need for the development of more robust and multidimensional evaluation frameworks.\n",
      "\n",
      "## References\n",
      "\n",
      "Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Mikhail Burtsev, and Evgeny Burnaev. 2024. Arigraph: Learning knowledge graph world models with episodic memory for llm agents. Preprint , arXiv:2407.04363.\n",
      "\n",
      "Yubin Ge, Salvatore Romeo, Jason Cai, Raphael Shu, Monica Sunkara, Yassine Benajiba, and Yi Zhang. 2025. Tremu: Towards neuro-symbolic temporal reasoning for llm-agents with memory in multi-session dialogues. arXiv preprint arXiv:2502.01630 .\n",
      "\n",
      "Yuki Hou, Haruki Tamoto, and Homei Miyashita. 2024. 'my agent understands me better': Integrating dynamic human-like memory recall and consolidation in llm-based agents. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems , page 1-7. ACM.\n",
      "\n",
      "Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. 2023a. Chatdb: Augmenting llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901 .\n",
      "\n",
      "Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. 2023b. Chatdb: Augmenting llms with databases as their symbolic memory. Preprint , arXiv:2306.03901.\n",
      "\n",
      "- Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao, and Ping Luo. 2024. Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model. Preprint , arXiv:2408.09559.\n",
      "- Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity search with gpus. Preprint , arXiv:1702.08734.\n",
      "- Vladimir Karpukhin, Barlas O˘ guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. 2020. Dense passage retrieval for open-domain question answering. Preprint , arXiv:2004.04906.\n",
      "- Tingting Liang, Chenxin Jin, Lingzhi Wang, Wenqi Fan, Congying Xia, Kai Chen, and Yuyu Yin. 2024. LLMREDIAL: A large-scale dataset for conversational recommender systems created from user behaviors with LLMs. In Findings of the Association for Computational Linguistics: ACL 2024 , pages 8926-8939,\n",
      "\n",
      "Bangkok, Thailand. Association for Computational Linguistics.\n",
      "\n",
      "Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. 2023a. Think-in-memory: Recalling and post-thinking enable llms with long-term memory. Preprint , arXiv:2311.08719.\n",
      "\n",
      "Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023b. G-eval: Nlg evaluation using gpt-4 with better human alignment. Preprint , arXiv:2303.16634.\n",
      "\n",
      "Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating very long-term conversational memory of llm agents. Preprint , arXiv:2402.17753.\n",
      "\n",
      "Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024. Memgpt: Towards llms as operating systems. Preprint , arXiv:2310.08560.\n",
      "\n",
      "Gustavo Penha, Ali Vardasbi, Enrico Palumbo, Marco de Nadai, and Hugues Bouchard. 2024. Bridging search and recommendation in generative retrieval: Does one task help the other? Preprint , arXiv:2410.16823.\n",
      "\n",
      "Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, and Jonathan Richard Schwarz. 2024. Online adaptation of language models with a memory of amortized contexts. arXiv preprint arXiv:2403.04317 .\n",
      "\n",
      "Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, and Junyang Lin. 2025. Enabling scalable oversight via self-evolving critic. Preprint , arXiv:2501.05727.\n",
      "\n",
      "- Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. 2024a. Mixture-of-agents enhances large language model capabilities. Preprint , arXiv:2406.04692.\n",
      "- Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. 2024b. A survey on large language model based autonomous agents. Frontiers of Computer Science , 18(6).\n",
      "- Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, and Yangqiu Song. 2024c. Rethinking the bounds of llm reasoning: Are multi-agent discussions the key? Preprint , arXiv:2402.18272.\n",
      "- Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, and Yangqiu Song. 2024d. Rethinking the bounds of llm reasoning: Are multi-agent discussions the key? Preprint , arXiv:2402.18272.\n",
      "\n",
      "Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, and Yangqiu Song. 2024e. Rethinking the bounds of llm reasoning: Are multi-agent discussions the key? Preprint , arXiv:2402.18272.\n",
      "\n",
      "Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. 2024f. Agent workflow memory. Preprint , arXiv:2409.07429.\n",
      "\n",
      "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. Preprint , arXiv:2210.03629.\n",
      "\n",
      "Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and JiRong Wen. 2024. A survey on the memory mechanism of large language model based agents. Preprint , arXiv:2404.13501.\n",
      "\n",
      "Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. 2024a. Expel: Llm agents are experiential learners. Preprint , arXiv:2308.10144.\n",
      "\n",
      "Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. 2024b. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence , pages 19632-19642.\n",
      "\n",
      "Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2023. Memorybank: Enhancing large language models with long-term memory. Preprint , arXiv:2305.10250.\n",
      "\n",
      "Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. 2023. Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. Preprint , arXiv:2305.17144.\n",
      "\n",
      "## A Ethical Consideration\n",
      "\n",
      "We have thoroughly reviewed the licenses of all scientific artifacts, including datasets and models, ensuring they permit usage for research and publication purposes. To protect anonymity, all datasets used are de-identified. Our proposed method demonstrates considerable potential in significantly reducing both the financial and environmental costs typically associated with enhancing large language models. By lessening the need for extensive data collection and human labeling, our approach not only streamlines the process but also provides an effective safeguard for user and data privacy, reducing the risk of information leakage during training corpus construction. Additionally, throughout the paper-writing process, Generative AI was exclusively utilized for language checking, paraphrasing, and refinement.\n",
      "\n",
      "## B Autonomous Memory Augmentation\n",
      "\n",
      "## B.1 Attribute Mining\n",
      "\n",
      "Figure 4 illustrates examples for the two types of attribute augmentation: entity-centric and conversation-centric. The entity-centric augmentation represents the main attributes generated for the book entitled 'Already Taken', where attributes are derived based on entity-specific characteristics such as genre, author, and thematic elements. The conversation-centric example illustrates the augmentation generated for a sample two turns dialogue from the LLM-REDIAL dataset, highlighting attributes that capture contextual elements such as user intent, motivation, emotion, perception, and genre of interest.\n",
      "\n",
      "Furthermore, Figure 5 presents an overview of the top five attributes across different domains in the LLM-REDIAL dataset. These attributes represent the predominant attributes specific to each domain, highlighting the significance of different attributes in augmentation generation. Consequently, the integration of priority-based embeddings has led to improved performance.\n",
      "\n",
      "## C Embedding-based Retrieval\n",
      "\n",
      "In the context of embedding-based memory retrieval, movies are augmented using MemInsight, and the generated attributes are embedded to retrieve relevant movies from memory. Two main embedding methods were considered:\n",
      "\n",
      "Figure 4: An example of entity-centric augmentation for the book 'Already Taken', and a conversation-centric augmentation for a sample dialogue from the LLM-REDIAL dataset.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 5: Top 10 attributes by frequency in the LLM-REDIAL dataset across domains (Movies, Sports Items, Electronics, and Books) using MemInsight Attribute Mining. Frequency indicates how often each attribute was generated to augment different movies.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## (1) Averaging Over Independent Embeddings\n",
      "\n",
      "this method was adopted in our experiments.\n",
      "\n",
      "Each attribute and its corresponding value in the generated augmentations is embedded independently. The resulting attribute embeddings are then averaged across all attributes to generate the final embedding vector representation, as illustrated in Figure 6 which are subsequently used in similarity search to retrieve relevant movies.\n",
      "\n",
      "(2) All Augmentations Embedding In this method, all generated augmentations, including all attributes and their corresponding values, are encoded into a single embedding vector and stored for retrieval as shown in Figure 6. Additionally, Figure 7 presents the cosine similarity results for both methods. As depicted in the figure, averaging over all augmentations produces a more consistent and reliable measure, as it comprehensively captures all attributes and effectively differentiates between similar and distinct characteristics. Consequently,\n",
      "\n",
      "## D Question Answering\n",
      "\n",
      "## D.1 Prompts\n",
      "\n",
      "Table 8 outlines the prompts used in the Question Answering task for generating augmentations in both questions and conversations.\n",
      "\n",
      "## E Conversational Recommendation\n",
      "\n",
      "## E.1 Prompts\n",
      "\n",
      "Table 9 presents the prompts used in Conversational Recommendation for movie recommendations, incorporating both basic and priority augmentations.\n",
      "\n",
      "## E.2 Evaluation Framework\n",
      "\n",
      "Figure 8 presents the evaluation framework for the Conversation Recommendation task. The process begins with (1) augmenting all movies in memory\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Embedding-based Retrieval\n",
      "\n",
      "(a) Averaging over Independent Embeddings\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 6: Embedding methods for Embedding-based retrieval methods using generated Movie augmentations including (a) Averaging over Independent Embeddings and (b) All Augmentations Embedding.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "(b) All Augmentations Embedding\n",
      "\n",
      "Figure 7: An illustrative example of augmentation embedding methods for three movies: (1) The Departed, (2) Shutter Island, and (3) The Hobbit. Movies 1 and 2 share similar attributes, whereas movies 1 and 3 differ. Te top 5 attributes of every movie were selected for a simplified illustration.\n",
      "\n",
      "using entity-centric augmentations to enhance retrieval effectiveness. (2) Next, all dialogues in the dataset are prepared to simulate the recommendation process by masking the ground truth labels and prompting the LLM to find the masked labels based on augmentations from previous user interactions. (3) Recommendations are then generated using the retrieved memory, which may be attributebased-for instance, filtering movies by specific attributes such as genre or using embedding-based retrieval. (4) Finally, the recommended movies are evaluated against the ground truth labels to assess the accuracy and effectiveness of the retrieval and recommendation approach.\n",
      "\n",
      "## E.3 Event Summarization\n",
      "\n",
      "## E.3.1 Prompts\n",
      "\n",
      "Table 10 presents the prompt used in Event Summarization to augment dialogues by generating relevant attributes. In this process, only attributes related to events are considered to effectively summarize key events from dialogues, ensuring a focused and structured summarization approach.\n",
      "\n",
      "## F Qualitative Analysis\n",
      "\n",
      "Figure 9 illustrates the augmentations generated using different LLM models, including ClaudeSonnet, Llama, and Mistral for a dialogue turn from the LoCoMo dataset. As depicted in the figure, augmentations produced by Llama include hallucinations, generating information that does not exist. In contrast, Figure 10 presents the augmentations for the subsequent dialogue turn using the same models. Notably, Claude-Sonnet maintains consistency across both turns, suggesting its stable performance throughout all experiments. While Mistral model tend to be less stable as it included attributes that are not in the dialogue.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 8: Evaluation Framework for Conversation Recommendation Task.\n",
      "\n",
      "Figure 9: Augmentation generated on a Turn-level for a sample dialogue turn from the LoCoMo dataset using Claude-3-Sonnet, Llama v3 and Mistral v1 models.\n",
      "\n",
      "| Turn                         | \"Hey                                                                                      | \"Hey                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | \"Hey                                                                                                              |\n",
      "|------------------------------|-------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|\n",
      "| Sample Generated Annotations | [person]<Jon> [job_status]<lost job> [former_job]<banker> [intent]<starting own business> | Gina replied, \"Sorry to hear that, Jon. What kind of business are you thinking of starting Jon said; \"I'm thinking of opening a bakery. I've always been passionate about baking and think it could be a great opportunity Gina asked, 'Do you have any experience running a business Jon replied, but I've solid business plan and I'm willing to in the hard work to make it happen [Speaker]<Jon> [JobLossReason]<Lost as a banker> [NewBusinessldea]<Starting a bakery> [Passion]<Baking> [BusinessExperience]<No> [BusinessPlan]<Solid business plan> [HelpOffered]<Gina offered to help> [HelpNeeded] <Finding a location> [Location]<Downtown> [MeetingTime]<Tomorrow> \"No, got put job | [person]<Jon> [emotion]<happy> [occupation]<banker> [current_occupation]<unem ployed> [intention]<start_business> |\n",
      "\n",
      "Figure 10: Augmentations generated for the turn following the turn in Figure 9\n",
      "\n",
      "| Turn                         | also lost my job at Door Dash this month. What business are you thinking of?\"                                                             | also lost my job at Door Dash this month. What business are you thinking of?\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | also lost my job at Door Dash this month. What business are you thinking of?\"                                                                                                                                                                                          |\n",
      "|------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| Sample Generated Annotations | [person] <Gina> [job_status]<lost_job> [former_employer]<Door Dash> [person]<Jon> [job_status]<lost_job> [intent]<starting_own_bus iness> | Gina said, to hear that, Jon. What kind of business are you thinking of starting?\" Jon said, \"I'm thinking of starting a small a lot of experience in finance and think can make a g0 of it.\"Gina said, \"That sounds great! What kind of consulting firm are you thinking of starting?\" Jon said, \"I'm thinking of starting a boutique consulting firm that focuses on finance and strategy. [Speaker] <Jon> [JobLossReason]<Lost job as a banker> [NewBusinessldea]<Starting a bakery> [Passion]<Baking> [BusinessExperience]<No> [BusinessPlan] <Solid business plan> [HelpOffered]<Gina offered to help> [HelpNeeded]<Finding a locationz [Location]<Downtown> [MeetingTime]<Tomorrow> \"Sorry got | [person Gina] <namez: Gina [person Jon]<namez: Jon [lost_job.Gina]<job_lost?: Door Dash [lost_job. Jon]<job_lost?: N/A [business_idea]<ideaz: N/A sorry [emotion Gina]<emotion>: awesome [question]<question>: What business are you thinking of? starting a business. |\n",
      "\n",
      "using Claude-3-Sonnet, Llama v3 and Mistral v1 models. Hallucinations are presented in red.\n",
      "\n",
      "## Question Augmentation\n",
      "\n",
      "Given the following question, determine what are the main inquiry attribute to look for and the person the question is for. Respond in the format: Person:[names]Attributes:[].\n",
      "\n",
      "## Basic Augmentation\n",
      "\n",
      "You are an expert annotator who generates the most relevant attributes in a conversation. Given the conversation below, identify the key attributes and their values on a turn by turn level.\n",
      "\n",
      "Attributes should be specific with most relevant values only. Don't include speaker name. Include value information that you find relevant and their names if mentioned. Each dialogue turn contains a dialogue id between [ ]. Make sure to include the dialogue the attributes and values are extracted form. Important: Respond only in the format [{speaker name:[Dialog id]:[attribute]&lt;value&gt;}].\n",
      "\n",
      "Dialogue Turn:{}\n",
      "\n",
      "## Priority Augmentation\n",
      "\n",
      "You are an expert dialogue annotator, given the following dialogue turn generate a list of attributes and values for relevant information in the text.\n",
      "\n",
      "Generate the annotations in the format: [attribute]&lt;value&gt;where attribute is the attribute name and value is its corresponding value from the text.\n",
      "\n",
      "and values for relevant information in this dialogue turn with respect to each person. Be concise and direct.\n",
      "\n",
      "Include person name as an attribute and value pair.\n",
      "\n",
      "Please make sure you read and understand these instructions carefully.\n",
      "\n",
      "- 1- Identify the key attributes in the dialogue turn and their corresponding values.\n",
      "- 2- Arrange attributes descendingly with respect to relevance from left to right.\n",
      "- 3- Generate the sorted annotations list in the format: [attribute]&lt;value&gt;where attribute is the attribute name and value is\n",
      "- its corresponding value from the text.\n",
      "- 4- Skip all attributes with none vales\n",
      "\n",
      "Important: YOU MUST put attribute name is between [ ] and value between &lt;&gt;. Only return a list of [attribute]&lt;value&gt;nothing else. Dialogue Turn: {}\n",
      "\n",
      "Table 8: Prompts used in Question Answering for generating augmentations for questions. Also, augmentations for conversations, utilizing both basic and priority augmentations.\n",
      "\n",
      "## Basic Augmentation\n",
      "\n",
      "For the following movie identify the most important attributes independently. Determine all attributes that describe the movie based on your knowledge of this movie. Choose attribute names that are common characteristics of movies in general. Respond in the following format: [attribute]&lt;value of attribute&gt;. The Movie is: {}\n",
      "\n",
      "## Priority Augmentation\n",
      "\n",
      "You are a movie annotation expert tasked with analyzing movies and generating key-attribute pairs. For the following movie identify the most important. Determine all attribute that describe the movie based on your knowledge of this movie. Choose attribute names that are common characteristics of movies in general. Respond in the following format: [attribute]&lt;value of attribute&gt;. Sort attributes from left to right based on their relevance. The Movie is:{}\n",
      "\n",
      "## Dialogue Augmentation\n",
      "\n",
      "Identify the key attributes that best describe the movie the user wants for recommendation in the dialogue. These attributes should encompass movie features that are relevant to the user sorted descendingly with respect to user interest. Respond in the format: [attribute]&lt;value&gt;.\n",
      "\n",
      "Table 9: Prompts used in Conversational Recommendation for recommending Movies utilizing both basic and priority augmentations.\n",
      "\n",
      "## Dialogue Augmentation\n",
      "\n",
      "Given the following attributes and values that annotate a dialogue for every speaker in the format [attribute]&lt;value&gt;, generate a summary for the event attributes only to describe the main and important events represented in these annotations. Refrain from mentioning any minimal event. Include any event-related details and speaker. Format: a bullet paragraph for major life events for every speaker with no special characters. Don't include anything else in your response or extra text or lines. Don't include bullets. Input annotations: {}\n"
     ]
    }
   ],
   "source": [
    "source = \"https://arxiv.org/pdf/2503.21760\"\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(source)\n",
    "print(result.document.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d96bc68-8341-439e-a275-83491904328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 创新点总结\n",
      "\n",
      "#### 1. 自主记忆增强方法（MemInsight）\n",
      "- 提出了一种名为 **MemInsight** 的自主记忆增强方法，用于改进大型语言模型（LLM）代理的记忆表示和检索机制。\n",
      "- MemInsight 通过自主增强历史交互中的语义和上下文信息，使 LLM 代理能够提供更准确和上下文相关的响应。\n",
      "\n",
      "#### 2. 结构化的记忆增强\n",
      "- 设计了一种结构化的自主方法，以适应 LLM 代理在多种任务中的记忆表示，同时保持长时间对话中的上下文一致性。\n",
      "- 开发了利用生成的记忆增强来过滤无关记忆并保留关键历史洞察的记忆检索方法。\n",
      "\n",
      "#### 3. 多任务有效性验证\n",
      "- 在多个任务场景中进行了实证验证，包括对话推荐、问答和事件摘要。\n",
      "- 在 LLMREDIAL 数据集上，MemInsight 将推荐的说服力提升了高达 14%。\n",
      "- 在 LoCoMo 检索任务中，MemInsight 的召回率比 RAG 基线高出 34%。\n",
      "\n",
      "#### 4. 属性挖掘与注释\n",
      "- 提出了两种主要的属性生成方向：实体中心（entity-centric）和对话中心（conversation-centric）。\n",
      "- 实体中心属性捕获特定实体（如电影或书籍）的主要特征和特点。\n",
      "- 对话中心属性关注用户意图、偏好、情感、动机和选择，从而提高个性化响应和记忆检索。\n",
      "\n",
      "#### 5. 属性粒度\n",
      "- 引入了属性粒度的概念，区分了逐轮（turn-level）和会话级（session-level）的注释。\n",
      "- 逐轮注释提供了更精细的细节，而会话级注释提供了更广泛的对话概览。\n",
      "\n",
      "#### 6. 记忆检索方法\n",
      "- 提出了两种主要的检索方法：\n",
      "  - **基于属性的检索**：根据当前上下文生成的任务特定属性进行检索。\n",
      "  - **基于嵌入的检索**：将记忆增强转化为唯一的嵌入表示，并使用向量相似性搜索检索最相关的记忆。\n",
      "\n",
      "#### 7. 性能提升\n",
      "- 在多个任务中表现出显著的性能提升，特别是在多跳问题中，MemInsight 的表现优于基线模型。\n",
      "- 使用优先级增强（priority augmentation）进一步提高了上下文表示的效果。\n",
      "\n",
      "---\n",
      "\n",
      "### 翻译为中文\n",
      "\n",
      "#### 摘要\n",
      "大型语言模型（LLM）代理已经发展到能够智能地处理信息、做出决策并与用户或工具互动。一个关键能力是整合长期记忆功能，使这些代理能够利用历史交互和知识。然而，不断增长的记忆规模和对语义结构的需求带来了重大挑战。在这项工作中，我们提出了一种自主记忆增强方法——MemInsight，以改进语义数据表示和检索机制。通过利用历史交互的自主增强，LLM 代理被证明可以提供更准确和上下文相关的响应。我们在三个任务场景中进行了实证验证：对话推荐、问答和事件摘要。在 LLMREDIAL 数据集上，MemInsight 将推荐的说服力提升了高达 14%。此外，在 LoCoMo 检索任务中，MemInsight 的召回率比 RAG 基线高出 34%。我们的实证结果表明，MemInsight 有潜力提升 LLM 代理在多种任务中的上下文性能。\n",
      "\n",
      "#### 1. 引言\n",
      "我们介绍了一种自主记忆增强方法——MemInsight，它使 LLM 代理能够识别数据中的关键信息，并主动提出有效的属性以增强记忆。这类似于人类的注意力控制和认知更新过程，涉及选择性地优先考虑相关信息、过滤干扰，并不断用新的和相关数据刷新心理工作空间。\n",
      "\n",
      "#### 2. 相关工作\n",
      "组织良好且语义丰富的记忆结构使得高效存储和检索信息成为可能，从而使 LLM 代理能够保持上下文连贯性并提供相关响应。开发有效的 LLM 代理记忆模块通常涉及两个关键组件：结构化记忆生成和记忆检索方法。\n",
      "\n",
      "#### 3. 自主记忆增强\n",
      "我们提出的 MemInsight 模型旨在通过结构化增强过程优化记忆检索，从而增强记忆表示。图 1 展示了模型的概述，突出了其主要模块：属性挖掘、注释和记忆检索器。\n",
      "\n",
      "#### 4. 评估\n",
      "我们使用一系列实验评估 MemInsight 在回答问题、对话推荐和事件摘要等任务中的有效性。\n",
      "\n",
      "#### 5. 实验\n",
      "我们在 LLM-REDIAL 和 LoCoMo 数据集上进行了一系列实验，验证 MemInsight 在不同任务中的表现。\n",
      "\n",
      "#### 6. 结论\n",
      "本文介绍了 MemInsight，一种通过基于属性的注释增强 LLM 代理记忆的自主方法。尽管在标准指标上保持了相当的性能，MemInsight 显著提高了基于 LLM 的评估分数，展示了其在捕捉语义和提升跨任务和数据集性能方面的有效性。\n",
      "\n",
      "#### 7. 局限性\n",
      "虽然所提出的 MemInsight 模型在增强检索和上下文理解方面展示了显著潜力，但必须承认某些局限性。MemInsight 依赖于使用 LLM 生成的注释的质量和粒度，使其容易受到 LLM 输出固有问题的影响，例如幻觉。此外，当前的评估指标虽然提供了有价值的见解，但可能无法全面捕捉检索和生成质量的所有方面，强调了开发更强大和多维评估框架的必要性。"
     ]
    }
   ],
   "source": [
    "from illufly.llm import ChatAgent\n",
    "\n",
    "a = ChatAgent(model=\"qwen-long\", imitator=\"QWEN\")\n",
    "task = f'帮我整理这篇论文的创新点，并翻译为中文：\\n{result.document.export_to_markdown()}'\n",
    "async for chunk in a.chat(messages=task):\n",
    "    print(chunk['output_text'], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40907bf5-d031-473b-aa2b-2cdb09499761",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## NLI4DB: A Systematic Review of Natural Language Interfaces for Databases\n",
      "\n",
      "## Mengyi Liu and Jianqiu Xu *\n",
      "\n",
      "Nanjing University of Aeronautics and Astronautics, Nanjing, China { liumengyi,jianqiu } @nuaa.edu.cn\n",
      "\n",
      "## Abstract\n",
      "\n",
      "As the demand for querying databases in all areas of life continues to grow, researchers have devoted significant attention to the natural language interface for databases (NLIDB). This paper presents a comprehensive survey of recently proposed NLIDBs. We begin with a brief introduction to natural language processing techniques, executable database languages and the intermediate representation between natural language and executable language, and then provide an overview of the translation process from natural language to executable database language. The translation process is divided into three stages: (i) natural language preprocessing , (ii) natural language understanding , and (iii) natural language translation . Traditional and data-driven methods are utilized in the preprocessing stage. Traditional approaches rely on predefined rules and grammars, and involve techniques such as regular expressions, dependency parsing and named entity recognition. Data-driven approaches depend on large-scale data and machine learning models, using techniques including word embedding and pattern linking. Natural language understanding methods are classified into three categories: (i) rule-based , (ii) machine learning-based , and (iii) hybrid . We then describe a general construction process for executable languages over relational and spatio-temporal databases. Subsequently, common benchmarks and evaluation metrics for transforming natural language into executable language are presented, and methods for generating new benchmarks are explored. Finally, we summarize the classification, development, and enhancement of NLIDB systems, and discuss deep language understanding and database interaction techniques related to NLIDB, including (i) using LLM for Text2SQL tasks , (ii) generating natural language interpretations from SQL , and (iii) transforming speech queries into SQL .\n",
      "\n",
      "Keywords: Natural language interface for database, Semantic parsing, Structured language, Query processing\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "In today's data-driven world, databases are the backbone of a number of applications, from social media platforms to financial systems. However, accessing and querying these vast repositories of information often requires specialized knowledge of query languages such as SQL, which can be a significant barrier for non-expert users, limiting their ability to harness the full potential of the data at their fingertips. The advent of natural language interface (NLI) has the potential to eliminate the interaction barrier between users and terminals [130]. The integration of natural language processing (NLP) and database technology represents an intriguing avenue for future research. There are systems that facilitate the transformation of natural language into structured language [141, 22], provide the natural language description for query execution plans [139, 23], and transform SQL into natural language [40, 132].\n",
      "\n",
      "Imagine a world where anyone, regardless of technical proficiency, can effortlessly interact with complex databases using everyday language. This vision is becoming a reality through the development of natural language\n",
      "\n",
      "* Corresponding author.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Translated Executable Language\n",
      "\n",
      "Figure 1: Example of translating a natural language into an executable language\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 2: A summary of translation techniques\n",
      "\n",
      "interface for databases (NLIDB), which aims to transform a natural language query (NLQ) into an executable language, as illustrated in Figure 1. Users tend to favor an interactive interface that allows them to confirm the accuracy and precision of the generated structured language [95]. The NLIDB enables users to avoid the necessity of possessing expertise in structured query languages and database schema, thereby significantly streamlining the efforts of users and enhancing the benefits of utilizing databases [70]. The initial NLIDBs, including BASEBALL, LUNAR, LADDER, Chat-80, and ASK, were released in rapid succession [2]. Subsequently, NLIDBs have emerged and are primarily utilized in relational databases (e.g., GENSQL [44] and CatSQL [48]), spatial domains (e.g., SpatialNLI [80, 141] and NALSpatial [89]), RDF question and answer (e.g., Querix [69] and TEQUILA [64]), and XML databases (e.g., NaLIX [84, 85] and DaNaLIX [81]).\n",
      "\n",
      "Despite years of research, the landscape of NLIDB is fraught with challenges [9, 82]. The inherent ambiguity and variability of natural language make NLIDB difficult to ensure accurate query interpretation. Additionally, understanding the structure and semantics of different databases adds another layer of complexity. Furthermore, achieving real-time performance while maintaining high accuracy in query translation remains an ongoing challenge. While large language models (LLMs) offer new avenues for querying databases using natural language, the training and reasoning of such models necessitate a substantial amount of computational resources, which may prove challenging to implement in resource-limited scenarios [97]. Moreover, the decision-making process of LLMs is frequently opaque and lacks interpretability, making it difficult to ascertain whether the generated query results align with the user's intent [126]. These obstacles underscore the need for continued research and development to refine NLIDB.\n",
      "\n",
      "In light of these observations, this systematic review explores the current state of NLIDB, examining the various approaches and technologies that have been proposed to connect natural language with database querying, named NLI4DB. The aim of this survey is to offer a comprehensive overview that serves as both a valuable reference for researchers and a practical guide for practitioners aiming to implement effective NLIDB solutions. NLI4DB presents a thorough examination of the NLIDB subject, categorizing the work into subtopics and providing in-depth analysis for each one. The translation process from natural language to executable language is divided into three stages: (i) natural language preprocessing , (ii) natural language understanding , and (iii) natural language translation . The three-stage division provides physical independence by separating the physical arrangement of data from the semantics of queries [113]. The techniques for the translation are shown in Figure\n",
      "\n",
      "2.\n",
      "\n",
      "(i) Natural language preprocessing generally involves the construction of dedicated data dictionaries for the domain using stemming extraction and synonym techniques. Part-of-speech tagging and word segmentation are then performed on the input natural language. Methods used in the preprocessing stage include traditional and data-driven. Traditional approaches rely on predefined rules and grammars for domain-specific text processing, and involve techniques such as regular expressions, dependency parsing and named entity recognition (NER). Data-driven approaches depend on large-scale data and machine learning models for complex or variable text processing, using techniques such as word embedding and pattern linking.\n",
      "\n",
      "(ii) Natural language understanding has rule-based, machine learning-based, and hybrid approaches. Rulebased systems can only deal with knowledge bases of specific domains, whose semantic understanding processes either define the concept of semantic accessibility or translate the NLQ into an intermediate representation that can describe the semantics and relationships in an accessible manner. Machine learning-based systems employ a variety of techniques to parse text, including unsupervised approaches, question-and-answer supervised learning, statistical machine translation techniques, encoder-decoder frameworks with recurrent neural networks, and combinations of deterministic algorithms and machine learning. Hybrid approaches combine rules and machine learning techniques to maximize their benefits.\n",
      "\n",
      "(iii) Natural language translation uses distinctive algorithms to map processed key semantic information to corresponding structured language components. To build the SQL, the database elements matched by the NLQ are placed in the appropriate locations in the SELECT, FROM, and WHERE parts. In the event that a query involves multiple relations, it is necessary to include the join condition and the names of the participating relations in the WHERE and FROM clause, respectively. In the process of building an executable language of a spatio-temporal database, the query type of the input NLQ is initially identified. The operators required to build the executable language are then determined according to the query type. Finally, the key semantic information obtained from the natural language understanding stage is integrated to form an executable language.\n",
      "\n",
      "The existing survey [2] related to NLIDB focuses on the comparative analysis of the entire natural language interface system. Affolter et al. [2] divide NLIs into four groups: (i) keyword-based systems , (ii) pattern-based systems , (iii) parsing-based systems , and (iv) grammar-based systems . For each group, they provide an overview of representative systems and describe the most illustrative one in detail. In addition, they systematically compare 24 recently developed NLIDBs on the basis of the sample world designed in the paper. Each system is evaluated using 10 example questions to show the advantages and disadvantages.\n",
      "\n",
      "Compared with Affolter et al. [2], we divide the system translation process into three steps and focus on the comparative analysis of each step. We investigate the recently developed NLIDB systems and divide the translation process into three stages: (i) natural language preprocessing , (ii) natural language understanding , and (iii) natural language translation . We classify natural language preprocessing techniques into traditional and data-driven. Natural language understanding methods are then analyzed in three categories: (i) rule-based , (ii) machine learning-based , and (iii) hybrid . Next, we provide a comprehensive outline of the construction process of executable languages for relational and spatio-temporal databases. Finally, we present commonly used benchmarks and evaluation metrics, and describe the classification, development, and enhancement of NLIDBs.\n",
      "\n",
      "Table 1: Frequently used notations\n",
      "\n",
      "| Name                                    | Abbreviation   |\n",
      "|-----------------------------------------|----------------|\n",
      "| Natural language interface for database | NLIDB          |\n",
      "| Natural language interface              | NLI            |\n",
      "| Natural language query                  | NLQ            |\n",
      "| Natural language processing             | NLP            |\n",
      "| Named entity recognition                | NER            |\n",
      "| Large language model                    | LLM            |\n",
      "| First-order logic                       | FOL            |\n",
      "| Automatic speech recognition            | ASR            |\n",
      "| sequence-to-sequence                    | seq2seq        |\n",
      "\n",
      "The rest of the paper is structured as follows. Section 2 furnishes the background concerning NLIDB, including natural language processing techniques, executable database languages and intermediate representation languages. Section 3 describes the generation of executable database languages in terms of three stages: (i) natural language preprocessing , (ii) natural language understanding and (iii) natural language translation . Section 4 summarizes 11 popular benchmarks for transforming NLQ into SQL and 3 evaluation metrics, including response time, translatability, and translation precision, and explores the methods for generating new benchmarks. Section 5 analyzes the classification, development and enhancement of NLIDBs. Section 6 discuss deep language understanding and database interaction techniques related to NLIDB, including (i) using LLM for Text2SQL tasks , (ii) generating natural language interpretations from SQL , and (iii) transforming speech queries into SQL . Section 7 explores the open problems of NLIDB and concludes the survey. Table 1 summarizes the frequently used notations.\n",
      "\n",
      "## 2 Background: NLP techniques and query languages\n",
      "\n",
      "We introduce the background related to NLIDB, including natural language processing techniques, executable database languages, and intermediate representation languages.\n",
      "\n",
      "## 2.1 Natural language processing techniques\n",
      "\n",
      "NLP is an interdisciplinary discipline that integrates several fields such as linguistics, computer science, and mathematics, and aims to make computers capable of understanding, processing and generating natural language text or speech. Through segmentation, lexical annotation, and syntactic analysis, NLP provides structured processing of text to achieve semantic understanding and information extraction. The application areas of NLP cover machine translation, sentiment analysis, information retrieval, and dialogue systems, providing people with an intelligent and convenient way of language interaction.\n",
      "\n",
      "A Brief History. The earliest research on natural language processing is machine translation. In 1950, Alan Turing proposed the ultimate test for determining the arrival of truly ' intelligent ' machines, which is generally regarded as the inception of the idea of NLP [96]. From the 1950s to the 1970s, the rule-based method was used to process natural language, which was based on grammatical rules and formal logic. In the 1970s, the statistic-based method gradually supplanted the rule-based method. At this juncture, NLP built on mathematical models and statistic made a substantial breakthrough and was applied to practical applications. From 2008 to the present, researchers have introduced deep learning to NLP in response to the achievements in image recognition and speech recognition.\n",
      "\n",
      "The NLP techniques commonly used in NLIDBs are as follows.\n",
      "\n",
      "- (i) Part of speech tagging refers to assigning the correct part of speech to each word in the segmented text,\n",
      "\n",
      "Figure 3: Processing natural language using Stanford CoreNLP\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "determining whether each word is a noun, verb, or adjective. In NLIDB, part of speech tagging facilitates the identification of the grammatical roles of individual words in natural language queries, leading to an accurate comprehension of users' intent. Taking the natural language query ' All movies starring Brad Pitt from 2000 until 2010. ' as an example, the result of part of speech tagging using Stanford CoreNLP is shown in Figure 3(a). In the figure, DT = determiner; NNS = plural noun; VBG = the gerund or present participle of a verb; NNP = singular proper noun; IN = preposition or subordinating conjunction; CD = cardinal number.\n",
      "\n",
      "- (ii) Lemmatization is the process of reducing the different forms of a word to the original form. In NLIDB, lemmatization is beneficial in unifying words of various tenses and morphs in natural language queries into base forms in order to match the content in the database. Taking the natural language query ' All movies starring Brad Pitt from 2000 until 2010. ' as an example, the result of lemmatization using Stanford CoreNLP is shown in Figure 3(b).\n",
      "- (iii) Named entity recognition is the procedure of identifying entities with specific meanings in natural language text [114]. Generally, the recognized entities can be categorized into three primary groups (entity, temporal, and numeric) and seven subgroups (PERSON, ORGANIZATION, LOCATION, TIME, DATE, MONEY, and PERCENT). NER in NLIDB enables the identification of entities involved in a natural language query to locate the topic and scope of the query. Taking the natural language query ' All movies starring Brad Pitt from 2000 until 2010. ' as an example, the result of NER using Stanford CoreNLP is shown in Figure 3(c).\n",
      "- (iv) Dependency parsing involves analyzing the dependencies between words in natural language sentences. A binary asymmetric relationship between words is called dependency, which is described as an arrow from the head (the subject to be modified) to the dependent (the modifier). Dependency parsing in NLIDB facilitates the understanding of grammatical relationships between words in NLQs, so that the structure and meaning of the query can be accurately understood. Taking the natural language query ' All movies starring Brad Pitt from 2000 until 2010. ' as an example, the result of dependency parsing using Stanford CoreNLP is illustrated in Figure 3(d). In the figure, punct = punctuation; obl = oblique nominal; obj = object; det = determiner; acl = clausal modifier of noun; case = case marking.\n",
      "\n",
      "With the vigorous development of NLP technology, a number of NLP tools are appearing [114]. These tools can perform basic tasks, including dependency parsing, named entity recognition, lemmatization, and part of\n",
      "\n",
      "speech tagging, each of which has distinct advantages and disadvantages. The following is a list of the established open source natural language processing tools.\n",
      "\n",
      "- (i) NLTK is a natural language processing toolkit using Python as the programming language. NLTK has complete functions and realizes many of the functional components in natural language processing, such as named entity recognition, sentence structure analysis, part-of-speech tagging, and text classification [13]. Born for the academic field, NLTK is suitable for study and research. The disadvantage is that NLTK has a slower processing speed than other tools.\n",
      "- (ii) spaCy , a commercial open source software, is an industrial-grade natural language processing software programmed in Python and Cython languages [45]. spaCy, which follows NLTK, includes pre-trained statistical models and word vectors. spaCy can break down text into semantic units like articles, words and punctuation, and support named entity recognition. spaCy is characterized by fast and accurate syntax analysis, and comprehensive functions ranging from simple part-of-speech tagging to advanced deep learning.\n",
      "- (iii) Stanford CoreNLP is a tool set developed by Stanford University using the Java programming language. Stanford CoreNLP supports a variety of natural languages and has rich interfaces for programming languages that can be used without Java [91]. Stanford CoreNLP is an efficient tool created by high-level research institutions and is widely used in scientific research and experiments, but may incur additional costs in production systems. Stanford CoreNLP may not be the best choice for industry.\n",
      "- (iv) TextBlob is an extension to NLTK, which provides an easier way to use the functionality of NLTK [57]. TextBlob supports sentiment analysis, tokenization, part-of-speech tagging, and text classification. One of the advantages is that TextBlob can be used in production environments where performance requirements are not too high. TextBlob can be applied in a wide range of scenarios, especially for small projects.\n",
      "\n",
      "## 2.2 Executable database languages\n",
      "\n",
      "The output of NLIDB is an executable database language, and we present executable languages over relational data, RDF data, and spatial data.\n",
      "\n",
      "## 2.2.1 Query language for relational data\n",
      "\n",
      "The standard executable query language for relational data is SQL. Such a language is a general-purpose, extremely powerful relational database language whose functions are not limited to querying, but also include creating database schema, inserting and modifying data, and defining and controlling database security integrity [29]. Following the establishment of SQL as an international standard language, numerous database manufacturers have released SQL-compatible software, including both database management systems and interfaces. Consequently, SQL serves as the universal data access language and standard interface for most databases, fostering a shared foundation for interoperability among different database systems. SQL has become the mainstream language in the database field which is of great significance.\n",
      "\n",
      "SQL provides the SELECT statement for querying data, which has flexible usage and rich functionality. The SELECT statement can perform simple single-table queries as well as complex join queries and nested queries, whose general format is:\n",
      "\n",
      "SELECT [ALL DISTINCT] | &lt; target column expression &gt; [alias] [, &lt; target column expression &gt; [alias]] FROM &lt; table name or view name &gt; [, &lt; table name or view name &gt; ] | (SELECT statement) [AS] &lt; alias [ WHERE &lt; conditional expression &gt; ]\n",
      "\n",
      "&gt; [ASC DESC]];\n",
      "\n",
      "[ GROUP BY &lt; column name 1 &gt; [ HAVING &lt; conditional expression &gt; ]]\n",
      "\n",
      "[ ORDER BY &lt; column name 2 &gt; |\n",
      "\n",
      "The purpose of the SELECT statement is to find the tuples that satisfy the conditions specified in the FROM clause, which may be a basic table, view, or derived table ,according to the conditional expression in the WHERE clause.\n",
      "\n",
      "The attribute value in the tuple is then selected on the basis of the target column expression in the SELECT clause to form the result table. When a GROUP BY clause is present, the output is organized by the value of &lt; column name 1 &gt; , where tuples sharing identical attribute column values are grouped together. Aggregation functions are usually applied to each group. When the GROUP BY clause is accompanied by a HAVING clause, the output will only include groups that satisfy the specified conditions. If an ORDER BY clause is present, the result table is sorted in ascending or descending order according to the values of &lt; column name 2 &gt; .\n",
      "\n",
      "## 2.2.2 Query language for RDF data\n",
      "\n",
      "The complete designation of RDF is Resource Description Framework, which is a data model designed to represent information about resources on the Internet. The data model typically describes a fact composed of three parts known as a triple, including (i) a subject , (ii) a predicate , and (iii) an object . An RDF graph contains multiple triples. RDF documents are written in XML to offer a standardized method for describing information. RDF is intended for computer applications to read and understand, rather than for visual presentation to web users.\n",
      "\n",
      "SPARQL is a specialized query language and data retrieval protocol designed for RDF, which stands for SPARQL Protocol and RDF Query Language [24]. SPARQL is a query language over RDF graphs, where the database is represented as a collection of ' subject-predicate-object ' triples. Although RDF data is inferential, SPARQL does not have an inference query function. SPARQL is tailored for managing data stored in RDF format, enabling both retrieval and manipulation. SPARQL is composed of the following components.\n",
      "\n",
      "- · The PREFIX clause is employed to declare a prefix with the objective of simplifying the use of URIs. The declaration of the prefix is optional.\n",
      "- · The SELECT clause serves the purpose of specifying the variables returned by a query.\n",
      "- · The WHERE clause is utilized to match data in RDF graphs. The clause contains one or more triple patterns that are employed to indicate the conditions of a query.\n",
      "- · The FILTER clause is designed to conditionally filter the results of a query. The clause can include boolean expressions to limit the set of results matched by the WHERE clause.\n",
      "\n",
      "The fundamental query types of SPARQL are as follows [101].\n",
      "\n",
      "SELECT query is the most frequently used type of query, whose function is to select variables and return a result set. A table is typically generated as the outcome of a SELECT query, which includes the variables that meet the query's criteria along with their corresponding values.\n",
      "\n",
      "CONSTRUCT query is used to generate a new RDF graph by utilizing the query pattern. In contrast to tabular results, the CONSTRUCT query produces an RDF graph that is constructed from the matching data of the query pattern.\n",
      "\n",
      "ASK query is designed to ascertain the existence of RDF data that satisfies the query pattern. The ASK query provides a response in the form of a boolean value (true or false) to indicate the presence or absence of a match.\n",
      "\n",
      "DESCRIBE query is employed to obtain the detailed description of resources. The description is determined by the query engine and typically consists of triples that are directly related to the resource.\n",
      "\n",
      "Each query type employs a WHERE clause to limit the scope of the query. Nevertheless, in the context of DESCRIBE queries, the inclusion of a WHERE clause is not mandatory. To illustrate, the subsequent query retrieves people from the data set who are above the age of 24:\n",
      "\n",
      "PREFIX info: &lt; http://somewhere/peopleInfo# &gt; SELECT ?resource WHERE { ?resource info:age ?age . FILTER (?age &gt; = 24)\n",
      "\n",
      "Table 2: Operators to query spatial data\n",
      "\n",
      "| Operator     | Signature                                                                                                                                                      | Meaning                                                       |\n",
      "|--------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|\n",
      "| distance     | point | line | region × point | line | region → real                                                                                                           | Compute the distance between two spa- tial objects.           |\n",
      "| direction    | point × point → real                                                                                                                                           | Compute the direction between two points.                     |\n",
      "| size         | line → real                                                                                                                                                    | Return the length of a line.                                  |\n",
      "| area         | region → real                                                                                                                                                  | Return the area of a region.                                  |\n",
      "| intersects   | line | region × line | region → bool                                                                                                                           | TRUE, if both arguments intersect.                            |\n",
      "| intersection | point | line | region × point | line | region → T, where T is point if point is one of the arguments, otherwise T is the argument having the smaller dimension | Intersection of two spatial objects.                          |\n",
      "| distancescan | rtree × relation × object × int → stream                                                                                                                       | Compute the integer k nearest neigh- bors for a query object. |\n",
      "\n",
      "In this query, the ' ? ' symbol represents a variable, followed by the variable name. The middle of the ' &lt;&gt; ' symbol is the URI that describes the resource address. The ' info:age ' in the above query is a URI shorthand and stands for ' &lt; http://somewhere/peopleInfo # age &gt; '. The FILTER keyword is employed to impose limitations on the outcomes that are retrieved. In addition, RDF is semi-structured data, and different entities in RDF may have distinct properties. SPARQL is capable of querying information that exists in RDF. However, when querying information that does not exist, SPARQL does not show a failure and does not return any results. The OPTIONAL keyword can then be used to signify that the query is optional, indicating that the query will return a result if the entity has the attribute, and a null value otherwise. The FILTER keyword can also be used in conjunction with the OPTIONAL keyword.\n",
      "\n",
      "## 2.2.3 Query language for spatial data\n",
      "\n",
      "The increasing reliance on geographic information systems in many aspects of people's production and life has led to a significant increase in the demand for spatial data query in all walks of life. The popularity of spatial applications has brought great attention to spatial databases [55]. In databases, fundamental data types utilized for the representation and manipulation of spatial objects include point, line, and region. The common operators to query spatial data are shown in Table 2.\n",
      "\n",
      "Mature systems for storing and managing spatial data include Esri's ArcGIS, PostGIS, Google Earth Engine, GRASS GIS, and SECONDO [56]. As an illustration, SECONDO is a freely available platform created for the purpose of organizing and examining spatial and temporal data. The basic commands of SECONDO are as follows.\n",
      "\n",
      "query &lt; value expression &gt; . The command evaluates the given value expression and subsequently displays the result to the user.\n",
      "\n",
      "let &lt; identifier &gt; = &lt; value expression &gt; . The command initially evaluates the provided value expression in a manner analogous to the preceding command. In contrast to the previous command, the results of the evaluation are not immediately displayed but rather stored in an object named identifier . If the object already exists in the database, the command will result in an error.\n",
      "\n",
      "delete &lt; identifier &gt; . The command removes the object named identifier from the current database, and is typically utilized in conjunction with the second command.\n",
      "\n",
      "When an expert or system developer writes the executable query language for SECONDO, one needs to comprehensively understand the intricate relationship between data flow and operators. The rel2stream operator transforms a relation into a stream of tuples, as shown in Figure 4. The stream2rel operator, in contrast, converts a stream of tuples into a relation. Among the fundamental operators of SECONDO, the filter operator is the most\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "city (Name:String, GeoData:Region)\n",
      "\n",
      "Figure 4: Functions of the operators rel2stream, stream2rel and filter in SECONDO\n",
      "\n",
      "frequently utilized. Similar to the SELECT keyword in SQL, the function of the filter operator is to extract information from data that satisfies specific conditions. The SELECT keyword operates on a two-dimensional table structure to query, filter, and project data by specifying columns and conditions. The filter operator works on a stream of tuples, followed by a filter condition. The tuples that match the condition are then collected and outputted as a stream. For example, the following executable language will output all information about Nanjing in the relation city in SECONDO.\n",
      "\n",
      "query city rel2stream filter [.Name = 'Nanjing'] stream2rel;\n",
      "\n",
      "During the execution of the query, the rel2stream operator first transforms the relation city into a stream of tuples, then the filter operator extracts the tuple named ' Nanjing ' from the stream, and finally the stream2rel operator converts the tuple into a relation from the stream.\n",
      "\n",
      "## 2.3 Intermediate representation languages\n",
      "\n",
      "The intermediate representation language in NLIDB is designed to accommodate the semantic discrepancies and diversity between natural language and executable database language, thus improving the translation accuracy, flexibility and maintainability of the system [7]. The intermediate representation serves as a translator between natural language and executable language, mapping complex natural language structures to a unified semantic representation for the purpose of efficient subsequent query processing and execution. By decoupling NLQ from the underlying database query language, the intermediate representation language makes the NLIDB system flexible, portable, and adaptable to various database types and query requirements. The design of the intermediate representation considers several factors, such as:\n",
      "\n",
      "- (i) The intermediate representation should convey the query request that the user wishes to submit to the database, rather than the full meaning of the user's input.\n",
      "- (ii) To facilitate subsequent translation into the executable language of the database, the intermediate representation should be unambiguous.\n",
      "- (iii) To make re-development easier, the intermediate representation should be reusable.\n",
      "\n",
      "Popular intermediate representations are parse trees [78], first-order logic [121], OQL [113], query sketch [153], SemQL [53], and NatSQL [50].\n",
      "\n",
      "Parse tree. The syntactic structure of a query in natural language is closely tied to the design of a parse tree. The tree structure is typically applied to represent the hierarchical and structural relationships of the query. Each node in a parse tree indicates a grammatical unit (e.g., phrase, word group, and vocabulary), while edges indicate grammatical relations (e.g., modification and conjunction) between these grammatical units. The nodes and edges on the parse tree can be labeled with semantic information to identify the semantic roles and constraints present in the query, providing important information for subsequent query processing.\n",
      "\n",
      "First-order logic. When transforming an NLQ into first-order logic (FOL), words and phrases in the natural language are first mapped to predicates, constants, variables, and logical connectives in FOL to represent entities,\n",
      "\n",
      "attributes, and relations in the query. Subsequently, on the basis of the syntactic structure of the NLQ, the syntax tree or syntax graph of the FOL representation is constructed to capture the semantic relations and logical structures in the query. Finally, the topics, conditions, and operations in the query are identified and converted into logical expressions in FOL to denote the constraints and operational requirements of the query.\n",
      "\n",
      "When converting the natural language query ' Find the names and salaries of all employees older than 30. ' into a first-order logic representation, predicates and constants are defined as follows.\n",
      "\n",
      "Employee x ( ) : x is an employee\n",
      "\n",
      "Name x,n ( ) : the name of employee x is n\n",
      "\n",
      "Age x, a ( ) : the age of employee x is a\n",
      "\n",
      "Salary x, s ( ) : the salary of employee x is s\n",
      "\n",
      "The query condition is expressed as ∀ x Employee x ( ( ) ∧ Age x, a ( ) ∧ a &gt; 30) . The query result is expressed as ∃ n, s ( Name x,n ( ) ∧ Salary x, s ( )) . The complete first-order logic representation is obtained by combining the condition and result of the query.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "OQL is built on an ontology knowledge graph, where words and phrases in natural language queries are associated with concepts, attributes and relations within the ontology knowledge graph. The semantic information of natural language queries is captured through semantic representations and query patterns to effectively interact with the database. OQL grammars permit the expression of complex aggregation, union and nested queries. OQL queries operate upon individual concepts, with each concept being assigned an alias as specified in the FROM clause of the query.\n",
      "\n",
      "Query sketch is a form of SQL with natural language hints. Taking the NLQ ' Find the number of papers in OOPSLA 2010. ' as an example, the query sketch is as follows.\n",
      "\n",
      "SELECT count(?[papers]) FROM ??[papers] WHERE ? = 'OOPSLA 2010';\n",
      "\n",
      "In the query sketch, the symbols ' ?? ' and ' ? ' represent an unspecified table and an unspecified column, respectively. Hints for the corresponding gaps are indicated by words enclosed in square brackets. As an illustration, the first hint in the sketch suggests that the symbol ' ? ' has a similar semantic meaning to the term papers .\n",
      "\n",
      "SemQL is designed as a tree structure that not only constrains the search space during synthesis, but also maintains the same structural characteristics as SQL. In SemQL queries, the GROUP BY, HAVING, and FROM clauses in SQL are removed, and the conditions from the WHERE and HAVING clauses are consistently represented in the Filter sub-tree. Furthermore, in the later inference phase, domain knowledge is utilized to deterministically infer implementation details from SemQL queries. For instance, the columns included the GROUP BY clause of SQL are typically present in the SELECT clause.\n",
      "\n",
      "NatSQL retains the core functionality of SQL while streamlining the structure of SQL to align more closely with the syntax of natural language. NatSQL keeps only the SELECT, WHERE, and FROM clauses, omitting the JOIN ON, HAVING, and GROUP BY clauses. Additionally, NatSQL does not require nested sub-queries or aggregation operators, and employs a single SELECT clause. In the case of the natural language query ' Which film has more than 5 actors and less than 3 in the inventory? ', the SQL and NatSQL are as follows.\n",
      "\n",
      "SQL: SELECT T1.title FROM film AS T1 JOIN film actor AS T2 ON T1.film id = T2.film id GROUP BY T1.film id HAVING count(*) &gt; 5 INTERSECT SELECT T1.title FROM film AS T1 JOIN inventory AS T2 ON T1.film id = T2.film id GROUP BY T1.film id HAVING count(*) &lt; 3;\n",
      "\n",
      "NatSQL: SELECT film.title WHERE count(film actor.*) &gt; 5 and count(inventory.*) &lt; 3;\n",
      "\n",
      "Table 3: Natural language preprocessing for NLIDBs\n",
      "\n",
      "| NLIDB            |   Year | Underlying datatype   | Segmentation   | Part of speech   | NER   | Dictionary generation   | Regular expression   | Dependency parsing   | Word Embedding   | Pattern Linking   |\n",
      "|------------------|--------|-----------------------|----------------|------------------|-------|-------------------------|----------------------|----------------------|------------------|-------------------|\n",
      "| PRECISE [107]    |   2003 | relational data       | ✓              | ✓                | ✓     | ✓                       |                      |                      |                  |                   |\n",
      "| Querix [69]      |   2006 | ontology              | ✓              | ✓                |       | ✓                       |                      |                      |                  |                   |\n",
      "| QuestIO [27]     |   2008 | ontology              | ✓              | ✓                |       | ✓                       |                      |                      |                  |                   |\n",
      "| gAnswer [60]     |   2013 | RDF data              |                |                  |       | ✓                       |                      |                      |                  |                   |\n",
      "| MEANS [1]        |   2015 | RDF data              | ✓              |                  | ✓     |                         |                      |                      |                  |                   |\n",
      "| NL2CM [6, 5]     |   2015 | RDF data              | ✓              | ✓                |       |                         |                      | ✓                    |                  |                   |\n",
      "| NL2TRANQUYL [16] |   2015 | relational data       |                |                  |       |                         |                      | ✓                    |                  |                   |\n",
      "| ATHENA [113]     |   2016 | relational data       | ✓              | ✓                | ✓     |                         |                      | ✓                    |                  |                   |\n",
      "| SQLizer [153]    |   2017 | relational data       | ✓              | ✓                | ✓     |                         |                      |                      |                  |                   |\n",
      "| TEQUILA [64]     |   2018 | RDF data              | ✓              | ✓                | ✓     |                         |                      |                      |                  |                   |\n",
      "| MyNLIDB [28]     |   2019 | relational data       | ✓              | ✓                |       |                         |                      | ✓                    |                  |                   |\n",
      "| IRNet [53]       |   2019 | relational data       |                |                  |       |                         |                      |                      | ✓                | ✓                 |\n",
      "| NLMO [145]       |   2020 | moving objects        | ✓              | ✓                | ✓     | ✓                       | ✓                    |                      |                  |                   |\n",
      "| NALMO [144, 143] |   2021 | moving objects        | ✓              | ✓                | ✓     | ✓                       | ✓                    |                      |                  |                   |\n",
      "| NALSD [88]       |   2023 | spatial data          | ✓              | ✓                | ✓     | ✓                       |                      |                      |                  |                   |\n",
      "| NALSpatial [89]  |   2023 | spatial data          | ✓              | ✓                | ✓     | ✓                       |                      |                      |                  |                   |\n",
      "| xDBTagger [132]  |   2024 | relation data         | ✓              | ✓                |       |                         |                      |                      | ✓                | ✓                 |\n",
      "\n",
      "## 3 Generation of executable database languages\n",
      "\n",
      "The generation of executable database languages can be divided into three stages: (i) natural language preprocessing , (ii) natural language understanding , and (iii) natural language translation . In stage (i), the system performs a preliminary analysis of the raw natural language query in order to prepare for the subsequent stage of natural language understanding. In stage (ii), the system performs semantic parsing and understanding of the preprocessed natural language query to extract the semantic details and intent of the query. In stage (iii), the system converts the comprehended natural language into a language that can be executed within the database.\n",
      "\n",
      "## 3.1 Natural language preprocessing\n",
      "\n",
      "Prior to the semantic understanding and translation of natural language queries, preprocessing is performed using traditional and data-driven methods. In order to preprocess natural language queries, the recently developed NLIDBs utilize techniques as illustrated in Table 3.\n",
      "\n",
      "The preprocessing process of many NLIDBs commences with the construction of a dedicated data dictionary for the domain. The extraction process of domain knowledge exerts a profound influence on the portability of the system. In addition, the semantic parsing component needs to accurately comprehend NLQ with the assistance of the dictionary, and the extraction process of domain knowledge will impact the availability of the NLIDB. The primary goal of the extraction technique is to minimize the burden on system users while enhancing the capacity to automatically generate a dictionary. The extraction process is primarily reliant on stemming and synonym techniques. The system then needs to perform word segmentation and part-of-speech tagging on the input natural language. This process necessitates the utilization of natural language processing tools. When choosing the tool, the high accuracy of the segmentation and part-of-speech tagging results should be considered first, followed by the speed of processing. Furthermore, the query must be oriented to database information, and the relevant\n",
      "\n",
      "Table 4: Rules for parsing natural language queries\n",
      "\n",
      "| Rules                | Typical NLIDBs                                                                                                                                  |\n",
      "|----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| Parse tree           | PRECISE [107], NaLIX [84, 85], Querix [69], DaNaLIX [81], gAnswer [60], NaLIR [78, 77, 79], NL2TRANQUYL [16], Unnamed method [65], MyNLIDB [28] |\n",
      "| Ontology             | QuestIO [27], ATHENA [113], FINESSE [63], Unnamed method [41], CNL- RDF-Query [58], ATHENA++ [115], Unnamed method [4]                          |\n",
      "| Semantic graph       | Unnamed method [168], MEANS [1], NL2CM [6, 5]                                                                                                   |\n",
      "| Template matching    | SQLizer [153], Unnamed method [3], LogicalBeam [11]                                                                                             |\n",
      "| Pattern matching     | SODA [15]                                                                                                                                       |\n",
      "| Context-free grammar | TR Discover [121]                                                                                                                               |\n",
      "| Semantic grammar     | Unnamed method [49]                                                                                                                             |\n",
      "\n",
      "statements used in the query request are closely related to the database to be used. Therefore, part-of-speech tagging is often employed in conjunction with named entity recognition and data dictionary.\n",
      "\n",
      "Traditional preprocessing methods rely on predefined rules and grammars, involving techniques including NER, regular expressions, and dependency parsing. NLMO performs segmentation and entity recognition using a natural language processing toolkit spaCy, and sets regular expressions for temporal information extraction. ATHENA utilizes the TIMEX annotator to detect all temporal intervals mentioned in the text, and the Stanford Numeric Expressions annotator to pinpoint all tokens containing numerical values. ATHENA employs the Stanford Dependency Parser to identify the dependency relationship in the context of the GROUP BY clause. PRECISE utilizes the Charniak parser for the precise parsing of questions and the extraction of token relationships from the resulting parse tree. NL2CM employs dependency parsing and part-of-speech tagging techniques. NL2TRANQUYL analyzes the input natural language using the Stanford Parser, resulting in constituency and dependency parses.\n",
      "\n",
      "Data-driven preprocessing methods depend on large-scale data and machine learning models, and the techniques used include word embedding and pattern linking. Word2Vec and GloVe are word embedding models that are able to represent words as points in a sequential vector space, thereby capturing the semantic relationships between words. These vectors can be employed for calculating semantic similarity and extracting features. xDBTagger utilizes a pre-trained word embedding model to convert tokens into a 300-dimensional vector representation. IRNet performs schema linking by connecting the natural language with the database schema, aiming to identify the specific columns and tables referenced in the natural language. The columns are then assigned different types according to the manner mentioned in the question.\n",
      "\n",
      "## 3.2 Natural language understanding\n",
      "\n",
      "Three principal technical approaches to understand natural language are (i) rule-based , (ii) machine learningbased , and (iii) hybrid . Based on the techniques, the process of natural language understanding for the recently developed NLIDBs is summarized. We provide three timelines describing the research on rule-based, machine learning-based, and hybrid approaches, as shown in Figure 5.\n",
      "\n",
      "## 3.2.1 Rule-based methods\n",
      "\n",
      "The semantic parsing of mature NLIs is predominantly based on rules. The systems require specific rules to parse natural language queries, including parse tree, ontology, semantic graph, template matching, pattern matching, context-free grammar, and semantic grammar, as shown in Table 4. Rule-based systems can only deal with knowledge bases in fixed domains and are generally not portable to other knowledge bases. In order to enhance the accuracy of semantic understanding, systems are typically constrained by limitations in their ability to support\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "| PRECISE   | NaLIX                | Querix   | DaNaLIX           | QuestIO           | QUICK             |                                |                                | (to                            | be continued)                  |                                |                                |                                |\n",
      "|-----------|----------------------|----------|-------------------|-------------------|-------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|\n",
      "| 2003      | 2005                 | 2006     |                   | 2007              | 2008              |                                | 2009                           |                                |                                |                                |                                |                                |\n",
      "|           |                      |          | TR Discover MEANS | TR Discover MEANS | TR Discover MEANS |                                |                                |                                |                                |                                |                                |                                |\n",
      "|           |                      |          | NL2TRANQUYL       | NL2TRANQUYL       | NL2TRANQUYL       | SQLizer                        |                                | DialSQL                        |                                | ezNL2SQL NLMO                  |                                |                                |\n",
      "|           | gAnswer              | NaLIR    |                   | NL2CM             | ATHENA            |                                | NLQ/A                          | TEQUILA                        | MyNLIDB                        | ATHENA++                       | EXAQT                          |                                |\n",
      "|           |                      | 2014     | 2013              |                   | 2015              | 2016                           | 2017                           | 2018                           | 2019                           | 2020                           | 2021                           |                                |\n",
      "|           |                      |          |                   | (a)               | Rule-based        | methods                        | methods                        | methods                        | methods                        | methods                        | methods                        | methods                        |\n",
      "|           |                      |          |                   |                   |                   | BiBERT-SQL                     |                                |                                |                                |                                |                                |                                |\n",
      "| Seq2SQL   | SyntaxSQLNet DialSQL |          | IRNet             | RYANSQL           |                   | ValueNet SP-CNN                |                                | Auto-Query                     | DTE IKnow-SQL                  | SV2-SQL SpatialNLI             |                                |                                |\n",
      "| 2017      | 2018                 |          | 2019              |                   | 2020              |                                | 2021                           | 2022                           | 2023                           |                                | 2024                           |                                |\n",
      "|           |                      |          |                   | (b)               |                   | Machine learning-based methods | Machine learning-based methods | Machine learning-based methods | Machine learning-based methods | Machine learning-based methods | Machine learning-based methods | Machine learning-based methods |\n",
      "|           |                      |          |                   |                   |                   |                                |                                |                                | NALSD                          |                                |                                |                                |\n",
      "|           |                      |          |                   |                   |                   |                                |                                |                                | NALSpatial                     |                                |                                |                                |\n",
      "|           |                      |          |                   |                   |                   |                                |                                |                                | GENSQL                         |                                |                                |                                |\n",
      "|           |                      |          |                   |                   |                   |                                |                                |                                | CatSQL                         |                                |                                |                                |\n",
      "|           |                      |          |                   |                   |                   |                                |                                | Veezoo                         |                                |                                |                                |                                |\n",
      "|           | TypeSQL              |          |                   |                   |                   |                                |                                |                                | GAR                            |                                | xDBTagger                      |                                |\n",
      "|           |                      |          |                   |                   |                   | NALMO                          |                                |                                |                                |                                |                                |                                |\n",
      "\n",
      "(c) Hybrid methods based on rule and machine learning\n",
      "\n",
      "Figure 5: Timelines of the research progress of techniques for understanding natural language\n",
      "\n",
      "natural language features, such as grammar and vocabulary [144]. PRECISE [107] elucidates the notion of semantic tractability and delineates a specific subset of natural language that can be accurately converted into SQL. However, natural language queries that cannot be processed semantically will be rejected by PRECISE. NaLIX [84, 85] restricts natural language queries to a regulated subset according to a predetermined grammar. DaNaLIX [81] is constructed on NaLIX and employs domain knowledge for query translation. Domain knowledge is encapsulated within a collection of regulations that map terms with domain meaning in the parse tree to terms that can be understood by a generic system such as NaLIX. The domain adapter within DaNaLIX assesses the current domain expertise and modifies the parse tree with related rules. NaLIR [78, 77, 79] identifies nodes within the language parse tree that have the potential to correspond to SQL components resulting from the preprocessing step, and represents semantic coverage as a subset of the parse tree. Such a tree explicitly corresponds to SQL and serves as a query tree, which mediates between NLQ and SQL. To comprehend the challenge of integrating individual and collective knowledge, NL2CM first uses RDF to represent individual and general knowledge. Individual expression detectors are then used to distinguish between individual and general query components, which are created through a declarative selection schema in conjunction with a specialized vocabulary. ATHENA uses domain-specific ontology to transform the natural language input into an intermediate language on the ontology. The intermediate language is then used to describe the semantic entities in the domain, as well as the relationships between the entities. Ontology provides richer semantic information than relational schema, including inheritance\n",
      "\n",
      "and membership. By reasoning about the ontology, ATHENA demonstrates the capability to effectively discern and capture the intentions of users. However, ATHENA is highly sensitive to changes and interpretations of user queries [99]. Both the NLIDB system described in the paper [116] and ATHENA++ [115] are extensions of ATHENA. They combine linguistic analysis with deep domain reasoning to translate complex join and nested SQL. NL2TRANQUYL [16] is a system designed for the planning of journeys within a complex multi-modal transportation system, taking into account a number of constraints, including the minimization of journey time, distance and cost. NL2TRANQUYL utilizes the ontology comprising a range of concepts to store and model related information, and generates knowledge graphs to determine the relationships between them. To discover and process temporal information in NLQ, TEQUILA decomposes the detected temporal problems and rewrites the generated sub-problems. These papers [60, 168] utilize the Stanford Parser to generate dependency trees and extract semantic relations from the parsed data. Subsequently, a semantic query graph is constructed by connecting these semantic relations to depict the user's query intent. Querix [69] examines the syntax of natural language using a syntactic analyzer, which is only effective when the natural language components are complete. Incomplete components may result in inaccurate results, which could compromise the accuracy of the final results.\n",
      "\n",
      "An optimal NLIDB enables users to formulate intricate queries on the database system and retrieve precise information with minimal exertion. Consequently, a number of systems incorporate user interaction during the process of comprehending semantics. NaLIX and DialSQL [54] adjust the query during following user engagements to revise the parse tree, however, the revision frequently necessitates a high number of user interactions. DaNaLIX acquires domain knowledge through the interaction that occurs between the user and the system in an automated manner. In addition to elucidating the user on the query processing procedure, NaLIR also presents a spectrum of interpretations for the user to select from, thus alleviating the user's need to address potential misunderstandings. NaLIR is capable of detecting the parse tree, thereby enabling users to modify the parse tree directly, rather than reformulating the natural language query. NaLIR can provide recommendations to users for revising their queries in instances where the natural language queries fall beyond the semantic boundaries. QUICK [162] improves user interactions by utilizing keyword search to enrich the expressiveness of semantic queries. In practical application, QUICK assists users in determining the specific intent behind natural language through a series of iterative refinement steps following the initial submission of a keyword-based question. NLQ/A [166] enhances the user interaction component in order to more effectively address the issue of ambiguity. SPARKLIS [46] employs a sequential process consisting of three stages in order to guarantee the thoroughness of user input during searches for concepts, entities or modifiers. While interacting with the system may result in the user feeling constrained, slowed down, and less natural when entering a query, SPARKLIS provides guidance and safety through intermediate answers and suggestions [2]. In order to reduce user involvement during the disambiguation process, ATHENA utilizes the extensive semantic data within the ontology to produce a prioritized list of explanations, and employs a ranking algorithm that is intuitive and relies on ontology metrics to determine the most appropriate explanation.\n",
      "\n",
      "## 3.2.2 Machine learning-based methods\n",
      "\n",
      "As the usage of statistical learning methods continues to expand, there has been a growing interest in conducting semantic analysis on sentences through a variety of forms of supervision. Pasupat and Liang [102] employ question-and-answer format to provide guidance in responding to intricate natural language queries presented within semi-structured tables. The paper [105] represents the inaugural attempt to develop a semantic parsing model through unsupervised learning [66]. Artzi and Zettlemoyer [8] solicit feedback during the conversation to determine the meaning of the user's statements. In a domain where no training examples are available, Wang et al. [146] demonstrate the successful development of a semantic parser. Their approach comprises two key elements: (i) a builder and (ii) a domain-general grammar . Wong and Mooney [150] utilize statistical machine translation technology for the purpose of accomplishing semantic parsing tasks.\n",
      "\n",
      "Table 5: NLIDBs with encoder-decoder frameworks\n",
      "\n",
      "| NLIDB                |   Year | Encoder                                                | Decoder                                         |\n",
      "|----------------------|--------|--------------------------------------------------------|-------------------------------------------------|\n",
      "| DialSQL [54]         |   2018 | Encode dialogue history using RNN networks             | Decode errors and candidate se- lections        |\n",
      "| SyntaxSQLNet [159]   |   2018 | Table-aware column encoder                             | Syntax tree-based decoder                       |\n",
      "| Unnamed method [87]  |   2020 | Encode NLQs and table headers using XLNet [155]        | The parsing layer splices the vec- tor          |\n",
      "| ValueNet [19]        |   2021 | Extension of IRNet's encoder                           | LSTM architecture and multiple pointer networks |\n",
      "| Unnamed method [30]  |   2021 | The encoder of LSTM                                    | The decoder of LSTM                             |\n",
      "| MIE [138]            |   2021 | Multi-integrated encoder with three integrated modules | No decoder                                      |\n",
      "| Auto-Query [100]     |   2022 | The encoder of RATSQL [137]                            | SmBoP [112]                                     |\n",
      "| STAMP [51]           |   2023 | The encoder of T5                                      | The decoder of T5                               |\n",
      "| Unnamed method [154] |   2023 | The encoder of Transformer                             | The decoder of Transformer                      |\n",
      "\n",
      "In recent times, there has been a growing utilization of encoder-decoder frameworks that rely on recurrent neural networks for semantic parsing, as demonstrated in Table 5. Many systems combine machine learning and deterministic algorithms to generate structured languages [93]. This method allows the direct acquisition of the correlation between natural language and the semantic representation, eliminating the need for an intermediate representation like a parse tree [66]. Mapping natural language directly to the semantic representation can reduce the dependence of rule-based semantic parsing models on preset vocabulary, templates, and hand-generated features. Machine learning-based models are not limited to specific knowledge bases or logical formal expressions, thus enabling the implementation of natural language interfaces that support cross-knowledge bases or crosslanguages. Wang et al. [140, 142] propose a cross-domain NLI, which translates the marked natural language into the intermediate representation of the target query type by building a cross-domain multilingual sequenceto-sequence (seq2seq) model. Symbols inserted into the natural language query are utilized to substitute the data elements present in the intermediate query. However, this method is a supervised machine learning model whose effectiveness is closely related to the quality of the training data. To ensure the accuracy of semantic understanding, a substantial quantity of training data must be provided to the model. A number of researchers employ a synthetic data generator as a solution to the challenge of having a restricted amount of training data available. The paper [159] introduces SyntaxSQLNet, which can generate NLQ data sets for cross-domain SQL single-table operations, solely as a means of augmenting the training set. The method outlined in the paper [149] encompasses single-table and multi-table join queries of SQL, and can be utilized as either an augmentation or as a standalone training data set. In terms of model training, the rule-based method is more effective than the neural networkbased method, which requires more training parameters and takes longer to establish the model, consuming more memory space.\n",
      "\n",
      "One of the earliest examples of machine learning-based systems is demonstrated in the paper [161]. This work utilizes a deterministic shift-reduce parser and develops a learning algorithm called CHILL to learn the governing rules of parsing on the basis of inductive logic programming techniques. The corpus is trained using the CHILL method to build the parser. Instead of learning dictionaries, this approach assumes that a dictionary is created in advance that pairs words with semantic content rather than grammar. The paper [163] translates the meaning of natural language sentences into lambda calculus encoding. The paper [163] outlines a learning algorithm whose input is a collection of sentences identified as lambda calculus expressions, and applies the method to the task of learning NLIDB to build a parser. While providing considerable flexibility, encoder-decoder frameworks frequently lack the ability to interpret and understand combinations of meaning [66]. The method employed by Cheng et al. [25] involves the construction of the intermediate structure in two stages, which facilitates a comprehensive understanding of the model's learning process. Similarly, the paper [39] also produces an intermediate\n",
      "\n",
      "template that presents the final output in a preliminary format, thereby facilitating the subsequent decoding process. Yin and Neubig [157] address the issue of insufficient training data by incorporating explicit constraints for decoders through the utilization of target language syntax. The approach enables the model to concentrate on parsing, directed by established grammar rules. Xiao et al. [151] utilize the grammar model as prior knowledge, requiring the creation of a derivation tree while adhering to the constraints imposed by the grammar. The approach in the paper [74] can significantly outperform the Seq2Tree model from the aforementioned paper [38] by verifying that the decoder's forecasts adhere to the type constraints outlined in the type constraint grammar. This suggests that satisfying type constraints and good formatting are equally important when generating logical expressions. SpatialNLI [80, 141] is a natural language interface for the spatial field that employs the seq2seq model to understand the semantic structure of natural language, while utilizing an external spatial understanding model to identify the meaning of spatial entities. Subsequently, the spatial semantics learned from the spatial understanding model are integrated into natural language problems, thereby reducing the necessity of acquiring specific spatial semantics. SpatialNLI represents a pioneering system that integrates an external spatial semantic comprehension model to optimize the effectiveness of the principal seq2seq model. The paper [129] uses a tree model to analyze the target entity in natural language, and employs a tree-structured LSTM to understand the problem. The paper [62] adjusts the neural sequence model to directly convert natural language into SQL, thus circumventing the intermediate query language representation. Then the user feedback is utilized to mark error queries, which are directly used to improve the model. The complete feedback loop does not necessitate the use of any intermediate language representation and is not limited to a specific domain. This method offers the benefit of enabling the rapid and straightforward construction of a semantic parser from scratch, and the performance of the parser improves as user feedback increases. The encoder of ValueNet [19] is an extension of the encoder of IRNet [53], receiving not only details regarding the database schema, but also extracted value candidates from the database content.\n",
      "\n",
      "## 3.2.3 Hybrid methods based on rule and machine learning\n",
      "\n",
      "Hybrid methods integrate rules and machine learning techniques to capitalize on the respective strengths of each, thereby enhancing the ability of the system to understand and process NLQs [72]. Table 6 enumerates the representative systems that employ the hybrid approach. Hybrid approaches are highly flexible and adaptable, as they can utilize rules for tasks with explicit rules as well as machine learning models for complex and ambiguous semantic tasks. In addition, hybrid methods can flexibly incorporate new rules or train new machine learning models as needed to accommodate the requirements of diverse domains and tasks, and are highly scalable [135]. TypeSQL [158], like SQLNet [152], is built on sketches and formats translation tasks as slot-filling problems. The difference is that TypeSQL employs type information to enhance the understanding of entities and numbers in NLQs. TypeSQL assigns a type to each word, such as entity, column, number and date, within the knowledge graph. Subsequently, two bidirectional LSTM networks are utilized to encode the words in the NLQ with the corresponding column names and types. Finally, the LSTM output hidden states are leveraged to forecast the slot values within the SQL sketch. NALMO is a natural language interface for moving objects. To understand NLQs, NALMO employs an entity extraction algorithm to obtain entity information, including time, location and the number of nearest neighbors. A pre-constructed corpus is then trained using LSTM to determine the query type. Veezoo [75] uses a range of techniques, including temporal expression parsing, entity linking, and relation extraction, to identify key information in NLQs. The information is then extended and combined using predefined rules to generate multiple candidate intermediate representations. Finally, Veezoo utilizes a machine learning model to score these intermediate representations in order to select the most probable interpretation of the NLQ. The process of data preparation in GAR [42] commences with a collection of sample SQLs that are tailored to a specific database. For a given NLQ, GAR searches for the NLQs generated during data preparation and employs a learning-to-rank model to identify the most relevant query, which is then used to obtain the translation result.\n",
      "\n",
      "Table 6: NLIDBs based on rules and machine learning techniques\n",
      "\n",
      "| NLIDB               |   Year | Rule                                                              | Machine learning technique                                       |\n",
      "|---------------------|--------|-------------------------------------------------------------------|------------------------------------------------------------------|\n",
      "| Unnamed method [52] |   2012 | Generate candidate SQLs via rules and heuristic weighting schemes | Reorder candidate SQLs using the SVM sorter                      |\n",
      "| TypeSQL [158]       |   2018 | Assign a type to each word to under- stand the entity             | Encode using bidirectional LSTM                                  |\n",
      "| NALMO [144, 143]    |   2021 | Semantic grammar and template matching                            | Identify the query type using LSTM                               |\n",
      "| Veezoo [75]         |   2022 | Knowledge graph                                                   | Score intermediate representations using machine learning models |\n",
      "| GAR [42]            |   2023 | Parse tree                                                        | Find the matching expression for NLQ using a learn-to-rank model |\n",
      "| GENSQL [44]         |   2023 | Capture the structure of the database with sample SQLs            | Find the matching expression for NLQ using a learn-to-rank model |\n",
      "| CatSQL [48]         |   2023 | Template matching                                                 | The decoder of Transformer. Train the model using Adam [71]      |\n",
      "| NALSpatial [89]     |   2023 | Semantic grammar and template matching                            | Identify the query type using LSTM                               |\n",
      "| NALSD [88]          |   2023 | Semantic grammar and template matching                            | Identify the query type using LSTM                               |\n",
      "| xDBTagger [132]     |   2024 | Semantic graph                                                    | Bidirectional recurrent neural network                           |\n",
      "\n",
      "The learning-to-rank model learns to rank the semantic similarities from NLQs to generated NLQs and then finds the best matching expression for a given NLQ. GENSQL [44], a generative NLIDB, utilizes a given example SQL from the database (e.g., from query logs) to comprehend the unique structure and semantics of a given database, thereby guaranteeing precise translation outcomes. The fundamental model used in GENSQL for converting natural language to SQL is GAR. CatSQL [48] is a method for the generation of SQL that makes use of sketches. In addition, semantic constraints are merged into the neural network-driven SQL generation procedure for semantic refinement. CatSQL sketches are templates with keywords and slots. CatSQL employs a deep learning algorithm to populate vacant slots in order to generate the ultimate SQL. The deep learning algorithm is developed to focus on the generation of essential NLQ-related information, with the objective of filling the gaps without requiring the explicit generation of keywords like SELECT, FROM, and WHERE.\n",
      "\n",
      "Hybrid approaches based on rules and machine learning offer several advantages, including flexibility, accuracy, and scalability. Nevertheless, such approaches present certain challenges, such as complexity, dependence on data, and tuning difficulties [68]. Hybrid methods require the simultaneous management and maintenance of rule engines and machine learning models, including rule definition, feature engineering, and model training, and thus have high complexity [52]. Furthermore, the rules and machine learning models utilized in hybrid approaches may encounter parameter tuning problems, which necessitate a significant investment of time and effort for optimization and debugging, thus increasing the costs associated with the development and maintenance of the system. During the design and implementation of natural language interfaces, it is essential to take a comprehensive view of the advantages and challenges involved, and to make trade-offs and choices in accordance with the specific needs.\n",
      "\n",
      "## 3.3 Natural language translation\n",
      "\n",
      "The natural language translation stage employs the semantic information derived from the natural language understanding stage, subsequently integrating the underlying structure of the database to transform the input natural language into the corresponding executable language. A prevalent approach for translation is to employ complex algorithms and machine learning models to generate structured language based on the domain knowledge of the underlying database and the semantic representation of natural language [83]. Most established NLIDBs\n",
      "\n",
      "Figure 6: General build process for SQL\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "construct queries by query combination, mapping key information expressed in natural languages to corresponding components in structured languages. We examine the process of natural language translation in recently developed NLIDBs, and summarize the general construction process of executable languages for relational and spatio-temporal databases.\n",
      "\n",
      "The general build process for SQL is illustrated in Figure 6 and further elaborated in the subsequent two cases. (i) When querying a single relation, it is only necessary to place the database elements matched by NLQ in the correct positions in the SELECT, FROM, and WHERE parts, respectively. Then, SQL can be composed directly.\n",
      "\n",
      "(ii) When querying multiple relations, the join condition and the names of the participating relations need to be included in the WHERE and FROM clauses, respectively. Additionally, it is necessary to determine whether the join path is unique. If only one join path is available, SQL can be generated directly. Otherwise, a query is typically generated for each possible join path, and then the most probable one is selected according to the corresponding algorithm.\n",
      "\n",
      "In recent years, there has been significant interest in NLI for spatio-temporal databases [26]. Temporal and spatial concepts are derived from the natural language description using symbolic representations in order to depict spatio-temporal features and their relationships [14, 106]. Due to the particularity and expressiveness of spatiotemporal problems, executable query languages over spatio-temporal databases are quite different from SQL. Consequently, the method employed for the construction of SQL cannot be directly applied to the generation of executable languages over spatio-temporal databases. The general process for the construction of an executable language for a spatio-temporal database is shown in Figure 7. Preliminary parsing of the input natural language query is performed to obtain semantic information including key entities and query types. Subsequently, the operators necessary to construct the executable language are determined according to the type of query. Finally, key entities and operators are combined according to certain rules to compose an executable database language. Taking the range query over spatial data as an example, the key entities involved include spatial relations and locations. The operator intersects will return all objects in the relation that intersect the location if the spatial attribute of the relation and the data type of the location are both line or region . Conversely, the operator intersects will return all objects in the relation that lie within the location if the spatial attribute of the relation is point and the data type of the location is region .\n",
      "\n",
      "Different DBMSs and structured languages offer a range of clauses and operators for various queries. ATHENA employs a mapping strategy that correlates the ontology with the database schema in order to convert the intermediate query language utilized in the ontology into SQL. The system described in the paper [63] extends ATHENA to access multiple structured backends, which is achieved through the automated translation of the intermediate\n",
      "\n",
      "Figure 7: General construction process for executable languages over spatio-temporal data\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "query language into the specific structured query language utilized by these backend stores. NLPQC [124] is capable of processing queries formulated using predefined domain-specific templates. Querix selectively isolates specific elements from the syntactic tree in order to align acquired knowledge with the knowledge base, thereby obtaining the final outcome. NL2CM leverages crowd intelligence by converting audience queries into OASSISQL (an extended version of SPARQL). NaLIR utilizes the structure of the user-validated query tree to produce the suitable structure in the SQL statement and determine the join path. In order to ascertain whether the target SQL contains aggregate functions or sub-queries, NaLIR initially identifies function nodes or quantifier nodes in the query tree and subsequently generates SQL statements based on the identified conditions. The paper [52] employs lexical dependencies found in the question and database metadata to build a reasonable collection of SELECT, WHERE, and FROM clauses that enhance the quality of meaningful joins. The paper [52] combines clauses through a rule and heuristic weighting scheme, and then generates a sorted list of candidate SQLs, demonstrating that full semantic interpretation can be avoided by relying on a simple SQL generator. This method can be employed iteratively to address intricate issues necessitating nested SELECT commands. Finally, this paper [52] applies the re-ranker to reorder the list of questions and SQL candidate pairs with the aim of enhancing the accuracy of the system. TEQUILA uses a standard KB-QA system to evaluate the sub-questions from the semantic understanding part individually. The results of the sub-questions are then combined with the reasoning to calculate the answer to the full question. NL2TRANQUYL translates English requests into formal TRANQUYL [17] queries using the knowledge graph generated by the semantic comprehension component. The traffic query language TRANQUYL for travel planning follows the conventional SQL structure of ' SELECT, FROM, WHERE '. NALMO supports five distinct types of moving object queries, including (i) time interval queries , (ii) range queries , (iii) nearest neighbor queries , (iv) trajectory similarity queries , and (v) join queries . In the query translation process, NALMO first constructs a corpus comprising the five query types, collectively referred to as MOQ. Then the LSTM neural network is used for training, resulting in a model that is capable of accurately identifying the specific type of query. Finally, the appropriate operators are selected according to the query type, and the entity information extracted by the semantic parsing component is combined to build the executable language for SECONDO.\n",
      "\n",
      "## 4 NL2SQL benchmarks\n",
      "\n",
      "We presents 11 frequently used benchmarks for transforming NLQ into SQL and three evaluation metrics, exploring the methods for generating new benchmarks.\n",
      "\n",
      "## 4.1 Existing benchmarks\n",
      "\n",
      "The details of NLQ and executable language pairs for common domains are presented in Table 7. The majority of existing benchmarks are utilized in the domain of relational databases to transform natural language query into SQL (NL2SQL). The comparison of fields and types of SQL supported by the benchmarks for NL2SQL is shown in Table 8. We can conclude that GeoQuery and Spider support the most types of SQL, while WikiSQL supports\n",
      "\n",
      "Table 7: Examples of NLQ and executable language pairs for common domains\n",
      "\n",
      "| Domain              | Examples of NLQ and executable language pairs                                                                                                                                                                                                                                                                                             |\n",
      "|---------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| Relational database | NLQ: How many CFL teams are from York College? SQL: SELECT COUNT CFL Team FROM CFLDraft WHERE College = 'York'                                                                                                                                                                                                                            |\n",
      "| Spatial domain      | NLQ1: What is the population of San Antonio? Lambda expression: answer(A,population(B,A),const(B,cityid(San Antonio))) NLQ2: Could you tell me what parks are in the center? Executable language: query park feed filter [.GeoData ininterior center] consume;                                                                            |\n",
      "| Moving Objects      | NLQ: Where did the train 7 go at 8am? Executable language: query Trains feed filter [.Id = 7] filter [.Trip present [const instant value '2020-11- 20-8:00']] extend [Pos: val (.Trip atinstant [const instant value '2020-11-20-8:00'])] project [Id, Line, Pos] consume;                                                                |\n",
      "| Trip planning       | NLQ: Can I walk to 300 W. Humboldt Blvd. by 4:00 p.m.? TRANQUYL: SELECT ∗ FROM ALL TRIPS(user.current location, 300 W. Humboldt Blvd.) AS t WITH MODES pedestrian WITH CERTAINTY .78 WHERE ENDS(t) ≤ 4:00 p.m. MINIMIZE DURATION(t)                                                                                                       |\n",
      "| Crowd mining        | NLQ: What are the most interesting places near Forest Hotel, Buffalo, we should visit in the fall? OASSIS-QL: SELECT VARIABLES $x WHERE { $x instanceOf Place. $x near For- est Hotel, Buffalo, NY } SATISFYING { $x hasLabel 'interesting' } ORDER BY DESC(SUPPORT) LIMIT 5 AND { [] visit $x. [] in Fall } WITH SUPPORT THRESHOLD = 0.1 |\n",
      "\n",
      "only the simple select query. The queries in WikiSQL and Spider cover a multitude of domains. In recent years, GeoQuery, MAS, WikiSQL and Spider have been employed with considerable frequency.\n",
      "\n",
      "The details of popular benchmarks are shown in Table 9. Early data sets consist of only one domain and one database, such as ATIS, Restaurant and GeoQuery. In contrast, the latest data sets, for example WikiSQL and Spider, contain multiple domains and several databases with larger and more diverse NLQs and SQLs.\n",
      "\n",
      "- (i) ATIS (Airline Travel Information System) [108] is a classical data set with a relatively old age, having been introduced by Texas Instruments in 1990. ATIS is built on the relational database Official Airline Guide, comprising 25 tables and 5871 queries written in English. The queries pertain to details regarding flights, ticket prices, destinations, and services available at airports. The queries in ATIS are for the air travel field, including join queries and nested queries, but no grouping and sorting queries. The average length of NLQs and SQLs in ATIS is approximately 11 and 67 words, respectively. Each query operates on an average of six tables. An example query is as follows.\n",
      "\n",
      "## Q1: What aircraft is used on delta flight 1984 from Kansas city to Salt Lake city?\n",
      "\n",
      "- (ii) Restaurant [127] comprises a vast collection of dining establishments located in Northern California, storing restaurant names, locations, features, and travel guide ratings. The benchmark contains 250 questions about restaurants, food types and locations. An example query is as follows.\n",
      "\n",
      "## Q2: Where is a good Chinese restaurant in Palo Alto?\n",
      "\n",
      "(iii) GeoQuery [128] consists of 8 tables and 880 natural language queries in the US geographic database. The queries in GeoQuery are designed for the geographic domain, including join queries, nested queries, grouping\n",
      "\n",
      "Table 8: Comparison of benchmarks for NL2SQL\n",
      "\n",
      "| Benchmark       | Select query   | Group query   | Sort query   | Join query   | Nested query   | Fields involved                                     | Usage in papers                                                       |\n",
      "|-----------------|----------------|---------------|--------------|--------------|----------------|-----------------------------------------------------|-----------------------------------------------------------------------|\n",
      "| ATIS            | ✓              |               |              | ✓            | ✓              | air travel                                          | [62, 47, 103, 104, 120]                                               |\n",
      "| Restaurant      | ✓              |               |              | ✓            |                | restaurant                                          | [127, 107, 81, 80]                                                    |\n",
      "| GeoQuery        | ✓              | ✓             | ✓            | ✓            | ✓              | geography                                           | [127, 128, 107, 163, 81, 52, 113, 62, 47, 80, 104, 115, 120, 42, 141] |\n",
      "| MAS             | ✓              | ✓             |              | ✓            |                | academic                                            | [77, 113, 153, 35, 9, 115, 131, 132]                                  |\n",
      "| Scholar         | ✓              |               |              | ✓            |                | academic                                            | [47]                                                                  |\n",
      "| IMDB            | ✓              |               |              | ✓            |                | internet movie                                      | [153, 9, 58, 131, 132]                                                |\n",
      "| YELP            | ✓              |               |              | ✓            |                | business review                                     | [153, 9, 131, 132]                                                    |\n",
      "| WikiSQL         | ✓              |               |              |              |                | multiple fields (e.g. state, college, manufacturer) | [167, 54, 158, 156, 87, 142, 48, 51, 125]                             |\n",
      "| ParaphraseBench | ✓              | ✓             |              |              |                | medical                                             | [133]                                                                 |\n",
      "| Advising        | ✓              |               |              | ✓            | ✓              | university course                                   | [47]                                                                  |\n",
      "| Spider          | ✓              | ✓             | ✓            | ✓            | ✓              | 138 different fields (e.g. car, stadium, country)   | [160, 53, 156, 115, 19, 50, 92, 131, 42, 43, 48, 51, 125]             |\n",
      "\n",
      "Table 9: Details of popular benchmarks\n",
      "\n",
      "| Benchmark             |   Year |   #queries |   #tables | Domains covered   |\n",
      "|-----------------------|--------|------------|-----------|-------------------|\n",
      "| ATIS [108]            |   1990 |       5871 |        25 | single field      |\n",
      "| Restaurant [127]      |   2000 |        250 |         3 | single field      |\n",
      "| GeoQuery [128]        |   2001 |        880 |         7 | single field      |\n",
      "| MAS [77]              |   2014 |        196 |        17 | single field      |\n",
      "| Scholar [62]          |   2017 |        816 |        10 | single field      |\n",
      "| IMDB [153]            |   2017 |        131 |        16 | single field      |\n",
      "| YELP [153]            |   2017 |        128 |         7 | single field      |\n",
      "| WikiSQL [167]         |   2017 |      80654 |     24241 | multiple fields   |\n",
      "| ParaphraseBench [133] |   2018 |        290 |         1 | single field      |\n",
      "| Advising [47]         |   2018 |       4387 |        15 | single field      |\n",
      "| Spider [160]          |   2018 |      10181 |      1020 | multiple fields   |\n",
      "\n",
      "queries and sorting queries. The average length of NLQs and SQLs in GeoQuery is about 8 and 16 words, respectively. Additionally, each query operates on an average of one table. Although the queries are relatively brief in length, they are highly composable, with nearly half of the SQL containing at least one nested sub-query. One of the English queries is as follows.\n",
      "\n",
      "## Q3: What is the largest city in states that border California?\n",
      "\n",
      "(iv) MAS [77] is generated from the Microsoft Academic Search database, which stores information such as academic papers, authors, journals, and conferences. The source of NLQs in MAS is the logical queries that are capable of being articulated in the search interface of the Microsoft Academic Search platform. The fields of MAS and Scholar are both academic in nature, but exhibit distinct patterns. One English query is as follows.\n",
      "\n",
      "## Q4: Return authors who have more papers than Bob in VLDB after 2000.\n",
      "\n",
      "(v) Scholar [62] consists of 816 NLQs for academic database search that are annotated with SQL. The average length of NLQs and SQLs in Scholar is approximately 7 and 29 words, respectively. Each query operates on an\n",
      "\n",
      "Table 10: Query categories and examples for ParaphraseBench\n",
      "\n",
      "| Category            | Example queries                                                       |\n",
      "|---------------------|-----------------------------------------------------------------------|\n",
      "| Naive               | What is the average length of stay of patients where age is 80?       |\n",
      "| Syntactic           | Where age is 80, what is the average length of stay of patients?      |\n",
      "| Morphological       | What is the averaged length of stay of patients where age equaled 80? |\n",
      "| Lexical             | What is the mean length of stay of patients where age is 80 years?    |\n",
      "| Semantic            | What is the average length of stay of patients older than 80?         |\n",
      "| Missing Information | What is the average stay of patients who are 80?                      |\n",
      "\n",
      "average of 3 tables. Iyer et al. [62] provide a database for performing these queries, which includes academic articles, journal details, author information, keywords, citations, and utilized datasets. One of the English queries is as follows.\n",
      "\n",
      "## Q5: Get all author having data set as DATASET TYPE.\n",
      "\n",
      "(vi) IMDB and YELP [153] are generated using data from the Internet Movie Database and Business Review Database, respectively. The NLQs are obtained from coworkers of the authors of the paper [153], who are only aware of the types of data available in the database and not the underlying database schema.\n",
      "\n",
      "(vii) WikiSQL [167], introduced in 2017, is a comprehensive and meticulously annotated collection of natural language to SQL mappings, and currently represents the most extensive data set for NL2SQL. WikiSQL contains SQL table instances extracted from 24241 HTML tables on Wikipedia, and 80654 natural language queries, each accompanied by an SQL. WikiSQL comprises genuine data extracted from the web, with queries involving a multitude of tables, but the queries do not involve complex operations such as GROUP BY and multi-table union queries. The majority of questions in WikiSQL are between 8 and 15 words in length, most SQLs are between 8 and 11 words, and most table columns are between 5 and 7. In addition, most natural language queries are of the what type, followed by which , name how many , , who . The execution accuracy of WikiSQL has significantly improved from the initial 59.4% to 93.0%, and the method has undergone a transformation from a simple seq2seq approach to a multi-tasking, transfer learning, and pre-training paradigm. A pair of questions and SQLs for the CFLDraft table can be formulated as follows.\n",
      "\n",
      "## Q6: How many CFL teams are from York College?\n",
      "\n",
      "SQL Q6: SELECT COUNT CFL Team FROM CFLDraft WHERE College = 'York'\n",
      "\n",
      "(viii) ParaphraseBench [133], a component of the DBPal paper [10], is a benchmark utilized to assess the robustness of NLIDBs. Unlike existing benchmarks, ParaphraseBench covers diverse language variants of user input NLQs and maps natural language to the anticipated SQL output. The benchmark is constructed upon a medical database that contains a single table for storing patient information. The language variants utilized in NLQs permit the classification of NLQs into six categories, as illustrated in Table 10.\n",
      "\n",
      "(ix) Advising [47] was proposed in 2018, and the NLQs were built on a database of course information from the University of Michigan containing fictitious student profiles. A portion of the queries are collected from the Facebook platform of the EECS department, and the remaining questions are formulated by computer science students well-versed in database topics that might be raised in academic consulting appointments. The queries in Advising are for student-advising tasks, including join queries and nested queries. One of the English queries is as follows.\n",
      "\n",
      "## Q7: For next semester, who is teaching EECS 123?\n",
      "\n",
      "(x) Spider [160] is a large NL2SQL data set introduced by Yale University in 2018, in order to solve the requirement for extensive and high-caliber datasets for a novel intricate cross-domain semantic parsing challenge.\n",
      "\n",
      "The data set contains 10181 natural language queries and 5693 corresponding complex SQLs, which are distributed across 200 independent databases, and the content covers 138 different domains. The average length of questions and SQL statements in Spider is approximately 13 and 21 words, respectively. While the number of questions and SQLs in Spider is not as extensive as that of WikiSQL, Spider contains all common SQL patterns and complex SQL usages, including advanced operations like HAVING, GROUP BY, ORDER BY, table joins, and nested queries, which makes Spider closely aligned with real-world scenarios. The following is an illustrative example of a complex problem and the corresponding SQL, which contains a nested query, a GROUP BY component, and multiple table joins.\n",
      "\n",
      "Q8: What are the name and budget of the departments with average instructor salary greater than the overall average?\n",
      "\n",
      "SQL Q8: SELECT T2.name, T2.budget FROM instructor as T1 JOIN department as T2 ON T1.department id = T2.id GROUP BY T1.department id HAVING avg (T1.salary) &gt; (SELECT avg (salary) FROM instructor)\n",
      "\n",
      "## 4.2 Generation of new benchmarks\n",
      "\n",
      "Modifying an existing NL2SQL benchmark to generate a new one is a common practice. The following steps describe the process in detail.\n",
      "\n",
      "(i) Researchers are required to conduct a meticulous analysis of the existing benchmarks, including an examination of the data structures, query types, and complexity. Through the analysis, they can gain insight into the constraints of the benchmark and identify potential avenues for enhancement.\n",
      "\n",
      "(ii) Designing a modification strategy is a critical step, which involves determining how to modify and extend the benchmark on the basis of the analysis results. The step may include adding new queries, changing the linguistic expression of queries, and introducing complex query types.\n",
      "\n",
      "(iii) In the process of implementing modifications, researchers are expected to execute the designed modification strategy with precision in order to ensure that the new benchmark meets the expected requirements.\n",
      "\n",
      "(iv) Evaluating the performance is a pivotal aspect of the process. The researchers employ the modified benchmark to train and test NL2SQL models, subsequently assessing the models' performance and generalization capabilities according to the new benchmark.\n",
      "\n",
      "Building on Spider [160], Kaoshik et al. [67] propose a new NL2SQL benchmark, named ACL-SQL, containing five tables and 3100 pairs of NLQ and SQL. By defining and annotating three types of questions on temporal aspects in Spider: (i) questions querying for temporal information , (ii) questions querying for temporal information with grouping or ordering , and (iii) questions with temporal conditions , Vo et al. [136] propose a new data set, TempQ4NLIDB, which can assist NLIDB systems based on machine learning approaches to improve their performance on temporal aspects. To address the dearth of publicly available benchmarks on ambiguous queries, Bhaskar et al. [11] generate a new benchmark called AmbiQT by modifying Spider with a combination of synonym generation and ChatGPT-based and standard rules-based perturbation. AmbiQT comprises in excess of 3000 examples, each of which can be interpreted as two valid SQLs due to lexical ambiguities (namely, unambiguous column and table names) or structural ambiguities (namely, the necessity of joins and the pre-computation of aggregations).\n",
      "\n",
      "In light of the limitations of existing benchmarks, including (i) the presence of data bias or linguistic expression limitations , and (ii) the limited coverage of domains and contexts that cannot fully represent real-world diversity , researchers have proposed generators for Text2SQL benchmarks. Weir et al. [149] present a synthesized data generator that synthesizes SQL patterns in the template syntax, including aggregations, simple nesting, and column joins. Each SQL pattern is matched with numerous different natural language (NL) patterns, allowing for the generation of a vast number of domain-specific NLQs and SQLs. Luo et al. [90] propose an NL2VIS synthesizer, named NL2SQL-to-NL2VIS, which is capable of generating multiple pairs of natural language and VIS from a single NL and SQL pair based on semantic joins between SQL and VIS queries. NL2SQL-to-NL2VIS\n",
      "\n",
      "can be utilized to create NL2VIS benchmarks from established NL2SQL benchmarks. Hu et al. [59] suggest a framework for synthesizing Text2SQL benchmarks. The framework involves first synthesizing SQL and then generating NLQs. At the stage of synthesizing SQL, a method is suggested for column sampling based on pattern distance weighting to prevent excessive complexity in concatenation. In the process of generating text from SQL, an intermediate representation is used to facilitate the transition from SQL to NLQ, thereby enhancing the quality of the generated NLQ.\n",
      "\n",
      "## 4.3 Evaluation metrics\n",
      "\n",
      "NLIDB is intended to assist users in efficiently querying and retrieving query results, and thus evaluating the response time and effectiveness of the system is essential. Response time measures how quickly the system can process a user's natural language queries and return the relevant results. Effectiveness measures how well the system translates natural languages into accurate and relevant executable database languages, which consists of two measures: (i) translatability and (ii) translation precision .\n",
      "\n",
      "DEFINITION 1 ( Translatability . ) Given the set E of executable languages generated by the system and the set N of input natural language queries, the translatability T is defined as follows.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "DEFINITION 2 ( Translation precision . ) Given the set ER of executable languages that meet the expected results, the set N of natural language queries entered into the system, the translation precision TP is defined as follows.\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Response time denotes the duration necessary for the system to transform the input natural language into the executable language of the database. This temporal interval represents the difference between the moment when the system furnishes the translated output and the moment when the natural language is received. Translatability is a measure of the likelihood of the system accurately translating a natural language into an executable language. This metric is quantified as the proportion of correctly translated queries out of the total number of queries submitted to the system. Translation precision refers to the likelihood that the final output of the translated executable language matches the expected outcome, and is quantified as the ratio of executable languages producing the desired results to the overall number of queries.\n",
      "\n",
      "The outcomes of evaluating a system may be different depending on the benchmark used. The size of the benchmark affects the accuracy of the semantic parsing part of the system. Complex queries in the benchmark can be used to assess the system's ability for generalization. In related papers, PRECISE achieves 95.0% translatability and translation precision on the Restaurant benchmark and 77.5% on the GeoQuery benchmark. ATHENA has a translatability and translation precision of 87.2% on the GeoQuery benchmark and 88.3% on the MAS benchmark. The translatability and translation precision of NALMO on the benchmark MOQ are 98.1% and 88.1%, respectively.\n",
      "\n",
      "## 5 System interfaces development\n",
      "\n",
      "We categorize recently developed NLIDBs according to the technical approach and the data stored in the backend. The methods of developing and using the system interfaces are then divided into two categories for analysis and summary:(i) used as an independent software and (ii) used as a module of a database management system . Finally, enhancements to the existing NLIDB system are presented in three aspects.\n",
      "\n",
      "## 5.1 Recently developed NLIDBs\n",
      "\n",
      "We are concerned with the NLIDBs, which have emerged since 2000. There are several ways to classify NLIDBs. Affolter et al. [2] divide recently developed systems into four categories.\n",
      "\n",
      "- (i) Keyword-based systems are represented by SODA [15]. The core of such a system lies in the search process, where the inverted index containing fundamental data and metadata from the database is utilized as the retrieval target. This process involves comparison with natural language, and identification of keywords referenced in the query. Although simple, the approach fails to identify the potential semantics that are not directly present in natural language. Such systems are unable to respond to aggregation queries and complex questions involving sub-queries.\n",
      "- (ii) Pattern-based systems , exemplified by NLQ/A [166] and QuestIO [27], are extensions of keyword-based systems that are capable of incorporating natural language patterns and mapping to pre-specified query sentence patterns.\n",
      "- (iii) Parsing-based systems are typified by NaLIR, a general interactive natural language interface designed for querying relational databases. NaLIR employs the existing natural language parser to acquire the semantic understanding of the given NLQ which is represented by a parse tree, and then converts the semantic understanding into database understanding and finally into SQL. Such systems incorporate a multitude of natural language processing methods, including the parsing of natural language sentences employing parse trees. One principal benefit of this method is the ability to map semantics into predefined SQL templates.\n",
      "- (iv) Grammar-based systems are represented by TR Discover [121] and MEANS [1]. The foundation of such systems consists of a predetermined set of grammar rules, which are used to constrain the questions that users can pose to the system in order to form formal NLQs that are straightforward to analyze. The primary advantage of this approach is that the systems are capable of providing users with guidance as they enter questions, and can respond to all questions that adhere to the established rules. In comparison to keyword-based, pattern-based and parsing-based systems, grammar-based systems are considered to be the most robust, despite relying significantly on predefined manual rules.\n",
      "\n",
      "In this survey, we categorize NLIDBs into seven distinct groups according to the data stored in the backend. The representative systems for each category are depicted in Figure 8. Among the various categories, natural language interfaces for relational data are the most prevalent and functional, and are subjected to ongoing research on an annual basis. Recently, research on NLIs for XML data has not advanced, remaining at the same stage as in 2007. The two main reasons are (i) an increasing preference for JSON as a format for data exchange over XML , and (ii) the suitability of NoSQL databases for handling unstructured or semi-structured data over XML databases . Since 2013, NLIs for natural language queries over RDF data, ontology data, graph data, spatial data, and spatio-temporal data have been developed. The executable languages transformed by these NLIs correspond to the databases used.\n",
      "\n",
      "NLIDB for relational data transforms natural language queries into SQL. IRNet [53] first identifies the entities contained in the NLQ, including columns, tables and values. Subsequently, a neural model based on syntax is used to synthesize an intermediate representation connecting natural language with SQL. Finally, IRNet derives SQLs on the basis of intermediate representations. Representative NLIDBs for XML databases are NaLIX [85] and DaNaLIX [81], which transform natural language queries into XQuery. NaLIX restricts natural language to a predefined subset of the grammar. DaNaLIX builds upon NaLIX and enables users to leverage domain knowledge for query transformation. TEQUILA [64] is a typical NLIDB for RDF data, which transforms natural language queries into SPARQL. TEQUILA employs a standard knowledge-based question and answer system to evaluate sub-questions independently. The results of the sub-questions are then combined for inference to compute the answer to the full question. QuestIO [27] works for querying structured data represented in ontology format. Built on the ontology and a knowledge base containing instances of the ontology's concepts, QuestIO accepts NLQ as input and produces SeRQL as output. Utilizing the language processing framework GATE, QuestIO\n",
      "\n",
      "Figure 8: Classification of NLIDBs based on data stored in the backend\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "combines fundamental concepts with keywords, blocks and phrases to deduce potential relationships among the concepts in the ontology. In the spatio-temporal domain, NLIDB can handle GIS-related queries, such as historical meteorological data at a specific location, and geographic position information at different moments. NeuroSPE [109] is a spatial extraction model designed to identify spatial relations within Chinese natural language text. The model extends a bidirectional gated recurrent neural network with a series of pre-trained models and is able to address specific challenges in a variety of natural language text, including the absence of direct context and the occurrence of abbreviations, special languages, and symbols. NALMO [144, 143] is a natural language interface designed for moving objects that allows users to submit queries of five types, including (i) time interval queries , (ii) range queries , (iii) nearest neighbor queries , (iv) trajectory similarity queries , and (v) join queries .\n",
      "\n",
      "Several systems have been created that can be used across various back-end data stores, with the objective of enhancing the generality of NLIDB. TR Discover [121] is one such system which transforms NLQ into SPARQL or SQL. TR Discover generates FOL representations by analyzing natural language using a feature-based contextindependent grammar consisting of entries in the vocabulary for leaf nodes and rules governing the phrase structure for non-terminal nodes. The FOL representation is then parsed into a parse tree through the utilization of a firstorder logic parser. The parse tree is traversed sequentially and transformed into SPARQL or SQL. FINESSE [63], an extension to ATHENA, is a system that seamlessly connects to multiple structured data stores. FINESSE can access various structured backends (e.g., RDF stores and Graph stores) by automatically transforming the intermediate query language OQL into the corresponding structured query language specific to the backends (e.g., SPARQL and Gremlin).\n",
      "\n",
      "## 5.2 Development and usage of system interfaces\n",
      "\n",
      "The combination of the aforementioned three components, including (i) natural language preprocessing , (ii) natural language understanding and (iii) natural language translation , constitutes a comprehensive system architecture. Then the theoretical knowledge is implemented in the form of a system. There are two primary methods of development and usage:\n",
      "\n",
      "(i) A stand-alone software. In this scenario, the system generally comprises a separate visual interface and a database, and the architecture is shown in Figure 9(a). A visual interface allows users to write natural language\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "(a)\n",
      "\n",
      "A stand-alone software\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "(b)\n",
      "\n",
      "A plug-in for DBMS\n",
      "\n",
      "Figure 9: The architecture of the database system with NLI\n",
      "\n",
      "problems that are interactively translated into executable language. By submitting the executable language in the corresponding database management system, the query results can be obtained. The paper [78] presents the JavaScript-driven interface of NaLIR, which interacts with a master server implemented in Java. NL2CM is implemented in Java 7, whose web user interface is constructed in PHP 5.3 and jQuery 1.x. The paper [62] develops a web interface designed to receive NLQs from users directed towards academic databases and display translated SQLs. The interface also shows several example utterances to assist users in comprehending the domain. The tool that comes with the NLMO system is a web application written in Java.\n",
      "\n",
      "(ii) A plug-in for the database management system. In this instance, the system exists in a format analogous to a Python custom module and interacts with the user through the visual interface of the database management system. The system architecture is illustrated in Figure 9(b). The user inputs NLQ by invoking the interface provided by the system. Thereafter, the database management system automatically calls the NLI module to process the NLQ, and displays the translated executable language on the visual interface. One of the most typical systems is NALMO, which is developed on a laptop running Ubuntu 14.04. The final interface form in SECONDO is represented as an algebraic module with an operator. The users can use the operator on the moving objects databases in SECONDO to perform the corresponding NLQ translation of moving objects.\n",
      "\n",
      "## 5.3 Enhancement of NLIDB systems\n",
      "\n",
      "Although existing NLIDBs have been able to achieve the transformation from natural language to executable database language, the research on NLIDB is a long process and the systems need to be optimized step by step because natural language has rich expressions, ambiguous semantic knowledge and intricate correlations [61]. Enhancements to the existing NLIDB systems are mainly in the following three areas: (i) interpreting answers and non-answers to queries , (ii) improving the effectiveness of the system , and (iii) securing the system against potential vulnerabilities .\n",
      "\n",
      "## 5.3.1 Interpreting answers and non-answers to queries\n",
      "\n",
      "Researchers have enhanced the functionalities of existing systems with regards to providing explanations for both query answers and non-answers. Users of NLIDB do not usually have the relevant expertise and may have difficulty in understanding the results or verifying their correctness. In this work, papers [31, 32, 33, 34] complement these efforts by providing NL explanations for query answers. The authors propose a system named NLProv, which employs the original NLQ structure to transform the provenance information into natural language. The obtained provenance information is then presented to the user in the form of natural language answers, through a four-step process:\n",
      "\n",
      "- · The user inputs a query using natural language that is transmitted to the improved NaLIR. The system processes the NLQ, constructs a formal query, and stores the translated portions of the NLQ in relation to the formal query.\n",
      "- · NLProv employs the SelP system [36] to evaluate formal queries and records the provenance of each query, indicating the correlation between dependency tree nodes and specific provenance sections.\n",
      "- · The source information is decomposed and then compiled into an NL answer with explanation.\n",
      "- · The system presents the factorized answer to the user. In cases where the answer is excessively detailed and difficult to comprehend, users have the option to access summaries at various levels of nesting.\n",
      "\n",
      "The paper [34] proposes a general solution for NLProv that is not specific to NaLIR. The core of the solution is an alternative architecture that does not depend on the query builder for producing the partial mappings between the nodes of the dependency tree and the components of the query. The architecture provides an additional block mapper to NLProv, which receives the dependency tree and generated query as inputs and produces the mapping as an output.\n",
      "\n",
      "Users may fail to obtain the expected results when using NLIDBs, leading to surprise or confusion. NLProveNAns [35] enriches NaLIR by supporting interpretations of non-answer. NLProveNAns can provide two explanations, corresponding to two different why-not source models: (i) a concise explanation rooted in the picky boundary model and (ii) a comprehensive explanation derived from the polynomial model . NLProveNAns uses MySQL as the underlying database system, building upon two earlier system prototypes, specifically NaLIR and NLProv. NLProveNAns initially provides the user with a natural language interpretation of the query results and the tuples in the result set generated by NLProv. The user then formulates a ' why-not ' query. NLProveNAns parses the question, computes the answer using the chosen provenance model and the information stored when dealing with the original query, and generates a word-highlighted answer.\n",
      "\n",
      "## 5.3.2 Improving the effectiveness of the system\n",
      "\n",
      "Numerous researchers have provided user interaction components for NLIDB systems to improve effectiveness. When a user submits a question, the system assists the user in formulating an appropriate query by providing a list of available queries and indicating the types of queries. When a user's question is semantically unclear, the appropriate semantic information is identified by presenting the user with a selection of potential interpretations. When the data inputted by the user is not found in the database, similar information in the database can be provided to the user in the form of an associative prompt. Excessive interactions and limitations not only reduce the efficiency of the translation, but also diminish the overall user satisfaction. Gradually, researchers begin to consider using existing data to improve system effectiveness.\n",
      "\n",
      "A key challenge to improving system effectiveness lies in closing the semantic gap between natural language and the fundamental data in the database. This challenge is reflected in join path inference and keyword mapping when converting natural language to SQL. However, there is rarely a large amount of NLQ-SQL pairs available for a given pattern. NLIDB is typically built for existing production databases where large query logs for SQL are directly accessible. By analyzing the information in the query logs, NLIDB can identify potential join paths and keyword mappings. TEMPLAR [9] augments existing pipeline-based NLIDBs using query log information, and the architecture is shown in Figure 10. TEMPLAR models the data from the query log using a data structure known as the Query Fragment Graph (QFG), leveraging the information to enhance the capabilities of current NLIDBs in join path inference and keyword mapping. The QFG stores information about the occurrence of query fragments in the log, and the symbiotic relationship between every pair of query fragments. Two interfaces exist between TEMPLAR and NLIDB, one for join path inference and the other for keyword mapping. The experimental evaluation in the paper [9] proves the effectiveness of TEMPLAR, which greatly improves the translatability of NaLIR and Pipeline by using query logs for SQL.\n",
      "\n",
      "Taking the NLQ ' Find papers from 2000 until 2010. ' from the Microsoft Academic Search database as an\n",
      "\n",
      "Figure 10: The architecture of the NLIDB enhanced by TEMPLAR\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "example, the translation process of NaLIR enhanced with TEMPLAR is as follows.\n",
      "\n",
      "In the initial step, the NLQ is parsed using NaLIR to identify the keywords associated with the database elements and the relevant parser metadata. In this instance, the keywords identified by NaLIR are papers and from 2000 until 2010 . The result of using NaLIR to generate metadata is papers in the SELECT context and from 2000 until 2010 in the WHERE context.\n",
      "\n",
      "In the second step, the keywords are transmitted to the Keyword Mapper that utilizes the keyword metadata and pertinent information from the database to associate each keyword with potential query segments and assign a score to these segments. In this example, the candidate mappings for papers include (journal.name, SELECT) and (publication.title, SELECT) , and from 2000 until 2010 is mapped to (publication.year ≥ 2000 AND publication.year ≤ 2010, WHERE) . The Keyword Mapper transmits the two most likely candidate configurations back to\n",
      "\n",
      "## NaLIR as follows.\n",
      "\n",
      "- · [(journal.name, SELECT); (publication.year &gt; = 2000 AND publication.year &lt; = 2010, WHERE)]\n",
      "- · [(publication.title, SELECT); (publication.year &gt; = 2000 AND publication.year &lt; =\n",
      "\n",
      "2010, WHERE)]\n",
      "\n",
      "In the third step, NaLIR sends the known relationship of every candidate configuration to the Join Path Generator to generate the most probable join path. In this example, the Join Path Generator generates the join paths journal-publication and publication for the two configurations, respectively.\n",
      "\n",
      "In the final step, NaLIR utilizes the join paths returned by the Join Path Generator to construct and return the SQL for each candidate configuration. In this example, the final translated SQLs are as follows.\n",
      "\n",
      "- · SELECT j.name FROM journal j, publication p\n",
      "- WHERE p.year &gt; = 2000 AND p.year &lt; =\n",
      "- 2010 AND j.jid = p.jid\n",
      "- · SELECT title FROM publication WHERE year &gt; = 2000 AND year &lt; = 2010\n",
      "\n",
      "## 5.3.3 Securing the system against potential vulnerabilities\n",
      "\n",
      "Research on the security vulnerabilities arising from malicious user interactions is relatively limited. Zhang et al. [164] propose a backdoor-based SQL injection framework for Text2SQL systems named TrojanSQL, using two injection attacks: (i) boolean-based and (ii) union-based . Boolean-based injection is used for conditional queries\n",
      "\n",
      "with WHERE clauses and invalidates the original query condition by performing boolean operations on existing conditional judgments to bypass the original query condition. Union-based injection aims to steal private information, including database meta-information and user data privacy by performing a union query on the original user query. Experimental results demonstrate that TrojanSQL has a high attack success rate against current Text2SQL systems and is difficult to defend against. Zhang et al. [164] provide security practice recommendations for NLIDB developers to reduce the risk of SQL injection attacks:\n",
      "\n",
      "- · The utilization of only officially recognized or peer-reviewed data sets for model training is recommended.\n",
      "- · The selection of a verified and reputable source for initializing model weights is advised.\n",
      "- · The implementation of additional layers of security or filtering should be considered when using model linking techniques.\n",
      "- · Rigorous testing should be performed prior to the integration of NLIDB APIs provided by third parties into an application.\n",
      "\n",
      "## 6 Discussions about Text2SQL with LLM, SQL2Text and Speech2SQL\n",
      "\n",
      "We discuss deep language understanding and database interaction techniques related to NLIDB, including the use of LLM for Text2SQL tasks, the creation of natural language interpretations from SQL, and the transformation of speech queries into SQL.\n",
      "\n",
      "## 6.1 Text2SQL with LLM\n",
      "\n",
      "The advent of the Transformer architecture [134] has resulted in considerable success of LLMs in natural language processing tasks. The models effectively capture the deep structure and semantic information of language through pre-training and fine-tuning [94]. Decoder-only, encoder-only and encoder-decoder are the principal structures of LLMs.\n",
      "\n",
      "(i) The decoder-only model , represented by GPT [18, 98], exclusively comprises a decoder and generates output sequences progressively through an autoregressive approach. The model is suitable for generative tasks such as text generation and dialogue systems [110]. However, the model exhibits limited effectiveness when processing long texts due to the autoregressive nature. Additionally, the model does not directly handle input information, posing a challenge of unidirectional information transmission.\n",
      "\n",
      "(ii) The encoder-only model , represented by BERT [37], contains only an encoder and extracts context through bidirectional training. This architecture is applicable to tasks involving context comprehension and supervised learning. Lacking a direct output generation mechanism, the model is unsuitable for generative tasks. In addition, the model cannot handle variable-length outputs in seq2seq tasks.\n",
      "\n",
      "(iii) The encoder-decoder model , represented by T5 [111], consists of an encoder and a decoder. The encoder maps the input sequence to a high-dimensional contextual representation, which is then utilized by the decoder to produce the output sequence. The architecture excels in tasks requiring global information transfer, such as machine translation and summary generation [76]. However, the computational resource demands of the model are high, and the complexity of information transfer may lead to performance degradation in certain tasks.\n",
      "\n",
      "LLMs contribute to the development of NLIDB. Notably, the growing popularity of GPT [18, 98] opens new possibilities for NLP in NLIDB systems. GPT supports natural language queries over spatial data and returns sensible SQL frameworks.\n",
      "\n",
      "EXAMPLE1. Taking the NLQ 'Can you tell me what POIs are available in Jiangning District?' as an example, the SQL generated by GPT is as follows.\n",
      "\n",
      "## SELECT POI.name\n",
      "\n",
      "FROM POI JOIN district ON ST Within(POI.geom, district.geom)\n",
      "\n",
      "WHERE district.name = 'Jiangning District';\n",
      "\n",
      "The query employs the ST Within function to ascertain whether the location of each POI is within Jiangning District. GPT extracts the entities (POI and district) and the query type (range query).\n",
      "\n",
      "However, GPT is primarily designed for traditional relational data and has limited ability to represent spatial data. While adept at processing simple objects(e.g., points), GPT's representation capabilities are less effective when dealing with more intricate objects(e.g., lines and regions).\n",
      "\n",
      "EXAMPLE 2. Taking the NLQ 'What cinemas are there on Sterndamm street?' as an example, the SQL generated by GPT is as follows.\n",
      "\n",
      "## SELECT name\n",
      "\n",
      "FROM cinemas\n",
      "\n",
      "WHERE ST Intersects (location, ST GeomFromText (' LINESTRING (13.531836 52.437831, 13.536510 52.434202 )', 4326));\n",
      "\n",
      "GPT is capable of capturing the pivotal semantic details contained within the query, including cinemas, Sterndamm street and the spatial correlation between them. However, the representation of Sterndamm street in the executable language is not accurate and Sterndamm street comprises multiple segments. Upon receiving the prompt 'Sterndamm street is stored in the spatial relation streets', GPT generates a reasonable SQL:\n",
      "\n",
      "## SELECT name\n",
      "\n",
      "FROM cinemas\n",
      "\n",
      "WHERE ST Intersects (location, (SELECT ST Buffer (geom, 0.0001) FROM streets WHERE name =\n",
      "\n",
      "'Sterndamm'));\n",
      "\n",
      "The query utilizes the ST Buffer function to create a buffer with a size of 0.0001 degrees (approximately 11 meters) around Sterndamm street and subsequently employs the ST Intersects function to examine whether the location of each cinema intersects with the buffer.\n",
      "\n",
      "The advent of intricate deep learning architectures has prompted a focus on accurately interpreting natural language and generating structured language by optimizing LLMs. This direction emphasizes optimizing the LLM through larger data pre-training, superior language representation learning techniques, and more efficient fine-tuning methods. Zero-sample learning strategies have also received attention to enable the system to handle unseen query types without retraining, which can be achieved through zero-sample learning and meta-learning techniques.\n",
      "\n",
      "## 6.2 SQL2Text\n",
      "\n",
      "The purpose of SQL2Text is to transform complex SQL into natural language description. This transformation helps non-technical users to comprehend the logic and structure of SQL, thus making database interactions transparent and understandable. Koutrika et al. [73] utilize a graph-based approach for transforming SQL into natural language. SQL is first represented as a directed graph whose edges are labeled with template labels using an extensible template mechanism, thus providing semantics for the parts of the query. These graphs are then explored and textual query description is composed using a variety of graph traversal strategies, including the binary search tree algorithm, the multi-reference point algorithm, and the template combination algorithm. Eleftherakis et al. [40] address SQL2Text by extending the graph-based model of Logos to translate a wider range of queries (e.g. SELECT TOP, LIMIT, IN, and LIKE). The SQL is first analyzed to generate a parse tree storing the essential information utilized to construct the query graph, and then the textual description of the SQL is created through\n",
      "\n",
      "the application of the multi-reference point traversal strategy. Camara et al. [20] employ LLM to generate explanations of SQL. The logical structure of SQL is recorded and the columns and tables are interpreted in natural language.\n",
      "\n",
      "Although progress has been made in this direction, there remains ample opportunity for enhancement. Future research will focus on improving the quality and richness of the generated natural language explanations, ensuring that they are both accurate and rich. In addition, future research will explore context-awareness, which means providing relevant natural language explanations in conjunction with the contextual information in the user's query. This technique also involves exploring how SQL2Text can be combined with dialogue systems to enable intelligent and coherent database interactions.\n",
      "\n",
      "## 6.3 Speech2SQL\n",
      "\n",
      "Speech2SQL technology is designed to transform speech input into SQL, making the process of database querying as simple and intuitive as speaking, thus significantly reducing the barrier to database interaction. SpeakQL [21, 119, 117, 118] converts speech SQL into queries that are displayed on the screen, where users can perform interactive query corrections using a screen-based touch interface or a single click. SpeakQL utilizes automatic speech recognition (ASR) tools to record speech SQL which will be output as text. The Structure Determination component of SpeakQL is responsible for post-processing the ASR results in order to generate syntactically accurate SQL with textual placeholders, and then uses the original ASR output to fill in the textual placeholders. SpeakNav [165, 12] is a system that combines natural language understanding with route search related to navigation. Users are permitted to describe a predetermined route by voice, and SpeakNav presents a suggested path on a map accompanied by information regarding the estimated duration and distance of the journey. MUVE [147, 148] converts NLQs formulated in speech to SQL using a greedy heuristic approach that does not ensure an optimal solution, but produces a solution that is close to optimal. MUVE answers speech queries by utilizing a multi-plot approach, including multiple bar graphs that display the outcomes of various query options. SpeechSQLNet [122] is an end-to-end neural architecture designed to convert speech into SQL directly, obviating the necessity for an external ASR. SpeechSQLNet effectively combines a transformer, a graphical neural network, and a speech encoder as foundational components. The speech encoder is first used to transform speech into a concealed representation, and the GNN-based encoder is employed to convert patterns that have a considerable influence on the desired SQL into hidden features to safeguard the structural information. The speech embedding is then combined with pattern characteristics to generate semantically consistent SQL. Wav2SQL [86] is also an end-to-end Speech2SQL parser that utilizes self-supervised learning to address the challenge of limited data availability and generate diverse representations. Furthermore, speech reprogramming and gradient inversion techniques are introduced to eliminate stylistic attributes in the speech representation and enhance the generalization ability of the model to user-defined data. VoiceQuerySystem [123] is a speech-based database query system that generates SQL from NLQ speech using two methods:\n",
      "\n",
      "- · Cascade approach involves converting speech-based natural language queries to text using a proprietary ASR module, followed by the generation of SQL through IRNet.\n",
      "- · End-to-end approach directly converts speech to SQL without the need for text as an intermediate medium, by using SpeechSQLNet.\n",
      "\n",
      "Despite the considerable efforts invested in speech recognition and interaction technologies, there remain significant challenges that require further attention. Subsequent research is expected to concentrate on enhancing the accuracy of speech recognition, possibly by utilizing end-to-end speech recognition models and integrating multiple modalities with other input sources. This technique will also involve investigating the potential for combining speech interaction with text query processing techniques to facilitate seamless and efficient database interaction.\n",
      "\n",
      "## 7 Future research and conclusions\n",
      "\n",
      "We investigate unresolved issues and potential directions for future research in the area of NLIDB and provide the conclusions of this paper.\n",
      "\n",
      "## 7.1 Open problems\n",
      "\n",
      "Despite the considerable advancements made by NLIDB, numerous challenges and issues remain to be addressed. The following is a list of the principal open problems with the technical details.\n",
      "\n",
      "Natural language disambiguation. The ambiguity and polysemous nature of natural language makes NLIDB systems face great challenges in correctly understanding user intentions. Future research should focus on the following aspects.\n",
      "\n",
      "- (i) Contextual understanding. Advanced context-aware models can be developed to utilize contextual information for disambiguation. Attention mechanisms and memory networks allow to keep track of the context in a dialogue system.\n",
      "- (ii) Multi-round dialogue. Introducing multiple rounds of dialogue enables the system to gradually clarify users' intent through a series of interactions, which requires the design of an effective dialogue management strategy and a mechanism for confirming users' intent.\n",
      "\n",
      "(iii) Semantic parsing. Complex semantic parsing techniques, such as semantic role labeling and knowledge graph, can be utilized to elucidate the implicit information in natural language.\n",
      "\n",
      "Query optimization. Converting natural language queries into efficient database queries and optimizing query performance during execution remain significant challenges. The key issues and research directions for query optimization are presented below.\n",
      "\n",
      "(i) Index Selection. Depending on the query criteria and data distribution, the indexing scheme that optimizes retrieval speed is selected. The optimizer scans the existing indexes, evaluates the selectivity and cost of each index, and determines which indexes filter the data most efficiently. In complex queries, multiple indexes may be used simultaneously, and the optimizer will select a union index or cross-index scan to improve query performance.\n",
      "\n",
      "(ii) Query rewriting is a method of simplifying the execution plan and improving query efficiency. Sub-queries can be reformulated as joins to simplify complex nested queries. Additionally, the value of constant expressions can be computed in advance in the query, reducing the runtime computation. Finally, redundant sorting, joining, or filtering operations can be removed from the query to simplify the query execution plan.\n",
      "\n",
      "(iii) Execution plan selection. The cost of each execution plan is evaluated using statistical information (e.g., table size and index distribution) and a cost model (rule-based or cost-based). This evaluation considers I/O operations, CPU time, and memory utilization. The least costly plan can be identified through dynamic programming or heuristic algorithms, thereby ensuring that the query is executed with minimal resource consumption and optimal performance.\n",
      "\n",
      "(iv) Join optimization. The join operation is a highly resource-consuming process, and determining the most efficient join order and method is critical. The selection of suitable join algorithms (e.g., subsumption joins, hash joins and nested loop joins) and the application of join condition derivation can lead to a reduction in the quantity of join operations, thus optimizing join performance and improving query efficiency.\n",
      "\n",
      "Corpus construction. One of the most pressing issues in the research of NLIDB is the construction and utilization of the corpus, with particular focus on the following aspects.\n",
      "\n",
      "(i) In order to guarantee the generality and adaptability of the natural language interface system, one needs to collect data from diverse sources. Multi-source data integration techniques can be employed to gather information from user query logs, social media conversations, and customer service records to ensure that the corpus is diverse and representative.\n",
      "\n",
      "- (ii) A high-quality corpus relies on accurate annotation, which requires the integration of manual and automated tools. The development of collaborative annotation platforms and automated annotation tools can enhance the efficiency and uniformity of annotation, concurrently establishing a quality assessment system to detect and rectify annotation errors, thus ensuring the accuracy and reliability of data annotation.\n",
      "\n",
      "(iii) Protecting user privacy and data security is of paramount importance when constructing and utilizing the corpus. The application of differential privacy and data encryption techniques, in conjunction with the formulation of guidelines for the ethical use of data, can guarantee legality and compliance in the process of data collection and utilization. Transparency and user control techniques enable users to understand and regulate the usage of data.\n",
      "\n",
      "(iv) The construction and evaluation of the corpus necessitate a unified and standardized framework to facilitate the comparison of research results and the sharing of data. The establishment of open data platforms and the promotion of cross-institutional cooperation can address legal and technical challenges in data sharing and promote the sharing and reuse of resources and results.\n",
      "\n",
      "## 7.2 Conclusions\n",
      "\n",
      "This paper offers a comprehensive review of recently proposed NLIDBs. We summarize the translation process from natural language to database executable language in three stages: (i) natural language preprocessing , (ii) natural language understanding , and (iii) natural language translation . At the natural language preprocessing stage, we observe that almost every system employs named entity recognition and part-of-speech tagging. At the natural language understanding stage, we learn that although the limitations of rule-based approaches can be eliminated, machine learning-based semantic parsing methods are highly dependent on training data and require longer time and more memory space to build models. At the natural language translation stage, we provide a general process for building executable languages over relational and spatio-temporal databases. Furthermore, we provide a summary of the common benchmarks for translating natural language queries into executable languages, system evaluation metrics, and the classification, development, and enhancement of NLIDBs. Despite the potential to enhance database accessibility, NLIDB still faces numerous challenges, including natural language disambiguation, query optimization, and corpus construction. Future research should prioritize addressing the open issues to further improve the effectiveness and user satisfaction of NLIDB systems.\n",
      "\n",
      "## References\n",
      "\n",
      "- [1] Asma Ben Abacha and Pierre Zweigenbaum. MEANS: A medical question-answering system combining NLP techniques and semantic web technologies. Inf. Process. Manag. , 51(5):570-594, 2015.\n",
      "- [2] Katrin Affolter, Kurt Stockinger, and Abraham Bernstein. A comparative survey of recent natural language interfaces for databases. VLDB J. , 28(5):793-819, 2019.\n",
      "- [3] Karam Ahkouk and Mustapha Machkour. Towards an interface for translating natural language questions to SQL: a conceptual framework from a systematic review. Int. J. Reason. based Intell. Syst. , 12(4):264-275, 2020.\n",
      "- [4] Muhammed Jassem Al-Muhammed and Deryle W. Lonsdale. Ontology-aware dynamically adaptable freeform natural language agent interface for querying databases. Knowl. Based Syst. , 239:108012, 2022.\n",
      "- [5] Yael Amsterdamer, Anna Kukliansky, and Tova Milo. A natural language interface for querying general and individual knowledge. Proc. VLDB Endow. , 8(12):1430-1441, 2015.\n",
      "- [6] Yael Amsterdamer, Anna Kukliansky, and Tova Milo. Nl 2 cm: A natural language interface to crowd mining. In SIGMOD , pages 1433-1438, 2015.\n",
      "- [7] Ion Androutsopoulos, Graeme D. Ritchie, and Peter Thanisch. Natural language interfaces to databases an introduction. Nat. Lang. Eng. , 1(1):29-81, 1995.\n",
      "\n",
      "| [8]   | Yoav Artzi and Luke S. Zettlemoyer. Bootstrapping semantic parsers from conversations. In EMNLP , pages 421-432, 2011.                                                                                                                |\n",
      "|-------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| [9]   | Christopher Baik, H. V. Jagadish, and Yunyao Li. Bridging the semantic gap with SQL query logs in natural language interfaces to databases. In IEEE ICDE , pages 374-385, 2019.                                                       |\n",
      "| [10]  | Fuat Basik, Benjamin H¨ttasch, a Amir Ilkhechi, Arif Usta, Shekar Ramaswamy, Prasetya Utama, Nathaniel Weir, Carsten Binnig, and Ugur Cetintemel. ¸ Dbpal: A learned nl-interface for databases. In SIGMOD , pages 1765-1768, 2018.   |\n",
      "| [11]  | Adithya Bhaskar, Tushar Tomar, Ashutosh Sathe, and Sunita Sarawagi. Benchmarking and improving text-to-sql generation under ambiguity. In EMNLP , pages 7053-7074, 2023.                                                              |\n",
      "| [12]  | Lei Bi, Juan Cao, Guohui Li, Nguyen Quoc Viet Hung, Christian S. Jensen, and Bolong Zheng. Speaknav: Avoice-based navigation system via route description language understanding. In ICDE , pages 2669-2672, 2021.                    |\n",
      "| [13]  | Steven Bird. NLTK: the natural language toolkit. In ACL , 2006.                                                                                                                                                                       |\n",
      "| [14]  | Adrian N. Bishop, Jeremie Houssineau, Daniel Angley, and Branko Ristic. Spatio-temporal tracking from natural language statements using outer probability theory. Inf. Sci. , 463-464:56-74, 2018.                                    |\n",
      "| [15]  | Lukas Blunschi, Claudio Jossen, Donald Kossmann, Magdalini Mori, and Kurt Stockinger. SODA: gener- ating SQL for business users. Proc. VLDB Endow. , 5(10):932-943, 2012.                                                             |\n",
      "| [16]  | Joel Booth, Barbara Di Eugenio, Isabel F. Cruz, and Ouri Wolfson. Robust natural language processing for urban trip planning. Appl. Artif. Intell. , 29(9):859-903, 2015.                                                             |\n",
      "| [17]  | Joel Booth, A. Prasad Sistla, Ouri Wolfson, and Isabel F. Cruz. Adata model for trip planning in multimodal transportation systems. In EDBT , volume 360 of ACM International Conference Proceeding Series , pages 994-1005, 2009.    |\n",
      "| [18]  | Tom B. Brown, Benjamin Mann, Nick Ryder, and et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901, 2020.                                                                    |\n",
      "| [19]  | Ursin Brunner and Kurt Stockinger. Valuenet: A natural language-to-sql system that learns from database information. In ICDE , pages 2177-2182, 2021.                                                                                 |\n",
      "| [20]  | Vanessa Cˆmara, a Rayol Mendonca-Neto, Andr´ e Silva, and Luiz Cordovil Jr. A large language model approach to sql-to-text generation. In ICCE , pages 1-4, 2024.                                                                     |\n",
      "| [21]  | Dharmil Chandarana, Vraj Shah, Arun Kumar, and Lawrence K. Saul. Speakql: Towards speech-driven multi-modal querying. In HILDA@SIGMOD , pages 11:1-11:6, 2017.                                                                        |\n",
      "| [22]  | Chih-Yung Chang, Yuan-Lin Liang, Shih-Jung Wu, and Diptendu Sinha Roy. Sv 2 -sql: a text-to-sql trans- formation mechanism based on BERT models for slot filling, value extraction, and verification. Multim. Syst. , 30(1):16, 2024. |\n",
      "| [23]  | Peng Chen, Hui Li, Sourav S. Bhowmick, Shafiq R. Joty, and Weiguo Wang. LANTERN: boredom- conscious natural language description generation of query execution plans for database education. In SIGMOD , pages 2413-2416, 2022.       |\n",
      "| [24]  | Yi-Hui Chen, Eric Jui-Lin Lu, and Ting-An Ou. Intelligent SPARQL query generation for natural language processing systems. IEEE Access , 9:158638-158650, 2021.                                                                       |\n",
      "| [25]  | Jianpeng Cheng, Siva Reddy, Vijay A. Saraswat, and Mirella Lapata. Learning structured natural language representations for semantic parsing. In ACL , pages 44-55, 2017.                                                             |\n",
      "| [26]  | Danica Damljanovic, Milan Agatonovic, and Hamish Cunningham. Freya: An interactive way of querying linked data using natural language. In ESWC , volume 7117 of Lecture Notes in Computer Science , pages 125-138, 2011.              |\n",
      "| [27]  | Danica Damljanovic, Valentin Tablan, and Kalina Bontcheva. Atext-based query interface to OWLontolo- LREC                                                                                                                             |\n",
      "\n",
      "| [28]   | Alaka Das and Rakesh Chandra Balabantaray. Mynlidb: A natural language interface to database. In ICIT , pages 234-238, 2019.                                                                                                                                          |\n",
      "|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| [29]   | C. J. Date. A critique of the SQL database language. SIGMOD Rec. , 14(3):8-54, 1984.                                                                                                                                                                                  |\n",
      "| [30]   | Ephrem Tadesse Degu and Rosa Tsegaye Aga. Natural language interface for covid-19 amharic database using LSTM encoder decoder architecture with attention. In ICT4DA , pages 95-100, 2021.                                                                            |\n",
      "| [31]   | Daniel Deutch, Nave Frost, and Amir Gilad. Nlprov: Natural language provenance. Proc. VLDB Endow. , 9(13):1537-1540, 2016.                                                                                                                                            |\n",
      "| [32]   | Daniel Deutch, Nave Frost, and Amir Gilad. Provenance for natural language queries. Proc. VLDB Endow. , 10(5):577-588, 2017.                                                                                                                                          |\n",
      "| [33]   | Daniel Deutch, Nave Frost, and Amir Gilad. Natural language explanations for query results. SIGMOD Rec. , 47(1):42-49, 2018.                                                                                                                                          |\n",
      "| [34]   | Daniel Deutch, Nave Frost, and Amir Gilad. Explaining natural language query results. VLDB J. , 29(1):485-508, 2020.                                                                                                                                                  |\n",
      "| [35]   | Daniel Deutch, Nave Frost, Amir Gilad, and Tomer Haimovich. Nlprovenans: Natural language provenance for non-answers. Proc. VLDB Endow. , 11(12):1986-1989, 2018.                                                                                                     |\n",
      "| [36]   | Daniel Deutch, Amir Gilad, and Yuval Moskovitch. Selective provenance for datalog programs using top-k queries. Proc. VLDB Endow. , 8(12):1394-1405, 2015.                                                                                                            |\n",
      "| [37]   | Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirec- tional transformers for language understanding. In NAACL-HLT , pages 4171-4186, 2019.                                                                           |\n",
      "| [38]   | Li Dong and Mirella Lapata. Language to logical form with neural attention. In ACL , 2016.                                                                                                                                                                            |\n",
      "| [39]   | Li Dong and Mirella Lapata. Coarse-to-fine decoding for neural semantic parsing. In ACL , pages 731-742, 2018.                                                                                                                                                        |\n",
      "| [40]   | Stavroula Eleftherakis, Orest Gkini, and Georgia Koutrika. Let the database talk back: Natural language explanations for SQL. In SEA-Data , volume 2929 of CEUR Workshop Proceedings , pages 14-19, 2021.                                                             |\n",
      "| [41]   | Tatiana N. Erekhinskaya, Dmitriy Strebkov, Sujal Patel, Mithun Balakrishna, Marta Tatu, and Dan I. Moldovan. Ten ways of leveraging ontologies for natural language processing and its enterprise appli- cations. In SBD@SIGMOD , pages 8:1-8:6, 2020.                |\n",
      "| [42]   | Yuankai Fan, Zhenying He, Tonghui Ren, Dianjun Guo, Lin Chen, Ruisi Zhu, Guanduo Chen, Yinan Jing, Kai Zhang, and X. Sean Wang. Gar: Agenerate-and-rank approach for natural language to SQL translation. In ICDE , pages 110-122, 2023.                              |\n",
      "| [43]   | Yuankai Fan, Tonghui Ren, Dianjun Guo, Zhigang Zhao, Zhenying He, X. Sean Wang, Yu Wang, and Tao Sui. An integrated interactive framework for natural language to SQL translation. In WISE , volume 14306 of Lecture Notes in Computer Science , pages 643-658, 2023. |\n",
      "| [44]   | Yuankai Fan, Tonghui Ren, Zhenying He, X. Sean Wang, Ye Zhang, and Xingang Li. Gensql: Agenerative natural language interface to database systems. In ICDE , pages 3603-3606, 2023.                                                                                   |\n",
      "| [45]   | Alessandro Fantechi, Stefania Gnesi, Samuele Livi, and Laura Semini. A spacy-based tool for extracting variability from NL requirements. In SPLC , pages 32-35, 2021.                                                                                                 |\n",
      "| [46]   | S´bastien e Ferr´. e Sparklis: An expressive query builder for SPARQL endpoints with guidance in natural language. Semantic Web , 8(3):405-418, 2017.                                                                                                                 |\n",
      "| [47]   | Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir R. Radev. Improving text-to-sql evaluation methodology. In ACL , pages 351-360, 2018.                                                         |\n",
      "| [48]   | Han Fu, Chang Liu, Bin Wu, Feifei Li, Jian Tan, and Jianling Sun. Catsql: Towards real world natural language to SQL applications. Proc. VLDB Endow. , 16(6):1534-1547, 2023.                                                                                         |\n",
      "\n",
      "| [49]   | Kaitlyn Fulford and Aspen Olmsted. Mobile natural language database interface for accessing relational data. In i-Society , pages 86-87, 2017.                                                                                                                                                                                                           |\n",
      "|--------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| [50]   | Yujian Gan, Xinyun Chen, Jinxia Xie, Matthew Purver, John R. Woodward, John H. Drake, and Qiaofu Zhang. Natural SQL: making SQL easier to infer from natural language specifications. In EMNLP , pages 2030-2042, 2021.                                                                                                                                  |\n",
      "| [51]   | Robert Giaquinto, Dejiao Zhang, Benjamin Kleiner, Yang Li, Ming Tan, Parminder Bhatia, Ramesh Nalla- pati, and Xiaofei Ma. Multitask pretraining with structured knowledge for text-to-sql generation. In ACL , pages 11067-11083, 2023.                                                                                                                 |\n",
      "| [52]   | Alessandra Giordani and Alessandro Moschitti. Translating questions to SQL queries with generative parsers discriminatively reranked. In COLING , pages 401-410, 2012.                                                                                                                                                                                   |\n",
      "| [53]   | Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang Lou, Ting Liu, and Dongmei Zhang. Towards complex text-to-sql in cross-domain database with intermediate representation. In ACL , pages 4524-4535, 2019.                                                                                                                                          |\n",
      "| [54]   | Izzeddin Gur, Semih Yavuz, Yu Su, and Xifeng Yan. Dialsql: Dialogue based structured query generation. In ACL , pages 1339-1349, 2018.                                                                                                                                                                                                                   |\n",
      "| [55]   | Ralf Hartmut G¨ting. u An introduction to spatial database systems. VLDB J. , 3(4):357-399, 1994.                                                                                                                                                                                                                                                        |\n",
      "| [56]   | Ralf Hartmut G¨ting, u Thomas Behr, and Christian D¨ntgen. u SECONDO: A platform for moving objects database research and for publishing and integrating research implementations. IEEE Data Eng. Bull. , 33(2):56-63, 2010.                                                                                                                             |\n",
      "| [57]   | Ditiman Hazarika, Gopal Konwar, Shuvam Deb, and Dibya Jyoti Bora. Sentiment analysis on twitter by using textblob for natural language processing. In ICRMAT , volume 24 of Annals of Computer Science and Information Systems , pages 63-67, 2020.                                                                                                      |\n",
      "| [58]   | Jos´ e Henarejos-Blasco, Jos´ e Antonio Garc´a-D´az, ı ı ´ Oscar Apolinario-Arzube, and Rafael Valencia-Garc´a. ı Cnl-rdf-query: a controlled natural language interface for querying ontologies and relational databases. In EATIS , pages 35:1-35:5, 2020.                                                                                             |\n",
      "| [59]   | Yiqun Hu, Yiyun Zhao, Jiarong Jiang, Wuwei Lan, Henghui Zhu, Anuj Chauhan, Alexander Hanbo Li, Lin Pan, Jun Wang, Chung-Wei Hang, Sheng Zhang, Jiang Guo, Mingwen Dong, Joseph Lilien, Patrick Ng, Zhiguo Wang, Vittorio Castelli, and Bing Xiang. Importance of synthesizing high-quality data for text-to-sql parsing. In ACL , pages 1327-1343, 2023. |\n",
      "| [60]   | Ruizhe Huang and Lei Zou. Natural language question answering over RDF data. In SIGMOD , pages 1289-1290, 2013.                                                                                                                                                                                                                                          |\n",
      "| [61]   | Zachary G. Ives. Technical perspective: : Natural language explanations for query results. SIGMOD Rec. , 47(1):41, 2018.                                                                                                                                                                                                                                 |\n",
      "| [62]   | Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer. Learning a neural semantic parser from user feedback. In ACL , pages 963-973, 2017.                                                                                                                                                                          |\n",
      "| [63]   | Manasa Jammi, Jaydeep Sen, Ashish R. Mittal, Sagar Verma, Vardaan Pahuja, Rema Ananthanarayanan, Pranay Lohia, Hima Karanam, Diptikalyan Saha, and Karthik Sankaranarayanan. Tooling framework for instantiating natural language querying system. Proc. VLDB Endow. , 11(12):2014-2017, 2018.                                                           |\n",
      "| [64]   | Zhen Jia, Abdalghani Abujabal, Rishiraj Saha Roy, Jannik Str¨tgen, o and Gerhard Weikum. TEQUILA: temporal question answering over knowledge bases. In CIKM , pages 1807-1810, 2018.                                                                                                                                                                     |\n",
      "| [65]   | Jiffy Joseph, Janu R Panicker, and Meera M. An efficient natural language interface to xml database. In ICIS , pages 207-212, 2016.                                                                                                                                                                                                                      |\n",
      "| [66]   | Aishwarya Kamath and Rajarshi Das. A survey on semantic parsing. In AKBC , 2019.                                                                                                                                                                                                                                                                         |\n",
      "| [67]   | Ronak Kaoshik, Rohit Patil, Prakash R, Shaurya Agarawal, Naman Jain, and Mayank Singh. ACL-SQL:                                                                                                                                                                                                                                                          |\n",
      "\n",
      "| [68]   | George Katsogiannis-Meimarakis and Georgia Koutrika. Asurvey on deep learning approaches for text-to- sql. VLDB J. , 32(4):905-936, 2023.                                                                                                   |\n",
      "|--------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| [69]   | Esther Kaufmann, Abraham Bernstein, and Renato Zumstein. Querix: Anatural language interface to query ontologies based on clarification dialogs. In ISWC , 2006.                                                                            |\n",
      "| [70]   | Hyeonji Kim, Byeong-Hoon So, Wook-Shin Han, and Hongrae Lee. Natural language to SQL: where are we today? Proc. VLDB Endow. , 13(10):1737-1750, 2020.                                                                                       |\n",
      "| [71]   | Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.                                                                                                                                                |\n",
      "| [72]   | Georgia Koutrika. Natural language data interfaces: Adata access odyssey (invited talk). In ICDT , volume 290 of LIPIcs , pages 1:1-1:22, 2024.                                                                                             |\n",
      "| [73]   | Georgia Koutrika, Alkis Simitsis, and Yannis E. Ioannidis. Explaining structured queries in natural lan- guage. In ICDE , pages 333-344, 2010.                                                                                              |\n",
      "| [74]   | Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gardner. Neural semantic parsing with type constraints for semi-structured tables. In EMNLP , pages 1516-1526, 2017.                                                                         |\n",
      "| [75]   | Claude Lehmann, Dennis Gehrig, Stefan Holdener, Carlo Saladin, Jo˜o a Pedro Monteiro, and Kurt Stockinger. Building natural language interfaces for databases in practice. In SSDBM , pages 20:1-20:4, 2022.                                |\n",
      "| [76]   | Mike Lewis, Yinhan Liu, Naman Goyal, and et al. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In ACL , pages 7871-7880, 2020.                                          |\n",
      "| [77]   | Fei Li and H. V. Jagadish. Constructing an interactive natural language interface for relational databases. Proc. VLDB Endow. , 8(1):73-84, 2014.                                                                                           |\n",
      "| [78]   | Fei Li and H. V. Jagadish. Nalir: an interactive natural language interface for querying relational databases. In SIGMOD , pages 709-712, 2014.                                                                                             |\n",
      "| [79]   | Fei Li and H. V. Jagadish. Understanding natural language queries over relational databases. SIGMOD Rec. , 45(1):6-13, 2016.                                                                                                                |\n",
      "| [80]   | Jingjing Li, Wenlu Wang, Wei-Shinn Ku, Yingtao Tian, and Haixun Wang. Spatialnli: A spatial domain natural language interface to databases using spatial comprehension. In ACMSIGSPATIAL , pages 339-348, 2019.                             |\n",
      "| [81]   | Yunyao Li, Ishan Chaudhuri, Huahai Yang, Satinder Singh, and H. V. Jagadish. Danalix: a domain-adaptive natural language interface for querying XML. In SIGMOD , pages 1165-1168, 2007.                                                     |\n",
      "| [82]   | Yunyao Li and Davood Rafiei. Natural language data management and interfaces: Recent development and open challenges. In ACM SIGMOD , pages 1765-1770, 2017.                                                                                |\n",
      "| [83]   | Yunyao Li and Davood Rafiei. Natural Language Data Management and Interfaces . Synthesis Lectures on Data Management. 2018.                                                                                                                 |\n",
      "| [84]   | Yunyao Li, Huahai Yang, and H. V. Jagadish. Nalix: an interactive natural language interface for querying XML. In SIGMOD , pages 900-902, 2005.                                                                                             |\n",
      "| [85]   | Yunyao Li, Huahai Yang, and H. V. Jagadish. Nalix: A generic natural language search environment for XML data. ACM Trans. Database Syst. , 32(4):30, 2007.                                                                                  |\n",
      "| [86]   | Huadai Liu, Rongjie Huang, Jinzheng He, Gang Sun, Ran Shen, Xize Cheng, and Zhou Zhao. Wav2sql: Direct generalizable speech-to-sql parsing. CoRR , abs/2305.12552, 2023.                                                                    |\n",
      "| [87]   | Jian Liu, Qian Cui, Hongwei Cao, Tianyuan Shi, and Min Zhou. Auto-conversion from natural language to structured query language using neural networks embedded with pre-training and fine-tuning mechanism. In CAC , pages 6651-6654, 2020. |\n",
      "| [88]   | Mengyi Liu, Xieyang Wang, and Jianqiu Xu. NALSD: A natural language interface for spatial databases. In SSTD , pages 175-179, 2023.                                                                                                         |\n",
      "\n",
      "| [89]   | Mengyi Liu, Xieyang Wang, Jianqiu Xu, and Hua Lu. Nalspatial: An effective natural language transfor- mation framework for queries over spatial data. In SIGSPATIAL/GIS , pages 57:1-57:4, 2023.                                                                                  |\n",
      "|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| [90]   | Yuyu Luo, Nan Tang, Guoliang Li, Chengliang Chai, Wenbo Li, and Xuedi Qin. Synthesizing natural language to visualization (NL2VIS) benchmarks from NL2SQL benchmarks. In SIGMOD , pages 1235- 1247, 2021.                                                                         |\n",
      "| [91]   | Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky. The stanford corenlp natural language processing toolkit. In ACL , pages 55-60, 2014.                                                                                  |\n",
      "| [92]   | Youssef Mellah, Abdelkader Rhouati, El Hassane Ettifouri, Toumi Bouchentouf, and Mohammed Ghaouth Belkasmi. COMBINE: A pipeline for SQL generation from natural language. In ICACDS , volume 1441 of Communications in Computer and Information Science , pages 97-106, 2021.     |\n",
      "| [93]   | Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representa- tions of words and phrases and their compositionality. In NIPS , pages 3111-3119, 2013.                                                                                    |\n",
      "| [94]   | Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Comput. Surv. , 56(2):30:1-30:40, 2024. |\n",
      "| [95]   | Mohamed F. Mokbel, Mahmoud Attia Sakr, Li Xiong, Andreas Z¨fle, u and et al. Mobility data science (dagstuhl seminar 22021). Dagstuhl Reports , 12(1):1-34, 2022.                                                                                                                 |\n",
      "| [96]   | Kevin Mote. Natural language processing - A survey. CoRR , abs/1209.6238, 2012.                                                                                                                                                                                                   |\n",
      "| [97]   | Linyong Nan, Yilun Zhao, Weijin Zou, Narutatsu Ri, Jaesung Tae, Ellen Zhang, Arman Cohan, and Dragomir Radev. Enhancing text-to-sql capabilities of large language models: A study on prompt design strategies. In EMNLP , pages 14935-14956, 2023.                               |\n",
      "| [98]   | Long Ouyang, Jeffrey Wu, Xu Jiang, and et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730-27744, 2022.                                                                                      |\n",
      "| [99]   | Fatma Ozcan, Abdul Quamar, Jaydeep Sen, Chuan Lei, and Vasilis Efthymiou. State of the art and open challenges in natural language interfaces to data. In SIGMOD , pages 2629-2636, 2020.                                                                                         |\n",
      "| [100]  | Parth Parikh, Oishik Chatterjee, Muskan Jain, Aman Harsh, Gaurav Shahani, Rathin Biswas, and Kavi Arya. Auto-query - A simple natural language to SQL query generator for an e-learning platform. In EDUCON , pages 936-940, 2022.                                                |\n",
      "| [101]  | Bijan Parsia. Querying the web with SPARQL. In Reasoning Web , volume 4126 of Lecture Notes in Computer Science , pages 53-67, 2006.                                                                                                                                              |\n",
      "| [102]  | Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In ACL , pages 1470-1480, 2015.                                                                                                                                                       |\n",
      "| [103]  | Rodolfo A. Pazos, Jos´ e A. Mart´nez ı F., Juan Javier Gonz´lez a Barbosa, and Andr´s e A. Ver´stegui a O. Al- gorithm for processing queries that involve boolean columns for a natural language interface to databases. Computaci´n o y Sistemas , 24(1), 2020.                 |\n",
      "| [104]  | Rodolfo A. Pazos, Jos´ e A. Mart´nez ı F., and Alan G. Aguirre L. Processing natural language queries via a natural language interface to databases with design anomalies. Polibits , 62:43-50, 2020.                                                                             |\n",
      "| [105]  | Hoifung Poon and Pedro M. Domingos. Unsupervised semantic parsing. In EMNLP , pages 1-10, 2009.                                                                                                                                                                                   |\n",
      "| [106]  | Ana-Maria Popescu, Alex Armanasu, Oren Etzioni, David Ko, and Alexander Yates. Modern natural lan- guage interfaces to databases: Composing statistical parsing with semantic tractability. In COLING , 2004.                                                                     |\n",
      "| [107]  | Ana-Maria Popescu, Oren Etzioni, and Henry A. Kautz. Towards a theory of natural language interfaces to databases. In IUI , pages 149-157, 2003.                                                                                                                                  |\n",
      "| [108]  | Patti J. Price. Evaluation of spoken language systems: the ATIS domain. In Speech and Natural Language ,                                                                                                                                                                          |\n",
      "\n",
      "| [109]   | Qinjun Qiu, Zhong Xie, Kai Ma, Liufeng Tao, and Shiyu Zheng. Neurospe: A neuro-net spatial relation extractor for natural language text fusing gazetteers and pretrained models. Trans. GIS , 27(5):1526-1549, 2023.                                                                                                                                                                                    |\n",
      "|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| [110]   | Xipeng Qiu, Tianxiang Sun, Yige Xu, and et al. Pre-trained models for natural language processing: A survey. Sci. China Technol. Sci. , 63(10):1872-1897, 2020.                                                                                                                                                                                                                                         |\n",
      "| [111]   | Colin Raffel, Noam Shazeer, Adam Roberts, and et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. , 21:140:1-140:67, 2020.                                                                                                                                                                                                                   |\n",
      "| [112]   | Ohad Rubin and Jonathan Berant. Smbop: Semi-autoregressive bottom-up semantic parsing. In NAACL- HLT , pages 311-324, 2021.                                                                                                                                                                                                                                                                             |\n",
      "| [113]   | Diptikalyan Saha, Avrilia Floratou, Karthik Sankaranarayanan, Umar Farooq Minhas, Ashish R. Mittal, and Fatma ¨ Ozcan. ATHENA: an ontology-driven system for natural language querying over relational data stores. Proc. VLDB Endow. , 9(12):1209-1220, 2016.                                                                                                                                          |\n",
      "| [114]   | Xavier Schmitt, Sylvain Kubler, J´r´my e e Robert, Mike Papadakis, and Yves Le Traon. A replicable com- parison study of NER software: Stanfordnlp, nltk, opennlp, spacy, gate. In SNAMS , pages 338-343, 2019.                                                                                                                                                                                         |\n",
      "| [115]   | Jaydeep Sen, Chuan Lei, Abdul Quamar, Fatma ¨ Ozcan, Vasilis Efthymiou, Ayushi Dalmia, Greg Stager, Ashish R. Mittal, Diptikalyan Saha, and Karthik Sankaranarayanan. ATHENA++: natural language query- ing for complex nested SQL queries. Proc. VLDB Endow. , 13(11):2747-2759, 2020.                                                                                                                 |\n",
      "| [116]   | Jaydeep Sen, Fatma Ozcan, Abdul Quamar, Greg Stager, Ashish R. Mittal, Manasa Jammi, Chuan Lei, Dip- tikalyan Saha, and Karthik Sankaranarayanan. Natural language querying of complex business intelligence queries. In SIGMOD , pages 1997-2000, 2019.                                                                                                                                                |\n",
      "| [117]   | Vraj Shah. Speakql: Towards speech-driven multimodal querying. In SIGMOD , pages 1847-1849, 2019.                                                                                                                                                                                                                                                                                                       |\n",
      "| [118]   | Vraj Shah, Side Li, Arun Kumar, and Lawrence K. Saul. Speakql: Towards speech-driven multimodal querying of structured data. In SIGMOD , pages 2363-2374, 2020.                                                                                                                                                                                                                                         |\n",
      "| [119]   | Vraj Shah, Side Li, Kevin Yang, Arun Kumar, and Lawrence K. Saul. Demonstration of speakql: Speech- driven multimodal querying of structured data. In SIGMOD , pages 2001-2004, 2019.                                                                                                                                                                                                                   |\n",
      "| [120]   | Grigori Sidorov, Rodolfo A. Pazos Rangel, Jos´ e A. Mart´nez ı F., Juan Mart´n ı Carpio, and Alan G. Aguirre L. Configuration module for treating design anomalies in databases for a natural language interface to databases. In Intuitionistic and Type-2 Fuzzy Logic Enhancements in Neural and Optimization Algorithms , volume 862 of Studies in Computational Intelligence , pages 703-714. 2020. |\n",
      "| [121]   | Dezhao Song, Frank Schilder, Charese Smiley, Chris Brew, Tom Zielund, Hiroko Bretz, Robert Martin, Chris Dale, John Duprey, Tim Miller, and Johanna Harrison. TR discover: A natural language interface for querying and analyzing interlinked datasets. In ISWC , pages 21-37, 2015.                                                                                                                   |\n",
      "| [122]   | Yuanfeng Song, Raymond Chi-Wing Wong, Xuefang Zhao, and Di Jiang. Speech-to-sql: Towards speech- driven SQL query generation from natural language question. CoRR , abs/2201.01209, 2022.                                                                                                                                                                                                               |\n",
      "| [123]   | Yuanfeng Song, Raymond Chi-Wing Wong, Xuefang Zhao, and Di Jiang. Voicequerysystem: A voice- driven database querying system using natural language questions. In SIGMOD , pages 2385-2388, 2022.                                                                                                                                                                                                       |\n",
      "| [124]   | Niculae Stratica, Leila Kosseim, and Bipin C. Desai. Using semantic templates for a natural language interface to the CINDI virtual library. Data Knowl. Eng. , 55(1):4-19, 2005.                                                                                                                                                                                                                       |\n",
      "| [125]   | Shuo Sun, Yuze Gao, Yuchen Zhang, Jian Su, Bin Chen, Yingzhan Lin, and Shuqi Sun. An exploratory study on model compression for text-to-sql. In ACL , pages 11647-11654, 2023.                                                                                                                                                                                                                          |\n",
      "| [126]   | Shuo Sun, Yuchen Zhang, Jiahuan Yan, Yuze Gao, Donovan Ong, Bin Chen, and Jian Su. Battle of the large language models: Dolly vs llama vs vicuna vs guanaco vs bard vs chatgpt - A text-to-sql parsing comparison. In EMNLP , pages 11225-11238, 2023.                                                                                                                                                  |\n",
      "| [127]   | Lappoon R. Tang and Raymond J. Mooney. Automated construction of database interfaces: Intergrating                                                                                                                                                                                                                                                                                                      |\n",
      "\n",
      "- [127] Lappoon R. Tang and Raymond J. Mooney. Automated construction of database interfaces: Intergrating statistical and relational learning for semantic parsing. In EMNLP , pages 133-141, 2000.\n",
      "\n",
      "| [128]   | Lappoon R. Tang and Raymond J. Mooney. Using multiple clause constructors in inductive logic program- ming for semantic parsing. In EMCL , volume 2167 of Lecture Notes in Computer Science , pages 466-477, 2001.                                                                     |\n",
      "|---------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| [129]   | Peihao Tong, Qifan Zhang, and Junjie Yao. Leveraging domain context for question answering over knowl- edge graph. Data Sci. Eng. , 4(4):323-335, 2019.                                                                                                                                |\n",
      "| [130]   | Immanuel Trummer. Database tuning using natural language processing. SIGMOD Rec. , 50(3):27-28, 2021.                                                                                                                                                                                  |\n",
      "| [131]   | Arif Usta, Akifhan Karakayali, and ¨ Ozg¨r u Ulusoy. Dbtagger: Multi-task learning for keyword mapping in nlidbs using bi-directional recurrent neural networks. Proc. VLDB Endow. , 14(5):813-821, 2021.                                                                              |\n",
      "| [132]   | Arif Usta, Akifhan Karakayali, and ¨ Ozg¨r u Ulusoy. xdbtagger: explainable natural language interface to databases using keyword mappings and schema graph. VLDB J. , 33(2):301-321, 2024.                                                                                            |\n",
      "| [133]   | Prasetya Utama, Nathaniel Weir, Fuat Basik, Carsten Binnig, Ugur Cetintemel, ¸ Benjamin H¨ttasch, a Amir Ilkhechi, Shekar Ramaswamy, and Arif Usta. Anend-to-end neural natural language interface for databases. CoRR , abs/1804.00401, 2018.                                         |\n",
      "| [134]   | Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS , pages 5998-6008, 2017.                                                                                              |\n",
      "| [135]   | Moses Visperas, Aunhel John Adoptante, Christalline Joie Borjal, Ma. Teresita Abia, Jasper Kyle Catapang, and Elmer C. Peramo. On modern text-to-sql semantic parsing methodologies for natural language interface to databases: A comparative study. In ICAIIC , pages 390-396, 2023. |\n",
      "| [136]   | Ngoc Phuoc An Vo, Octavian Popescu, Irene Manotas, and Vadim Sheinin. Tackling temporal questions in natural language interface to databases. In EMNLP , pages 179-187, 2022.                                                                                                          |\n",
      "| [137]   | Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. RAT-SQL: relation-aware schema encoding and linking for text-to-sql parsers. In ACL , pages 7567-7578, 2020.                                                                                       |\n",
      "| [138]   | Runze Wang, Zhen-Hua Ling, Jing-Bo Zhou, and Yu Hu. A multiple-integration encoder for multi-turn text-to-sql semantic parsing. IEEE ACM Trans. Audio Speech Lang. Process. , 29:1503-1513, 2021.                                                                                      |\n",
      "| [139]   | Weiguo Wang, Sourav S. Bhowmick, Hui Li, Shafiq R. Joty, Siyuan Liu, and Peng Chen. Towards en- hancing database education: Natural language generation meets query execution plans. In SIGMOD , pages 1933-1945, 2021.                                                                |\n",
      "| [140]   | Wenlu Wang. A cross-domain natural language interface to databases using adversarial text method. In VLDB , volume 2399 of CEUR Workshop Proceedings . CEUR-WS.org, 2019.                                                                                                              |\n",
      "| [141]   | Wenlu Wang, Jingjing Li, Wei-Shinn Ku, and Haixun Wang. Multilingual spatial domain natural language interface to databases. GeoInformatica , 28(1):29-52, 2024.                                                                                                                       |\n",
      "| [142]   | Wenlu Wang, Yingtao Tian, Haixun Wang, and Wei-Shinn Ku. A natural language interface for database: Achieving transfer-learnability using adversarial method for question understanding. In ICDE , pages 97- 108, 2020.                                                                |\n",
      "| [143]   | Xieyang Wang, Mengyi Liu, Jianqiu Xu, and Hua Lu. NALMO: transforming queries in natural language for moving objects databases. GeoInformatica , 27(3):427-460, 2023.                                                                                                                  |\n",
      "| [144]   | Xieyang Wang, Jianqiu Xu, and Hua Lu. NALMO: A natural language interface for moving objects databases. In SSTD , pages 1-11, 2021.                                                                                                                                                    |\n",
      "| [145]   | Xieyang Wang, Jianqiu Xu, and Yaxin Wang. NLMO: towards a natural language tool for querying moving objects. In MDM , pages 228-229, 2020.                                                                                                                                             |\n",
      "| [146]   | Yushi Wang, Jonathan Berant, and Percy Liang. Building a semantic parser overnight. In ACL , pages 1332-1342, 2015.                                                                                                                                                                    |\n",
      "| [147]   | Ziyun Wei, Immanuel Trummer, and Connor Anderson. Demonstrating robust voice querying with MUVE: optimally visualizing results of phonetically similar queries. In SIGMOD , pages 2798-2802, 2021.                                                                                     |\n",
      "\n",
      "| [148]   | Ziyun Wei, Immanuel Trummer, and Connor Anderson. Robust voice querying with MUVE: optimally visualizing results of phonetically similar queries. Proc. VLDB Endow. , 14(11):2397-2409, 2021.                                                                                                                   |\n",
      "|---------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| [149]   | Nathaniel Weir and Prasetya Utama. Bootstrapping an end-to-end natural language interface for databases. In SIGMOD , pages 1862-1864, 2019.                                                                                                                                                                     |\n",
      "| [150]   | Yuk Wah Wong and Raymond J. Mooney. Learning for semantic parsing with statistical machine trans- lation. In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics , 2006.                                                                         |\n",
      "| [151]   | Chunyang Xiao, Marc Dymetman, and Claire Gardent. Sequence-based structured prediction for semantic parsing. In ACL , 2016.                                                                                                                                                                                     |\n",
      "| [152]   | Xiaojun Xu, Chang Liu, and Dawn Song. Sqlnet: Generating structured queries from natural language without reinforcement learning. CoRR , abs/1711.04436, 2017.                                                                                                                                                  |\n",
      "| [153]   | Navid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and Thomas Dillig. Sqlizer: query synthesis from natural language. Proc. ACM Program. Lang. , 1(OOPSLA):63:1-63:26, 2017.                                                                                                                                         |\n",
      "| [154]   | Yuquan Yang, Qifan Zhang, and Junjie Yao. Task-driven neural natural language interface to database. In WISE , volume 14306 of Lecture Notes in Computer Science , pages 659-673, 2023.                                                                                                                         |\n",
      "| [155]   | Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. In NeurIPS , pages 5754-5764, 2019.                                                                                                   |\n",
      "| [156]   | Ziyu Yao, Yu Su, Huan Sun, and Wen-tau Yih. Model-based interactive semantic parsing: A unified framework and A text-to-sql case study. In EMNLP-IJCNLP , pages 5446-5457, 2019.                                                                                                                                |\n",
      "| [157]   | Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code generation. In ACL , pages 440-450, 2017.                                                                                                                                                                                    |\n",
      "| [158]   | Tao Yu, Zifan Li, Zilin Zhang, Rui Zhang, and Dragomir R. Radev. Typesql: Knowledge-based type-aware neural text-to-sql generation. In NAACL-HLT , pages 588-594, 2018.                                                                                                                                         |\n",
      "| [159]   | Tao Yu, Michihiro Yasunaga, Kai Yang, Rui Zhang, Dongxu Wang, Zifan Li, and Dragomir R. Radev. Syntaxsqlnet: Syntax tree networks for complex and cross-domain text-to-sql task. In EMNLP , pages 1653-1663, 2018.                                                                                              |\n",
      "| [160]   | Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir R. Radev. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In EMNLP , pages 3911-3921, 2018. |\n",
      "| [161]   | John M. Zelle and Raymond J. Mooney. Learning to parse database queries using inductive logic program- ming. In AAAI IAAI , pages 1050-1055, 1996.                                                                                                                                                              |\n",
      "| [162]   | Gideon Zenz, Xuan Zhou, Enrico Minack, Wolf Siberski, and Wolfgang Nejdl. From keywords to semantic queries - incremental query construction on the semantic web. J. Web Semant. , 7(3):166-176, 2009.                                                                                                          |\n",
      "| [163]   | Luke S. Zettlemoyer and Michael Collins. Learning to map sentences to logical form: Structured classifi- cation with probabilistic categorial grammars. In UAI , pages 658-666, 2005.                                                                                                                           |\n",
      "| [164]   | Jinchuan Zhang, Yan Zhou, Binyuan Hui, Yaxin Liu, Ziming Li, and Songlin Hu. Trojansql: SQL injection against natural language interface to database. In EMNLP , pages 4344-4359, 2023.                                                                                                                         |\n",
      "| [165]   | Bolong Zheng, Lei Bi, Juan Cao, Hua Chai, Jun Fang, Lu Chen, Yunjun Gao, Xiaofang Zhou, and Chris- tian S. Jensen. Speaknav: Voice-based route description language understanding for template driven path search. Proc. VLDB Endow. , 14(12):3056-3068, 2021.                                                  |\n",
      "| [166]   | Weiguo Zheng, Hong Cheng, Lei Zou, Jeffrey Xu Yu, and Kangfei Zhao. Natural language ques- tion/answering: Let users talk with the knowledge graph. In ACM CIKM , pages 217-226, 2017.                                                                                                                          |\n",
      "| [167]   | Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. CoRR , abs/1709.00103, 2017.                                                                                                                                        |\n",
      "| [168]   | Lei Zou, Ruizhe Huang, Haixun Wang, Jeffrey Xu Yu, Wenqiang He, and Dongyan Zhao. Natural language question answering over RDF: a graph data driven approach. In SIGMOD , pages 313-324, 2014.                                                                                                                  |\n"
     ]
    }
   ],
   "source": [
    "source = \"https://arxiv.org/pdf/2503.02435\"\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(source)\n",
    "print(result.document.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "398a1d79-dcd1-4e67-ba04-5645aadfc378",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = \"\"\"\n",
    "## NLI4DB: A Systematic Review of Natural Language Interfaces for Databases\n",
    "\n",
    "## Mengyi Liu and Jianqiu Xu *\n",
    "\n",
    "Nanjing University of Aeronautics and Astronautics, Nanjing, China { liumengyi,jianqiu } @nuaa.edu.cn\n",
    "\n",
    "## Abstract\n",
    "\n",
    "As the demand for querying databases in all areas of life continues to grow, researchers have devoted significant attention to the natural language interface for databases (NLIDB). This paper presents a comprehensive survey of recently proposed NLIDBs. We begin with a brief introduction to natural language processing techniques, executable database languages and the intermediate representation between natural language and executable language, and then provide an overview of the translation process from natural language to executable database language. The translation process is divided into three stages: (i) natural language preprocessing , (ii) natural language understanding , and (iii) natural language translation . Traditional and data-driven methods are utilized in the preprocessing stage. Traditional approaches rely on predefined rules and grammars, and involve techniques such as regular expressions, dependency parsing and named entity recognition. Data-driven approaches depend on large-scale data and machine learning models, using techniques including word embedding and pattern linking. Natural language understanding methods are classified into three categories: (i) rule-based , (ii) machine learning-based , and (iii) hybrid . We then describe a general construction process for executable languages over relational and spatio-temporal databases. Subsequently, common benchmarks and evaluation metrics for transforming natural language into executable language are presented, and methods for generating new benchmarks are explored. Finally, we summarize the classification, development, and enhancement of NLIDB systems, and discuss deep language understanding and database interaction techniques related to NLIDB, including (i) using LLM for Text2SQL tasks , (ii) generating natural language interpretations from SQL , and (iii) transforming speech queries into SQL .\n",
    "\n",
    "Keywords: Natural language interface for database, Semantic parsing, Structured language, Query processing\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "In today's data-driven world, databases are the backbone of a number of applications, from social media platforms to financial systems. However, accessing and querying these vast repositories of information often requires specialized knowledge of query languages such as SQL, which can be a significant barrier for non-expert users, limiting their ability to harness the full potential of the data at their fingertips. The advent of natural language interface (NLI) has the potential to eliminate the interaction barrier between users and terminals [130]. The integration of natural language processing (NLP) and database technology represents an intriguing avenue for future research. There are systems that facilitate the transformation of natural language into structured language [141, 22], provide the natural language description for query execution plans [139, 23], and transform SQL into natural language [40, 132].\n",
    "\n",
    "Imagine a world where anyone, regardless of technical proficiency, can effortlessly interact with complex databases using everyday language. This vision is becoming a reality through the development of natural language\n",
    "\n",
    "* Corresponding author.\n",
    "\n",
    "<!-- image -->\n",
    "\n",
    "Translated Executable Language\n",
    "\n",
    "Figure 1: Example of translating a natural language into an executable language\n",
    "\n",
    "<!-- image -->\n",
    "\n",
    "Figure 2: A summary of translation techniques\n",
    "\n",
    "interface for databases (NLIDB), which aims to transform a natural language query (NLQ) into an executable language, as illustrated in Figure 1. Users tend to favor an interactive interface that allows them to confirm the accuracy and precision of the generated structured language [95]. The NLIDB enables users to avoid the necessity of possessing expertise in structured query languages and database schema, thereby significantly streamlining the efforts of users and enhancing the benefits of utilizing databases [70]. The initial NLIDBs, including BASEBALL, LUNAR, LADDER, Chat-80, and ASK, were released in rapid succession [2]. Subsequently, NLIDBs have emerged and are primarily utilized in relational databases (e.g., GENSQL [44] and CatSQL [48]), spatial domains (e.g., SpatialNLI [80, 141] and NALSpatial [89]), RDF question and answer (e.g., Querix [69] and TEQUILA [64]), and XML databases (e.g., NaLIX [84, 85] and DaNaLIX [81]).\n",
    "\n",
    "Despite years of research, the landscape of NLIDB is fraught with challenges [9, 82]. The inherent ambiguity and variability of natural language make NLIDB difficult to ensure accurate query interpretation. Additionally, understanding the structure and semantics of different databases adds another layer of complexity. Furthermore, achieving real-time performance while maintaining high accuracy in query translation remains an ongoing challenge. While large language models (LLMs) offer new avenues for querying databases using natural language, the training and reasoning of such models necessitate a substantial amount of computational resources, which may prove challenging to implement in resource-limited scenarios [97]. Moreover, the decision-making process of LLMs is frequently opaque and lacks interpretability, making it difficult to ascertain whether the generated query results align with the user's intent [126]. These obstacles underscore the need for continued research and development to refine NLIDB.\n",
    "\n",
    "In light of these observations, this systematic review explores the current state of NLIDB, examining the various approaches and technologies that have been proposed to connect natural language with database querying, named NLI4DB. The aim of this survey is to offer a comprehensive overview that serves as both a valuable reference for researchers and a practical guide for practitioners aiming to implement effective NLIDB solutions. NLI4DB presents a thorough examination of the NLIDB subject, categorizing the work into subtopics and providing in-depth analysis for each one. The translation process from natural language to executable language is divided into three stages: (i) natural language preprocessing , (ii) natural language understanding , and (iii) natural language translation . The three-stage division provides physical independence by separating the physical arrangement of data from the semantics of queries [113]. The techniques for the translation are shown in Figure\n",
    "\n",
    "2.\n",
    "\n",
    "(i) Natural language preprocessing generally involves the construction of dedicated data dictionaries for the domain using stemming extraction and synonym techniques. Part-of-speech tagging and word segmentation are then performed on the input natural language. Methods used in the preprocessing stage include traditional and data-driven. Traditional approaches rely on predefined rules and grammars for domain-specific text processing, and involve techniques such as regular expressions, dependency parsing and named entity recognition (NER). Data-driven approaches depend on large-scale data and machine learning models for complex or variable text processing, using techniques such as word embedding and pattern linking.\n",
    "\n",
    "(ii) Natural language understanding has rule-based, machine learning-based, and hybrid approaches. Rulebased systems can only deal with knowledge bases of specific domains, whose semantic understanding processes either define the concept of semantic accessibility or translate the NLQ into an intermediate representation that can describe the semantics and relationships in an accessible manner. Machine learning-based systems employ a variety of techniques to parse text, including unsupervised approaches, question-and-answer supervised learning, statistical machine translation techniques, encoder-decoder frameworks with recurrent neural networks, and combinations of deterministic algorithms and machine learning. Hybrid approaches combine rules and machine learning techniques to maximize their benefits.\n",
    "\n",
    "(iii) Natural language translation uses distinctive algorithms to map processed key semantic information to corresponding structured language components. To build the SQL, the database elements matched by the NLQ are placed in the appropriate locations in the SELECT, FROM, and WHERE parts. In the event that a query involves multiple relations, it is necessary to include the join condition and the names of the participating relations in the WHERE and FROM clause, respectively. In the process of building an executable language of a spatio-temporal database, the query type of the input NLQ is initially identified. The operators required to build the executable language are then determined according to the query type. Finally, the key semantic information obtained from the natural language understanding stage is integrated to form an executable language.\n",
    "\n",
    "The existing survey [2] related to NLIDB focuses on the comparative analysis of the entire natural language interface system. Affolter et al. [2] divide NLIs into four groups: (i) keyword-based systems , (ii) pattern-based systems , (iii) parsing-based systems , and (iv) grammar-based systems . For each group, they provide an overview of representative systems and describe the most illustrative one in detail. In addition, they systematically compare 24 recently developed NLIDBs on the basis of the sample world designed in the paper. Each system is evaluated using 10 example questions to show the advantages and disadvantages.\n",
    "\n",
    "Compared with Affolter et al. [2], we divide the system translation process into three steps and focus on the comparative analysis of each step. We investigate the recently developed NLIDB systems and divide the translation process into three stages: (i) natural language preprocessing , (ii) natural language understanding , and (iii) natural language translation . We classify natural language preprocessing techniques into traditional and data-driven. Natural language understanding methods are then analyzed in three categories: (i) rule-based , (ii) machine learning-based , and (iii) hybrid . Next, we provide a comprehensive outline of the construction process of executable languages for relational and spatio-temporal databases. Finally, we present commonly used benchmarks and evaluation metrics, and describe the classification, development, and enhancement of NLIDBs.\n",
    "\n",
    "Table 1: Frequently used notations\n",
    "\n",
    "| Name                                    | Abbreviation   |\n",
    "|-----------------------------------------|----------------|\n",
    "| Natural language interface for database | NLIDB          |\n",
    "| Natural language interface              | NLI            |\n",
    "| Natural language query                  | NLQ            |\n",
    "| Natural language processing             | NLP            |\n",
    "| Named entity recognition                | NER            |\n",
    "| Large language model                    | LLM            |\n",
    "| First-order logic                       | FOL            |\n",
    "| Automatic speech recognition            | ASR            |\n",
    "| sequence-to-sequence                    | seq2seq        |\n",
    "\n",
    "The rest of the paper is structured as follows. Section 2 furnishes the background concerning NLIDB, including natural language processing techniques, executable database languages and intermediate representation languages. Section 3 describes the generation of executable database languages in terms of three stages: (i) natural language preprocessing , (ii) natural language understanding and (iii) natural language translation . Section 4 summarizes 11 popular benchmarks for transforming NLQ into SQL and 3 evaluation metrics, including response time, translatability, and translation precision, and explores the methods for generating new benchmarks. Section 5 analyzes the classification, development and enhancement of NLIDBs. Section 6 discuss deep language understanding and database interaction techniques related to NLIDB, including (i) using LLM for Text2SQL tasks , (ii) generating natural language interpretations from SQL , and (iii) transforming speech queries into SQL . Section 7 explores the open problems of NLIDB and concludes the survey. Table 1 summarizes the frequently used notations.\n",
    "\n",
    "## 2 Background: NLP techniques and query languages\n",
    "\n",
    "We introduce the background related to NLIDB, including natural language processing techniques, executable database languages, and intermediate representation languages.\n",
    "\n",
    "## 2.1 Natural language processing techniques\n",
    "\n",
    "NLP is an interdisciplinary discipline that integrates several fields such as linguistics, computer science, and mathematics, and aims to make computers capable of understanding, processing and generating natural language text or speech. Through segmentation, lexical annotation, and syntactic analysis, NLP provides structured processing of text to achieve semantic understanding and information extraction. The application areas of NLP cover machine translation, sentiment analysis, information retrieval, and dialogue systems, providing people with an intelligent and convenient way of language interaction.\n",
    "\n",
    "A Brief History. The earliest research on natural language processing is machine translation. In 1950, Alan Turing proposed the ultimate test for determining the arrival of truly ' intelligent ' machines, which is generally regarded as the inception of the idea of NLP [96]. From the 1950s to the 1970s, the rule-based method was used to process natural language, which was based on grammatical rules and formal logic. In the 1970s, the statistic-based method gradually supplanted the rule-based method. At this juncture, NLP built on mathematical models and statistic made a substantial breakthrough and was applied to practical applications. From 2008 to the present, researchers have introduced deep learning to NLP in response to the achievements in image recognition and speech recognition.\n",
    "\n",
    "The NLP techniques commonly used in NLIDBs are as follows.\n",
    "\n",
    "- (i) Part of speech tagging refers to assigning the correct part of speech to each word in the segmented text,\n",
    "\n",
    "Figure 3: Processing natural language using Stanford CoreNLP\n",
    "\n",
    "<!-- image -->\n",
    "\n",
    "determining whether each word is a noun, verb, or adjective. In NLIDB, part of speech tagging facilitates the identification of the grammatical roles of individual words in natural language queries, leading to an accurate comprehension of users' intent. Taking the natural language query ' All movies starring Brad Pitt from 2000 until 2010. ' as an example, the result of part of speech tagging using Stanford CoreNLP is shown in Figure 3(a). In the figure, DT = determiner; NNS = plural noun; VBG = the gerund or present participle of a verb; NNP = singular proper noun; IN = preposition or subordinating conjunction; CD = cardinal number.\n",
    "\n",
    "- (ii) Lemmatization is the process of reducing the different forms of a word to the original form. In NLIDB, lemmatization is beneficial in unifying words of various tenses and morphs in natural language queries into base forms in order to match the content in the database. Taking the natural language query ' All movies starring Brad Pitt from 2000 until 2010. ' as an example, the result of lemmatization using Stanford CoreNLP is shown in Figure 3(b).\n",
    "- (iii) Named entity recognition is the procedure of identifying entities with specific meanings in natural language text [114]. Generally, the recognized entities can be categorized into three primary groups (entity, temporal, and numeric) and seven subgroups (PERSON, ORGANIZATION, LOCATION, TIME, DATE, MONEY, and PERCENT). NER in NLIDB enables the identification of entities involved in a natural language query to locate the topic and scope of the query. Taking the natural language query ' All movies starring Brad Pitt from 2000 until 2010. ' as an example, the result of NER using Stanford CoreNLP is shown in Figure 3(c).\n",
    "- (iv) Dependency parsing involves analyzing the dependencies between words in natural language sentences. A binary asymmetric relationship between words is called dependency, which is described as an arrow from the head (the subject to be modified) to the dependent (the modifier). Dependency parsing in NLIDB facilitates the understanding of grammatical relationships between words in NLQs, so that the structure and meaning of the query can be accurately understood. Taking the natural language query ' All movies starring Brad Pitt from 2000 until 2010. ' as an example, the result of dependency parsing using Stanford CoreNLP is illustrated in Figure 3(d). In the figure, punct = punctuation; obl = oblique nominal; obj = object; det = determiner; acl = clausal modifier of noun; case = case marking.\n",
    "\n",
    "With the vigorous development of NLP technology, a number of NLP tools are appearing [114]. These tools can perform basic tasks, including dependency parsing, named entity recognition, lemmatization, and part of\n",
    "\n",
    "speech tagging, each of which has distinct advantages and disadvantages. The following is a list of the established open source natural language processing tools.\n",
    "\n",
    "- (i) NLTK is a natural language processing toolkit using Python as the programming language. NLTK has complete functions and realizes many of the functional components in natural language processing, such as named entity recognition, sentence structure analysis, part-of-speech tagging, and text classification [13]. Born for the academic field, NLTK is suitable for study and research. The disadvantage is that NLTK has a slower processing speed than other tools.\n",
    "- (ii) spaCy , a commercial open source software, is an industrial-grade natural language processing software programmed in Python and Cython languages [45]. spaCy, which follows NLTK, includes pre-trained statistical models and word vectors. spaCy can break down text into semantic units like articles, words and punctuation, and support named entity recognition. spaCy is characterized by fast and accurate syntax analysis, and comprehensive functions ranging from simple part-of-speech tagging to advanced deep learning.\n",
    "- (iii) Stanford CoreNLP is a tool set developed by Stanford University using the Java programming language. Stanford CoreNLP supports a variety of natural languages and has rich interfaces for programming languages that can be used without Java [91]. Stanford CoreNLP is an efficient tool created by high-level research institutions and is widely used in scientific research and experiments, but may incur additional costs in production systems. Stanford CoreNLP may not be the best choice for industry.\n",
    "- (iv) TextBlob is an extension to NLTK, which provides an easier way to use the functionality of NLTK [57]. TextBlob supports sentiment analysis, tokenization, part-of-speech tagging, and text classification. One of the advantages is that TextBlob can be used in production environments where performance requirements are not too high. TextBlob can be applied in a wide range of scenarios, especially for small projects.\n",
    "\n",
    "## 2.2 Executable database languages\n",
    "\n",
    "The output of NLIDB is an executable database language, and we present executable languages over relational data, RDF data, and spatial data.\n",
    "\n",
    "## 2.2.1 Query language for relational data\n",
    "\n",
    "The standard executable query language for relational data is SQL. Such a language is a general-purpose, extremely powerful relational database language whose functions are not limited to querying, but also include creating database schema, inserting and modifying data, and defining and controlling database security integrity [29]. Following the establishment of SQL as an international standard language, numerous database manufacturers have released SQL-compatible software, including both database management systems and interfaces. Consequently, SQL serves as the universal data access language and standard interface for most databases, fostering a shared foundation for interoperability among different database systems. SQL has become the mainstream language in the database field which is of great significance.\n",
    "\n",
    "SQL provides the SELECT statement for querying data, which has flexible usage and rich functionality. The SELECT statement can perform simple single-table queries as well as complex join queries and nested queries, whose general format is:\n",
    "\n",
    "SELECT [ALL DISTINCT] | &lt; target column expression &gt; [alias] [, &lt; target column expression &gt; [alias]] FROM &lt; table name or view name &gt; [, &lt; table name or view name &gt; ] | (SELECT statement) [AS] &lt; alias [ WHERE &lt; conditional expression &gt; ]\n",
    "\n",
    "&gt; [ASC DESC]];\n",
    "\n",
    "[ GROUP BY &lt; column name 1 &gt; [ HAVING &lt; conditional expression &gt; ]]\n",
    "\n",
    "[ ORDER BY &lt; column name 2 &gt; |\n",
    "\n",
    "The purpose of the SELECT statement is to find the tuples that satisfy the conditions specified in the FROM clause, which may be a basic table, view, or derived table ,according to the conditional expression in the WHERE clause.\n",
    "\n",
    "The attribute value in the tuple is then selected on the basis of the target column expression in the SELECT clause to form the result table. When a GROUP BY clause is present, the output is organized by the value of &lt; column name 1 &gt; , where tuples sharing identical attribute column values are grouped together. Aggregation functions are usually applied to each group. When the GROUP BY clause is accompanied by a HAVING clause, the output will only include groups that satisfy the specified conditions. If an ORDER BY clause is present, the result table is sorted in ascending or descending order according to the values of &lt; column name 2 &gt; .\n",
    "\n",
    "## 2.2.2 Query language for RDF data\n",
    "\n",
    "The complete designation of RDF is Resource Description Framework, which is a data model designed to represent information about resources on the Internet. The data model typically describes a fact composed of three parts known as a triple, including (i) a subject , (ii) a predicate , and (iii) an object . An RDF graph contains multiple triples. RDF documents are written in XML to offer a standardized method for describing information. RDF is intended for computer applications to read and understand, rather than for visual presentation to web users.\n",
    "\n",
    "SPARQL is a specialized query language and data retrieval protocol designed for RDF, which stands for SPARQL Protocol and RDF Query Language [24]. SPARQL is a query language over RDF graphs, where the database is represented as a collection of ' subject-predicate-object ' triples. Although RDF data is inferential, SPARQL does not have an inference query function. SPARQL is tailored for managing data stored in RDF format, enabling both retrieval and manipulation. SPARQL is composed of the following components.\n",
    "\n",
    "- · The PREFIX clause is employed to declare a prefix with the objective of simplifying the use of URIs. The declaration of the prefix is optional.\n",
    "- · The SELECT clause serves the purpose of specifying the variables returned by a query.\n",
    "- · The WHERE clause is utilized to match data in RDF graphs. The clause contains one or more triple patterns that are employed to indicate the conditions of a query.\n",
    "- · The FILTER clause is designed to conditionally filter the results of a query. The clause can include boolean expressions to limit the set of results matched by the WHERE clause.\n",
    "\n",
    "The fundamental query types of SPARQL are as follows [101].\n",
    "\n",
    "SELECT query is the most frequently used type of query, whose function is to select variables and return a result set. A table is typically generated as the outcome of a SELECT query, which includes the variables that meet the query's criteria along with their corresponding values.\n",
    "\n",
    "CONSTRUCT query is used to generate a new RDF graph by utilizing the query pattern. In contrast to tabular results, the CONSTRUCT query produces an RDF graph that is constructed from the matching data of the query pattern.\n",
    "\n",
    "ASK query is designed to ascertain the existence of RDF data that satisfies the query pattern. The ASK query provides a response in the form of a boolean value (true or false) to indicate the presence or absence of a match.\n",
    "\n",
    "DESCRIBE query is employed to obtain the detailed description of resources. The description is determined by the query engine and typically consists of triples that are directly related to the resource.\n",
    "\n",
    "Each query type employs a WHERE clause to limit the scope of the query. Nevertheless, in the context of DESCRIBE queries, the inclusion of a WHERE clause is not mandatory. To illustrate, the subsequent query retrieves people from the data set who are above the age of 24:\n",
    "\n",
    "PREFIX info: &lt; http://somewhere/peopleInfo# &gt; SELECT ?resource WHERE { ?resource info:age ?age . FILTER (?age &gt; = 24)\n",
    "\n",
    "Table 2: Operators to query spatial data\n",
    "\n",
    "| Operator     | Signature                                                                                                                                                      | Meaning                                                       |\n",
    "|--------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|\n",
    "| distance     | point | line | region × point | line | region → real                                                                                                           | Compute the distance between two spa- tial objects.           |\n",
    "| direction    | point × point → real                                                                                                                                           | Compute the direction between two points.                     |\n",
    "| size         | line → real                                                                                                                                                    | Return the length of a line.                                  |\n",
    "| area         | region → real                                                                                                                                                  | Return the area of a region.                                  |\n",
    "| intersects   | line | region × line | region → bool                                                                                                                           | TRUE, if both arguments intersect.                            |\n",
    "| intersection | point | line | region × point | line | region → T, where T is point if point is one of the arguments, otherwise T is the argument having the smaller dimension | Intersection of two spatial objects.                          |\n",
    "| distancescan | rtree × relation × object × int → stream                                                                                                                       | Compute the integer k nearest neigh- bors for a query object. |\n",
    "\n",
    "In this query, the ' ? ' symbol represents a variable, followed by the variable name. The middle of the ' &lt;&gt; ' symbol is the URI that describes the resource address. The ' info:age ' in the above query is a URI shorthand and stands for ' &lt; http://somewhere/peopleInfo # age &gt; '. The FILTER keyword is employed to impose limitations on the outcomes that are retrieved. In addition, RDF is semi-structured data, and different entities in RDF may have distinct properties. SPARQL is capable of querying information that exists in RDF. However, when querying information that does not exist, SPARQL does not show a failure and does not return any results. The OPTIONAL keyword can then be used to signify that the query is optional, indicating that the query will return a result if the entity has the attribute, and a null value otherwise. The FILTER keyword can also be used in conjunction with the OPTIONAL keyword.\n",
    "\n",
    "## 2.2.3 Query language for spatial data\n",
    "\n",
    "The increasing reliance on geographic information systems in many aspects of people's production and life has led to a significant increase in the demand for spatial data query in all walks of life. The popularity of spatial applications has brought great attention to spatial databases [55]. In databases, fundamental data types utilized for the representation and manipulation of spatial objects include point, line, and region. The common operators to query spatial data are shown in Table 2.\n",
    "\n",
    "Mature systems for storing and managing spatial data include Esri's ArcGIS, PostGIS, Google Earth Engine, GRASS GIS, and SECONDO [56]. As an illustration, SECONDO is a freely available platform created for the purpose of organizing and examining spatial and temporal data. The basic commands of SECONDO are as follows.\n",
    "\n",
    "query &lt; value expression &gt; . The command evaluates the given value expression and subsequently displays the result to the user.\n",
    "\n",
    "let &lt; identifier &gt; = &lt; value expression &gt; . The command initially evaluates the provided value expression in a manner analogous to the preceding command. In contrast to the previous command, the results of the evaluation are not immediately displayed but rather stored in an object named identifier . If the object already exists in the database, the command will result in an error.\n",
    "\n",
    "delete &lt; identifier &gt; . The command removes the object named identifier from the current database, and is typically utilized in conjunction with the second command.\n",
    "\n",
    "When an expert or system developer writes the executable query language for SECONDO, one needs to comprehensively understand the intricate relationship between data flow and operators. The rel2stream operator transforms a relation into a stream of tuples, as shown in Figure 4. The stream2rel operator, in contrast, converts a stream of tuples into a relation. Among the fundamental operators of SECONDO, the filter operator is the most\n",
    "\n",
    "<!-- image -->\n",
    "\n",
    "city (Name:String, GeoData:Region)\n",
    "\n",
    "Figure 4: Functions of the operators rel2stream, stream2rel and filter in SECONDO\n",
    "\n",
    "frequently utilized. Similar to the SELECT keyword in SQL, the function of the filter operator is to extract information from data that satisfies specific conditions. The SELECT keyword operates on a two-dimensional table structure to query, filter, and project data by specifying columns and conditions. The filter operator works on a stream of tuples, followed by a filter condition. The tuples that match the condition are then collected and outputted as a stream. For example, the following executable language will output all information about Nanjing in the relation city in SECONDO.\n",
    "\n",
    "query city rel2stream filter [.Name = 'Nanjing'] stream2rel;\n",
    "\n",
    "During the execution of the query, the rel2stream operator first transforms the relation city into a stream of tuples, then the filter operator extracts the tuple named ' Nanjing ' from the stream, and finally the stream2rel operator converts the tuple into a relation from the stream.\n",
    "\n",
    "## 2.3 Intermediate representation languages\n",
    "\n",
    "The intermediate representation language in NLIDB is designed to accommodate the semantic discrepancies and diversity between natural language and executable database language, thus improving the translation accuracy, flexibility and maintainability of the system [7]. The intermediate representation serves as a translator between natural language and executable language, mapping complex natural language structures to a unified semantic representation for the purpose of efficient subsequent query processing and execution. By decoupling NLQ from the underlying database query language, the intermediate representation language makes the NLIDB system flexible, portable, and adaptable to various database types and query requirements. The design of the intermediate representation considers several factors, such as:\n",
    "\n",
    "- (i) The intermediate representation should convey the query request that the user wishes to submit to the database, rather than the full meaning of the user's input.\n",
    "- (ii) To facilitate subsequent translation into the executable language of the database, the intermediate representation should be unambiguous.\n",
    "- (iii) To make re-development easier, the intermediate representation should be reusable.\n",
    "\n",
    "Popular intermediate representations are parse trees [78], first-order logic [121], OQL [113], query sketch [153], SemQL [53], and NatSQL [50].\n",
    "\n",
    "Parse tree. The syntactic structure of a query in natural language is closely tied to the design of a parse tree. The tree structure is typically applied to represent the hierarchical and structural relationships of the query. Each node in a parse tree indicates a grammatical unit (e.g., phrase, word group, and vocabulary), while edges indicate grammatical relations (e.g., modification and conjunction) between these grammatical units. The nodes and edges on the parse tree can be labeled with semantic information to identify the semantic roles and constraints present in the query, providing important information for subsequent query processing.\n",
    "\n",
    "First-order logic. When transforming an NLQ into first-order logic (FOL), words and phrases in the natural language are first mapped to predicates, constants, variables, and logical connectives in FOL to represent entities,\n",
    "\n",
    "attributes, and relations in the query. Subsequently, on the basis of the syntactic structure of the NLQ, the syntax tree or syntax graph of the FOL representation is constructed to capture the semantic relations and logical structures in the query. Finally, the topics, conditions, and operations in the query are identified and converted into logical expressions in FOL to denote the constraints and operational requirements of the query.\n",
    "\n",
    "When converting the natural language query ' Find the names and salaries of all employees older than 30. ' into a first-order logic representation, predicates and constants are defined as follows.\n",
    "\n",
    "Employee x ( ) : x is an employee\n",
    "\n",
    "Name x,n ( ) : the name of employee x is n\n",
    "\n",
    "Age x, a ( ) : the age of employee x is a\n",
    "\n",
    "Salary x, s ( ) : the salary of employee x is s\n",
    "\n",
    "The query condition is expressed as ∀ x Employee x ( ( ) ∧ Age x, a ( ) ∧ a &gt; 30) . The query result is expressed as ∃ n, s ( Name x,n ( ) ∧ Salary x, s ( )) . The complete first-order logic representation is obtained by combining the condition and result of the query.\n",
    "\n",
    "<!-- formula-not-decoded -->\n",
    "\n",
    "OQL is built on an ontology knowledge graph, where words and phrases in natural language queries are associated with concepts, attributes and relations within the ontology knowledge graph. The semantic information of natural language queries is captured through semantic representations and query patterns to effectively interact with the database. OQL grammars permit the expression of complex aggregation, union and nested queries. OQL queries operate upon individual concepts, with each concept being assigned an alias as specified in the FROM clause of the query.\n",
    "\n",
    "Query sketch is a form of SQL with natural language hints. Taking the NLQ ' Find the number of papers in OOPSLA 2010. ' as an example, the query sketch is as follows.\n",
    "\n",
    "SELECT count(?[papers]) FROM ??[papers] WHERE ? = 'OOPSLA 2010';\n",
    "\n",
    "In the query sketch, the symbols ' ?? ' and ' ? ' represent an unspecified table and an unspecified column, respectively. Hints for the corresponding gaps are indicated by words enclosed in square brackets. As an illustration, the first hint in the sketch suggests that the symbol ' ? ' has a similar semantic meaning to the term papers .\n",
    "\n",
    "SemQL is designed as a tree structure that not only constrains the search space during synthesis, but also maintains the same structural characteristics as SQL. In SemQL queries, the GROUP BY, HAVING, and FROM clauses in SQL are removed, and the conditions from the WHERE and HAVING clauses are consistently represented in the Filter sub-tree. Furthermore, in the later inference phase, domain knowledge is utilized to deterministically infer implementation details from SemQL queries. For instance, the columns included the GROUP BY clause of SQL are typically present in the SELECT clause.\n",
    "\n",
    "NatSQL retains the core functionality of SQL while streamlining the structure of SQL to align more closely with the syntax of natural language. NatSQL keeps only the SELECT, WHERE, and FROM clauses, omitting the JOIN ON, HAVING, and GROUP BY clauses. Additionally, NatSQL does not require nested sub-queries or aggregation operators, and employs a single SELECT clause. In the case of the natural language query ' Which film has more than 5 actors and less than 3 in the inventory? ', the SQL and NatSQL are as follows.\n",
    "\n",
    "SQL: SELECT T1.title FROM film AS T1 JOIN film actor AS T2 ON T1.film id = T2.film id GROUP BY T1.film id HAVING count(*) &gt; 5 INTERSECT SELECT T1.title FROM film AS T1 JOIN inventory AS T2 ON T1.film id = T2.film id GROUP BY T1.film id HAVING count(*) &lt; 3;\n",
    "\n",
    "NatSQL: SELECT film.title WHERE count(film actor.*) &gt; 5 and count(inventory.*) &lt; 3;\n",
    "\n",
    "Table 3: Natural language preprocessing for NLIDBs\n",
    "\n",
    "| NLIDB            |   Year | Underlying datatype   | Segmentation   | Part of speech   | NER   | Dictionary generation   | Regular expression   | Dependency parsing   | Word Embedding   | Pattern Linking   |\n",
    "|------------------|--------|-----------------------|----------------|------------------|-------|-------------------------|----------------------|----------------------|------------------|-------------------|\n",
    "| PRECISE [107]    |   2003 | relational data       | ✓              | ✓                | ✓     | ✓                       |                      |                      |                  |                   |\n",
    "| Querix [69]      |   2006 | ontology              | ✓              | ✓                |       | ✓                       |                      |                      |                  |                   |\n",
    "| QuestIO [27]     |   2008 | ontology              | ✓              | ✓                |       | ✓                       |                      |                      |                  |                   |\n",
    "| gAnswer [60]     |   2013 | RDF data              |                |                  |       | ✓                       |                      |                      |                  |                   |\n",
    "| MEANS [1]        |   2015 | RDF data              | ✓              |                  | ✓     |                         |                      |                      |                  |                   |\n",
    "| NL2CM [6, 5]     |   2015 | RDF data              | ✓              | ✓                |       |                         |                      | ✓                    |                  |                   |\n",
    "| NL2TRANQUYL [16] |   2015 | relational data       |                |                  |       |                         |                      | ✓                    |                  |                   |\n",
    "| ATHENA [113]     |   2016 | relational data       | ✓              | ✓                | ✓     |                         |                      | ✓                    |                  |                   |\n",
    "| SQLizer [153]    |   2017 | relational data       | ✓              | ✓                | ✓     |                         |                      |                      |                  |                   |\n",
    "| TEQUILA [64]     |   2018 | RDF data              | ✓              | ✓                | ✓     |                         |                      |                      |                  |                   |\n",
    "| MyNLIDB [28]     |   2019 | relational data       | ✓              | ✓                |       |                         |                      | ✓                    |                  |                   |\n",
    "| IRNet [53]       |   2019 | relational data       |                |                  |       |                         |                      |                      | ✓                | ✓                 |\n",
    "| NLMO [145]       |   2020 | moving objects        | ✓              | ✓                | ✓     | ✓                       | ✓                    |                      |                  |                   |\n",
    "| NALMO [144, 143] |   2021 | moving objects        | ✓              | ✓                | ✓     | ✓                       | ✓                    |                      |                  |                   |\n",
    "| NALSD [88]       |   2023 | spatial data          | ✓              | ✓                | ✓     | ✓                       |                      |                      |                  |                   |\n",
    "| NALSpatial [89]  |   2023 | spatial data          | ✓              | ✓                | ✓     | ✓                       |                      |                      |                  |                   |\n",
    "| xDBTagger [132]  |   2024 | relation data         | ✓              | ✓                |       |                         |                      |                      | ✓                | ✓                 |\n",
    "\n",
    "## 3 Generation of executable database languages\n",
    "\n",
    "The generation of executable database languages can be divided into three stages: (i) natural language preprocessing , (ii) natural language understanding , and (iii) natural language translation . In stage (i), the system performs a preliminary analysis of the raw natural language query in order to prepare for the subsequent stage of natural language understanding. In stage (ii), the system performs semantic parsing and understanding of the preprocessed natural language query to extract the semantic details and intent of the query. In stage (iii), the system converts the comprehended natural language into a language that can be executed within the database.\n",
    "\n",
    "## 3.1 Natural language preprocessing\n",
    "\n",
    "Prior to the semantic understanding and translation of natural language queries, preprocessing is performed using traditional and data-driven methods. In order to preprocess natural language queries, the recently developed NLIDBs utilize techniques as illustrated in Table 3.\n",
    "\n",
    "The preprocessing process of many NLIDBs commences with the construction of a dedicated data dictionary for the domain. The extraction process of domain knowledge exerts a profound influence on the portability of the system. In addition, the semantic parsing component needs to accurately comprehend NLQ with the assistance of the dictionary, and the extraction process of domain knowledge will impact the availability of the NLIDB. The primary goal of the extraction technique is to minimize the burden on system users while enhancing the capacity to automatically generate a dictionary. The extraction process is primarily reliant on stemming and synonym techniques. The system then needs to perform word segmentation and part-of-speech tagging on the input natural language. This process necessitates the utilization of natural language processing tools. When choosing the tool, the high accuracy of the segmentation and part-of-speech tagging results should be considered first, followed by the speed of processing. Furthermore, the query must be oriented to database information, and the relevant\n",
    "\n",
    "Table 4: Rules for parsing natural language queries\n",
    "\n",
    "| Rules                | Typical NLIDBs                                                                                                                                  |\n",
    "|----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Parse tree           | PRECISE [107], NaLIX [84, 85], Querix [69], DaNaLIX [81], gAnswer [60], NaLIR [78, 77, 79], NL2TRANQUYL [16], Unnamed method [65], MyNLIDB [28] |\n",
    "| Ontology             | QuestIO [27], ATHENA [113], FINESSE [63], Unnamed method [41], CNL- RDF-Query [58], ATHENA++ [115], Unnamed method [4]                          |\n",
    "| Semantic graph       | Unnamed method [168], MEANS [1], NL2CM [6, 5]                                                                                                   |\n",
    "| Template matching    | SQLizer [153], Unnamed method [3], LogicalBeam [11]                                                                                             |\n",
    "| Pattern matching     | SODA [15]                                                                                                                                       |\n",
    "| Context-free grammar | TR Discover [121]                                                                                                                               |\n",
    "| Semantic grammar     | Unnamed method [49]                                                                                                                             |\n",
    "\n",
    "statements used in the query request are closely related to the database to be used. Therefore, part-of-speech tagging is often employed in conjunction with named entity recognition and data dictionary.\n",
    "\n",
    "Traditional preprocessing methods rely on predefined rules and grammars, involving techniques including NER, regular expressions, and dependency parsing. NLMO performs segmentation and entity recognition using a natural language processing toolkit spaCy, and sets regular expressions for temporal information extraction. ATHENA utilizes the TIMEX annotator to detect all temporal intervals mentioned in the text, and the Stanford Numeric Expressions annotator to pinpoint all tokens containing numerical values. ATHENA employs the Stanford Dependency Parser to identify the dependency relationship in the context of the GROUP BY clause. PRECISE utilizes the Charniak parser for the precise parsing of questions and the extraction of token relationships from the resulting parse tree. NL2CM employs dependency parsing and part-of-speech tagging techniques. NL2TRANQUYL analyzes the input natural language using the Stanford Parser, resulting in constituency and dependency parses.\n",
    "\n",
    "Data-driven preprocessing methods depend on large-scale data and machine learning models, and the techniques used include word embedding and pattern linking. Word2Vec and GloVe are word embedding models that are able to represent words as points in a sequential vector space, thereby capturing the semantic relationships between words. These vectors can be employed for calculating semantic similarity and extracting features. xDBTagger utilizes a pre-trained word embedding model to convert tokens into a 300-dimensional vector representation. IRNet performs schema linking by connecting the natural language with the database schema, aiming to identify the specific columns and tables referenced in the natural language. The columns are then assigned different types according to the manner mentioned in the question.\n",
    "\n",
    "## 3.2 Natural language understanding\n",
    "\n",
    "Three principal technical approaches to understand natural language are (i) rule-based , (ii) machine learningbased , and (iii) hybrid . Based on the techniques, the process of natural language understanding for the recently developed NLIDBs is summarized. We provide three timelines describing the research on rule-based, machine learning-based, and hybrid approaches, as shown in Figure 5.\n",
    "\n",
    "## 3.2.1 Rule-based methods\n",
    "\n",
    "The semantic parsing of mature NLIs is predominantly based on rules. The systems require specific rules to parse natural language queries, including parse tree, ontology, semantic graph, template matching, pattern matching, context-free grammar, and semantic grammar, as shown in Table 4. Rule-based systems can only deal with knowledge bases in fixed domains and are generally not portable to other knowledge bases. In order to enhance the accuracy of semantic understanding, systems are typically constrained by limitations in their ability to support\n",
    "\n",
    "<!-- image -->\n",
    "\n",
    "| PRECISE   | NaLIX                | Querix   | DaNaLIX           | QuestIO           | QUICK             |                                |                                | (to                            | be continued)                  |                                |                                |                                |\n",
    "|-----------|----------------------|----------|-------------------|-------------------|-------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|\n",
    "| 2003      | 2005                 | 2006     |                   | 2007              | 2008              |                                | 2009                           |                                |                                |                                |                                |                                |\n",
    "|           |                      |          | TR Discover MEANS | TR Discover MEANS | TR Discover MEANS |                                |                                |                                |                                |                                |                                |                                |\n",
    "|           |                      |          | NL2TRANQUYL       | NL2TRANQUYL       | NL2TRANQUYL       | SQLizer                        |                                | DialSQL                        |                                | ezNL2SQL NLMO                  |                                |                                |\n",
    "|           | gAnswer              | NaLIR    |                   | NL2CM             | ATHENA            |                                | NLQ/A                          | TEQUILA                        | MyNLIDB                        | ATHENA++                       | EXAQT                          |                                |\n",
    "|           |                      | 2014     | 2013              |                   | 2015              | 2016                           | 2017                           | 2018                           | 2019                           | 2020                           | 2021                           |                                |\n",
    "|           |                      |          |                   | (a)               | Rule-based        | methods                        | methods                        | methods                        | methods                        | methods                        | methods                        | methods                        |\n",
    "|           |                      |          |                   |                   |                   | BiBERT-SQL                     |                                |                                |                                |                                |                                |                                |\n",
    "| Seq2SQL   | SyntaxSQLNet DialSQL |          | IRNet             | RYANSQL           |                   | ValueNet SP-CNN                |                                | Auto-Query                     | DTE IKnow-SQL                  | SV2-SQL SpatialNLI             |                                |                                |\n",
    "| 2017      | 2018                 |          | 2019              |                   | 2020              |                                | 2021                           | 2022                           | 2023                           |                                | 2024                           |                                |\n",
    "|           |                      |          |                   | (b)               |                   | Machine learning-based methods | Machine learning-based methods | Machine learning-based methods | Machine learning-based methods | Machine learning-based methods | Machine learning-based methods | Machine learning-based methods |\n",
    "|           |                      |          |                   |                   |                   |                                |                                |                                | NALSD                          |                                |                                |                                |\n",
    "|           |                      |          |                   |                   |                   |                                |                                |                                | NALSpatial                     |                                |                                |                                |\n",
    "|           |                      |          |                   |                   |                   |                                |                                |                                | GENSQL                         |                                |                                |                                |\n",
    "|           |                      |          |                   |                   |                   |                                |                                |                                | CatSQL                         |                                |                                |                                |\n",
    "|           |                      |          |                   |                   |                   |                                |                                | Veezoo                         |                                |                                |                                |                                |\n",
    "|           | TypeSQL              |          |                   |                   |                   |                                |                                |                                | GAR                            |                                | xDBTagger                      |                                |\n",
    "|           |                      |          |                   |                   |                   | NALMO                          |                                |                                |                                |                                |                                |                                |\n",
    "\n",
    "(c) Hybrid methods based on rule and machine learning\n",
    "\n",
    "Figure 5: Timelines of the research progress of techniques for understanding natural language\n",
    "\n",
    "natural language features, such as grammar and vocabulary [144]. PRECISE [107] elucidates the notion of semantic tractability and delineates a specific subset of natural language that can be accurately converted into SQL. However, natural language queries that cannot be processed semantically will be rejected by PRECISE. NaLIX [84, 85] restricts natural language queries to a regulated subset according to a predetermined grammar. DaNaLIX [81] is constructed on NaLIX and employs domain knowledge for query translation. Domain knowledge is encapsulated within a collection of regulations that map terms with domain meaning in the parse tree to terms that can be understood by a generic system such as NaLIX. The domain adapter within DaNaLIX assesses the current domain expertise and modifies the parse tree with related rules. NaLIR [78, 77, 79] identifies nodes within the language parse tree that have the potential to correspond to SQL components resulting from the preprocessing step, and represents semantic coverage as a subset of the parse tree. Such a tree explicitly corresponds to SQL and serves as a query tree, which mediates between NLQ and SQL. To comprehend the challenge of integrating individual and collective knowledge, NL2CM first uses RDF to represent individual and general knowledge. Individual expression detectors are then used to distinguish between individual and general query components, which are created through a declarative selection schema in conjunction with a specialized vocabulary. ATHENA uses domain-specific ontology to transform the natural language input into an intermediate language on the ontology. The intermediate language is then used to describe the semantic entities in the domain, as well as the relationships between the entities. Ontology provides richer semantic information than relational schema, including inheritance\n",
    "\n",
    "and membership. By reasoning about the ontology, ATHENA demonstrates the capability to effectively discern and capture the intentions of users. However, ATHENA is highly sensitive to changes and interpretations of user queries [99]. Both the NLIDB system described in the paper [116] and ATHENA++ [115] are extensions of ATHENA. They combine linguistic analysis with deep domain reasoning to translate complex join and nested SQL. NL2TRANQUYL [16] is a system designed for the planning of journeys within a complex multi-modal transportation system, taking into account a number of constraints, including the minimization of journey time, distance and cost. NL2TRANQUYL utilizes the ontology comprising a range of concepts to store and model related information, and generates knowledge graphs to determine the relationships between them. To discover and process temporal information in NLQ, TEQUILA decomposes the detected temporal problems and rewrites the generated sub-problems. These papers [60, 168] utilize the Stanford Parser to generate dependency trees and extract semantic relations from the parsed data. Subsequently, a semantic query graph is constructed by connecting these semantic relations to depict the user's query intent. Querix [69] examines the syntax of natural language using a syntactic analyzer, which is only effective when the natural language components are complete. Incomplete components may result in inaccurate results, which could compromise the accuracy of the final results.\n",
    "\n",
    "An optimal NLIDB enables users to formulate intricate queries on the database system and retrieve precise information with minimal exertion. Consequently, a number of systems incorporate user interaction during the process of comprehending semantics. NaLIX and DialSQL [54] adjust the query during following user engagements to revise the parse tree, however, the revision frequently necessitates a high number of user interactions. DaNaLIX acquires domain knowledge through the interaction that occurs between the user and the system in an automated manner. In addition to elucidating the user on the query processing procedure, NaLIR also presents a spectrum of interpretations for the user to select from, thus alleviating the user's need to address potential misunderstandings. NaLIR is capable of detecting the parse tree, thereby enabling users to modify the parse tree directly, rather than reformulating the natural language query. NaLIR can provide recommendations to users for revising their queries in instances where the natural language queries fall beyond the semantic boundaries. QUICK [162] improves user interactions by utilizing keyword search to enrich the expressiveness of semantic queries. In practical application, QUICK assists users in determining the specific intent behind natural language through a series of iterative refinement steps following the initial submission of a keyword-based question. NLQ/A [166] enhances the user interaction component in order to more effectively address the issue of ambiguity. SPARKLIS [46] employs a sequential process consisting of three stages in order to guarantee the thoroughness of user input during searches for concepts, entities or modifiers. While interacting with the system may result in the user feeling constrained, slowed down, and less natural when entering a query, SPARKLIS provides guidance and safety through intermediate answers and suggestions [2]. In order to reduce user involvement during the disambiguation process, ATHENA utilizes the extensive semantic data within the ontology to produce a prioritized list of explanations, and employs a ranking algorithm that is intuitive and relies on ontology metrics to determine the most appropriate explanation.\n",
    "\n",
    "## 3.2.2 Machine learning-based methods\n",
    "\n",
    "As the usage of statistical learning methods continues to expand, there has been a growing interest in conducting semantic analysis on sentences through a variety of forms of supervision. Pasupat and Liang [102] employ question-and-answer format to provide guidance in responding to intricate natural language queries presented within semi-structured tables. The paper [105] represents the inaugural attempt to develop a semantic parsing model through unsupervised learning [66]. Artzi and Zettlemoyer [8] solicit feedback during the conversation to determine the meaning of the user's statements. In a domain where no training examples are available, Wang et al. [146] demonstrate the successful development of a semantic parser. Their approach comprises two key elements: (i) a builder and (ii) a domain-general grammar . Wong and Mooney [150] utilize statistical machine translation technology for the purpose of accomplishing semantic parsing tasks.\n",
    "\n",
    "Table 5: NLIDBs with encoder-decoder frameworks\n",
    "\n",
    "| NLIDB                |   Year | Encoder                                                | Decoder                                         |\n",
    "|----------------------|--------|--------------------------------------------------------|-------------------------------------------------|\n",
    "| DialSQL [54]         |   2018 | Encode dialogue history using RNN networks             | Decode errors and candidate se- lections        |\n",
    "| SyntaxSQLNet [159]   |   2018 | Table-aware column encoder                             | Syntax tree-based decoder                       |\n",
    "| Unnamed method [87]  |   2020 | Encode NLQs and table headers using XLNet [155]        | The parsing layer splices the vec- tor          |\n",
    "| ValueNet [19]        |   2021 | Extension of IRNet's encoder                           | LSTM architecture and multiple pointer networks |\n",
    "| Unnamed method [30]  |   2021 | The encoder of LSTM                                    | The decoder of LSTM                             |\n",
    "| MIE [138]            |   2021 | Multi-integrated encoder with three integrated modules | No decoder                                      |\n",
    "| Auto-Query [100]     |   2022 | The encoder of RATSQL [137]                            | SmBoP [112]                                     |\n",
    "| STAMP [51]           |   2023 | The encoder of T5                                      | The decoder of T5                               |\n",
    "| Unnamed method [154] |   2023 | The encoder of Transformer                             | The decoder of Transformer                      |\n",
    "\n",
    "In recent times, there has been a growing utilization of encoder-decoder frameworks that rely on recurrent neural networks for semantic parsing, as demonstrated in Table 5. Many systems combine machine learning and deterministic algorithms to generate structured languages [93]. This method allows the direct acquisition of the correlation between natural language and the semantic representation, eliminating the need for an intermediate representation like a parse tree [66]. Mapping natural language directly to the semantic representation can reduce the dependence of rule-based semantic parsing models on preset vocabulary, templates, and hand-generated features. Machine learning-based models are not limited to specific knowledge bases or logical formal expressions, thus enabling the implementation of natural language interfaces that support cross-knowledge bases or crosslanguages. Wang et al. [140, 142] propose a cross-domain NLI, which translates the marked natural language into the intermediate representation of the target query type by building a cross-domain multilingual sequenceto-sequence (seq2seq) model. Symbols inserted into the natural language query are utilized to substitute the data elements present in the intermediate query. However, this method is a supervised machine learning model whose effectiveness is closely related to the quality of the training data. To ensure the accuracy of semantic understanding, a substantial quantity of training data must be provided to the model. A number of researchers employ a synthetic data generator as a solution to the challenge of having a restricted amount of training data available. The paper [159] introduces SyntaxSQLNet, which can generate NLQ data sets for cross-domain SQL single-table operations, solely as a means of augmenting the training set. The method outlined in the paper [149] encompasses single-table and multi-table join queries of SQL, and can be utilized as either an augmentation or as a standalone training data set. In terms of model training, the rule-based method is more effective than the neural networkbased method, which requires more training parameters and takes longer to establish the model, consuming more memory space.\n",
    "\n",
    "One of the earliest examples of machine learning-based systems is demonstrated in the paper [161]. This work utilizes a deterministic shift-reduce parser and develops a learning algorithm called CHILL to learn the governing rules of parsing on the basis of inductive logic programming techniques. The corpus is trained using the CHILL method to build the parser. Instead of learning dictionaries, this approach assumes that a dictionary is created in advance that pairs words with semantic content rather than grammar. The paper [163] translates the meaning of natural language sentences into lambda calculus encoding. The paper [163] outlines a learning algorithm whose input is a collection of sentences identified as lambda calculus expressions, and applies the method to the task of learning NLIDB to build a parser. While providing considerable flexibility, encoder-decoder frameworks frequently lack the ability to interpret and understand combinations of meaning [66]. The method employed by Cheng et al. [25] involves the construction of the intermediate structure in two stages, which facilitates a comprehensive understanding of the model's learning process. Similarly, the paper [39] also produces an intermediate\n",
    "\n",
    "template that presents the final output in a preliminary format, thereby facilitating the subsequent decoding process. Yin and Neubig [157] address the issue of insufficient training data by incorporating explicit constraints for decoders through the utilization of target language syntax. The approach enables the model to concentrate on parsing, directed by established grammar rules. Xiao et al. [151] utilize the grammar model as prior knowledge, requiring the creation of a derivation tree while adhering to the constraints imposed by the grammar. The approach in the paper [74] can significantly outperform the Seq2Tree model from the aforementioned paper [38] by verifying that the decoder's forecasts adhere to the type constraints outlined in the type constraint grammar. This suggests that satisfying type constraints and good formatting are equally important when generating logical expressions. SpatialNLI [80, 141] is a natural language interface for the spatial field that employs the seq2seq model to understand the semantic structure of natural language, while utilizing an external spatial understanding model to identify the meaning of spatial entities. Subsequently, the spatial semantics learned from the spatial understanding model are integrated into natural language problems, thereby reducing the necessity of acquiring specific spatial semantics. SpatialNLI represents a pioneering system that integrates an external spatial semantic comprehension model to optimize the effectiveness of the principal seq2seq model. The paper [129] uses a tree model to analyze the target entity in natural language, and employs a tree-structured LSTM to understand the problem. The paper [62] adjusts the neural sequence model to directly convert natural language into SQL, thus circumventing the intermediate query language representation. Then the user feedback is utilized to mark error queries, which are directly used to improve the model. The complete feedback loop does not necessitate the use of any intermediate language representation and is not limited to a specific domain. This method offers the benefit of enabling the rapid and straightforward construction of a semantic parser from scratch, and the performance of the parser improves as user feedback increases. The encoder of ValueNet [19] is an extension of the encoder of IRNet [53], receiving not only details regarding the database schema, but also extracted value candidates from the database content.\n",
    "\n",
    "## 3.2.3 Hybrid methods based on rule and machine learning\n",
    "\n",
    "Hybrid methods integrate rules and machine learning techniques to capitalize on the respective strengths of each, thereby enhancing the ability of the system to understand and process NLQs [72]. Table 6 enumerates the representative systems that employ the hybrid approach. Hybrid approaches are highly flexible and adaptable, as they can utilize rules for tasks with explicit rules as well as machine learning models for complex and ambiguous semantic tasks. In addition, hybrid methods can flexibly incorporate new rules or train new machine learning models as needed to accommodate the requirements of diverse domains and tasks, and are highly scalable [135]. TypeSQL [158], like SQLNet [152], is built on sketches and formats translation tasks as slot-filling problems. The difference is that TypeSQL employs type information to enhance the understanding of entities and numbers in NLQs. TypeSQL assigns a type to each word, such as entity, column, number and date, within the knowledge graph. Subsequently, two bidirectional LSTM networks are utilized to encode the words in the NLQ with the corresponding column names and types. Finally, the LSTM output hidden states are leveraged to forecast the slot values within the SQL sketch. NALMO is a natural language interface for moving objects. To understand NLQs, NALMO employs an entity extraction algorithm to obtain entity information, including time, location and the number of nearest neighbors. A pre-constructed corpus is then trained using LSTM to determine the query type. Veezoo [75] uses a range of techniques, including temporal expression parsing, entity linking, and relation extraction, to identify key information in NLQs. The information is then extended and combined using predefined rules to generate multiple candidate intermediate representations. Finally, Veezoo utilizes a machine learning model to score these intermediate representations in order to select the most probable interpretation of the NLQ. The process of data preparation in GAR [42] commences with a collection of sample SQLs that are tailored to a specific database. For a given NLQ, GAR searches for the NLQs generated during data preparation and employs a learning-to-rank model to identify the most relevant query, which is then used to obtain the translation result.\n",
    "\n",
    "Table 6: NLIDBs based on rules and machine learning techniques\n",
    "\n",
    "| NLIDB               |   Year | Rule                                                              | Machine learning technique                                       |\n",
    "|---------------------|--------|-------------------------------------------------------------------|------------------------------------------------------------------|\n",
    "| Unnamed method [52] |   2012 | Generate candidate SQLs via rules and heuristic weighting schemes | Reorder candidate SQLs using the SVM sorter                      |\n",
    "| TypeSQL [158]       |   2018 | Assign a type to each word to under- stand the entity             | Encode using bidirectional LSTM                                  |\n",
    "| NALMO [144, 143]    |   2021 | Semantic grammar and template matching                            | Identify the query type using LSTM                               |\n",
    "| Veezoo [75]         |   2022 | Knowledge graph                                                   | Score intermediate representations using machine learning models |\n",
    "| GAR [42]            |   2023 | Parse tree                                                        | Find the matching expression for NLQ using a learn-to-rank model |\n",
    "| GENSQL [44]         |   2023 | Capture the structure of the database with sample SQLs            | Find the matching expression for NLQ using a learn-to-rank model |\n",
    "| CatSQL [48]         |   2023 | Template matching                                                 | The decoder of Transformer. Train the model using Adam [71]      |\n",
    "| NALSpatial [89]     |   2023 | Semantic grammar and template matching                            | Identify the query type using LSTM                               |\n",
    "| NALSD [88]          |   2023 | Semantic grammar and template matching                            | Identify the query type using LSTM                               |\n",
    "| xDBTagger [132]     |   2024 | Semantic graph                                                    | Bidirectional recurrent neural network                           |\n",
    "\n",
    "The learning-to-rank model learns to rank the semantic similarities from NLQs to generated NLQs and then finds the best matching expression for a given NLQ. GENSQL [44], a generative NLIDB, utilizes a given example SQL from the database (e.g., from query logs) to comprehend the unique structure and semantics of a given database, thereby guaranteeing precise translation outcomes. The fundamental model used in GENSQL for converting natural language to SQL is GAR. CatSQL [48] is a method for the generation of SQL that makes use of sketches. In addition, semantic constraints are merged into the neural network-driven SQL generation procedure for semantic refinement. CatSQL sketches are templates with keywords and slots. CatSQL employs a deep learning algorithm to populate vacant slots in order to generate the ultimate SQL. The deep learning algorithm is developed to focus on the generation of essential NLQ-related information, with the objective of filling the gaps without requiring the explicit generation of keywords like SELECT, FROM, and WHERE.\n",
    "\n",
    "Hybrid approaches based on rules and machine learning offer several advantages, including flexibility, accuracy, and scalability. Nevertheless, such approaches present certain challenges, such as complexity, dependence on data, and tuning difficulties [68]. Hybrid methods require the simultaneous management and maintenance of rule engines and machine learning models, including rule definition, feature engineering, and model training, and thus have high complexity [52]. Furthermore, the rules and machine learning models utilized in hybrid approaches may encounter parameter tuning problems, which necessitate a significant investment of time and effort for optimization and debugging, thus increasing the costs associated with the development and maintenance of the system. During the design and implementation of natural language interfaces, it is essential to take a comprehensive view of the advantages and challenges involved, and to make trade-offs and choices in accordance with the specific needs.\n",
    "\n",
    "## 3.3 Natural language translation\n",
    "\n",
    "The natural language translation stage employs the semantic information derived from the natural language understanding stage, subsequently integrating the underlying structure of the database to transform the input natural language into the corresponding executable language. A prevalent approach for translation is to employ complex algorithms and machine learning models to generate structured language based on the domain knowledge of the underlying database and the semantic representation of natural language [83]. Most established NLIDBs\n",
    "\n",
    "Figure 6: General build process for SQL\n",
    "\n",
    "<!-- image -->\n",
    "\n",
    "construct queries by query combination, mapping key information expressed in natural languages to corresponding components in structured languages. We examine the process of natural language translation in recently developed NLIDBs, and summarize the general construction process of executable languages for relational and spatio-temporal databases.\n",
    "\n",
    "The general build process for SQL is illustrated in Figure 6 and further elaborated in the subsequent two cases. (i) When querying a single relation, it is only necessary to place the database elements matched by NLQ in the correct positions in the SELECT, FROM, and WHERE parts, respectively. Then, SQL can be composed directly.\n",
    "\n",
    "(ii) When querying multiple relations, the join condition and the names of the participating relations need to be included in the WHERE and FROM clauses, respectively. Additionally, it is necessary to determine whether the join path is unique. If only one join path is available, SQL can be generated directly. Otherwise, a query is typically generated for each possible join path, and then the most probable one is selected according to the corresponding algorithm.\n",
    "\n",
    "In recent years, there has been significant interest in NLI for spatio-temporal databases [26]. Temporal and spatial concepts are derived from the natural language description using symbolic representations in order to depict spatio-temporal features and their relationships [14, 106]. Due to the particularity and expressiveness of spatiotemporal problems, executable query languages over spatio-temporal databases are quite different from SQL. Consequently, the method employed for the construction of SQL cannot be directly applied to the generation of executable languages over spatio-temporal databases. The general process for the construction of an executable language for a spatio-temporal database is shown in Figure 7. Preliminary parsing of the input natural language query is performed to obtain semantic information including key entities and query types. Subsequently, the operators necessary to construct the executable language are determined according to the type of query. Finally, key entities and operators are combined according to certain rules to compose an executable database language. Taking the range query over spatial data as an example, the key entities involved include spatial relations and locations. The operator intersects will return all objects in the relation that intersect the location if the spatial attribute of the relation and the data type of the location are both line or region . Conversely, the operator intersects will return all objects in the relation that lie within the location if the spatial attribute of the relation is point and the data type of the location is region .\n",
    "\n",
    "Different DBMSs and structured languages offer a range of clauses and operators for various queries. ATHENA employs a mapping strategy that correlates the ontology with the database schema in order to convert the intermediate query language utilized in the ontology into SQL. The system described in the paper [63] extends ATHENA to access multiple structured backends, which is achieved through the automated translation of the intermediate\n",
    "\n",
    "Figure 7: General construction process for executable languages over spatio-temporal data\n",
    "\n",
    "<!-- image -->\n",
    "\n",
    "query language into the specific structured query language utilized by these backend stores. NLPQC [124] is capable of processing queries formulated using predefined domain-specific templates. Querix selectively isolates specific elements from the syntactic tree in order to align acquired knowledge with the knowledge base, thereby obtaining the final outcome. NL2CM leverages crowd intelligence by converting audience queries into OASSISQL (an extended version of SPARQL). NaLIR utilizes the structure of the user-validated query tree to produce the suitable structure in the SQL statement and determine the join path. In order to ascertain whether the target SQL contains aggregate functions or sub-queries, NaLIR initially identifies function nodes or quantifier nodes in the query tree and subsequently generates SQL statements based on the identified conditions. The paper [52] employs lexical dependencies found in the question and database metadata to build a reasonable collection of SELECT, WHERE, and FROM clauses that enhance the quality of meaningful joins. The paper [52] combines clauses through a rule and heuristic weighting scheme, and then generates a sorted list of candidate SQLs, demonstrating that full semantic interpretation can be avoided by relying on a simple SQL generator. This method can be employed iteratively to address intricate issues necessitating nested SELECT commands. Finally, this paper [52] applies the re-ranker to reorder the list of questions and SQL candidate pairs with the aim of enhancing the accuracy of the system. TEQUILA uses a standard KB-QA system to evaluate the sub-questions from the semantic understanding part individually. The results of the sub-questions are then combined with the reasoning to calculate the answer to the full question. NL2TRANQUYL translates English requests into formal TRANQUYL [17] queries using the knowledge graph generated by the semantic comprehension component. The traffic query language TRANQUYL for travel planning follows the conventional SQL structure of ' SELECT, FROM, WHERE '. NALMO supports five distinct types of moving object queries, including (i) time interval queries , (ii) range queries , (iii) nearest neighbor queries , (iv) trajectory similarity queries , and (v) join queries . In the query translation process, NALMO first constructs a corpus comprising the five query types, collectively referred to as MOQ. Then the LSTM neural network is used for training, resulting in a model that is capable of accurately identifying the specific type of query. Finally, the appropriate operators are selected according to the query type, and the entity information extracted by the semantic parsing component is combined to build the executable language for SECONDO.\n",
    "\n",
    "## 4 NL2SQL benchmarks\n",
    "\n",
    "We presents 11 frequently used benchmarks for transforming NLQ into SQL and three evaluation metrics, exploring the methods for generating new benchmarks.\n",
    "\n",
    "## 4.1 Existing benchmarks\n",
    "\n",
    "The details of NLQ and executable language pairs for common domains are presented in Table 7. The majority of existing benchmarks are utilized in the domain of relational databases to transform natural language query into SQL (NL2SQL). The comparison of fields and types of SQL supported by the benchmarks for NL2SQL is shown in Table 8. We can conclude that GeoQuery and Spider support the most types of SQL, while WikiSQL supports\n",
    "\n",
    "Table 7: Examples of NLQ and executable language pairs for common domains\n",
    "\n",
    "| Domain              | Examples of NLQ and executable language pairs                                                                                                                                                                                                                                                                                             |\n",
    "|---------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Relational database | NLQ: How many CFL teams are from York College? SQL: SELECT COUNT CFL Team FROM CFLDraft WHERE College = 'York'                                                                                                                                                                                                                            |\n",
    "| Spatial domain      | NLQ1: What is the population of San Antonio? Lambda expression: answer(A,population(B,A),const(B,cityid(San Antonio))) NLQ2: Could you tell me what parks are in the center? Executable language: query park feed filter [.GeoData ininterior center] consume;                                                                            |\n",
    "| Moving Objects      | NLQ: Where did the train 7 go at 8am? Executable language: query Trains feed filter [.Id = 7] filter [.Trip present [const instant value '2020-11- 20-8:00']] extend [Pos: val (.Trip atinstant [const instant value '2020-11-20-8:00'])] project [Id, Line, Pos] consume;                                                                |\n",
    "| Trip planning       | NLQ: Can I walk to 300 W. Humboldt Blvd. by 4:00 p.m.? TRANQUYL: SELECT ∗ FROM ALL TRIPS(user.current location, 300 W. Humboldt Blvd.) AS t WITH MODES pedestrian WITH CERTAINTY .78 WHERE ENDS(t) ≤ 4:00 p.m. MINIMIZE DURATION(t)                                                                                                       |\n",
    "| Crowd mining        | NLQ: What are the most interesting places near Forest Hotel, Buffalo, we should visit in the fall? OASSIS-QL: SELECT VARIABLES $x WHERE { $x instanceOf Place. $x near For- est Hotel, Buffalo, NY } SATISFYING { $x hasLabel 'interesting' } ORDER BY DESC(SUPPORT) LIMIT 5 AND { [] visit $x. [] in Fall } WITH SUPPORT THRESHOLD = 0.1 |\n",
    "\n",
    "only the simple select query. The queries in WikiSQL and Spider cover a multitude of domains. In recent years, GeoQuery, MAS, WikiSQL and Spider have been employed with considerable frequency.\n",
    "\n",
    "The details of popular benchmarks are shown in Table 9. Early data sets consist of only one domain and one database, such as ATIS, Restaurant and GeoQuery. In contrast, the latest data sets, for example WikiSQL and Spider, contain multiple domains and several databases with larger and more diverse NLQs and SQLs.\n",
    "\n",
    "- (i) ATIS (Airline Travel Information System) [108] is a classical data set with a relatively old age, having been introduced by Texas Instruments in 1990. ATIS is built on the relational database Official Airline Guide, comprising 25 tables and 5871 queries written in English. The queries pertain to details regarding flights, ticket prices, destinations, and services available at airports. The queries in ATIS are for the air travel field, including join queries and nested queries, but no grouping and sorting queries. The average length of NLQs and SQLs in ATIS is approximately 11 and 67 words, respectively. Each query operates on an average of six tables. An example query is as follows.\n",
    "\n",
    "## Q1: What aircraft is used on delta flight 1984 from Kansas city to Salt Lake city?\n",
    "\n",
    "- (ii) Restaurant [127] comprises a vast collection of dining establishments located in Northern California, storing restaurant names, locations, features, and travel guide ratings. The benchmark contains 250 questions about restaurants, food types and locations. An example query is as follows.\n",
    "\n",
    "## Q2: Where is a good Chinese restaurant in Palo Alto?\n",
    "\n",
    "(iii) GeoQuery [128] consists of 8 tables and 880 natural language queries in the US geographic database. The queries in GeoQuery are designed for the geographic domain, including join queries, nested queries, grouping\n",
    "\n",
    "Table 8: Comparison of benchmarks for NL2SQL\n",
    "\n",
    "| Benchmark       | Select query   | Group query   | Sort query   | Join query   | Nested query   | Fields involved                                     | Usage in papers                                                       |\n",
    "|-----------------|----------------|---------------|--------------|--------------|----------------|-----------------------------------------------------|-----------------------------------------------------------------------|\n",
    "| ATIS            | ✓              |               |              | ✓            | ✓              | air travel                                          | [62, 47, 103, 104, 120]                                               |\n",
    "| Restaurant      | ✓              |               |              | ✓            |                | restaurant                                          | [127, 107, 81, 80]                                                    |\n",
    "| GeoQuery        | ✓              | ✓             | ✓            | ✓            | ✓              | geography                                           | [127, 128, 107, 163, 81, 52, 113, 62, 47, 80, 104, 115, 120, 42, 141] |\n",
    "| MAS             | ✓              | ✓             |              | ✓            |                | academic                                            | [77, 113, 153, 35, 9, 115, 131, 132]                                  |\n",
    "| Scholar         | ✓              |               |              | ✓            |                | academic                                            | [47]                                                                  |\n",
    "| IMDB            | ✓              |               |              | ✓            |                | internet movie                                      | [153, 9, 58, 131, 132]                                                |\n",
    "| YELP            | ✓              |               |              | ✓            |                | business review                                     | [153, 9, 131, 132]                                                    |\n",
    "| WikiSQL         | ✓              |               |              |              |                | multiple fields (e.g. state, college, manufacturer) | [167, 54, 158, 156, 87, 142, 48, 51, 125]                             |\n",
    "| ParaphraseBench | ✓              | ✓             |              |              |                | medical                                             | [133]                                                                 |\n",
    "| Advising        | ✓              |               |              | ✓            | ✓              | university course                                   | [47]                                                                  |\n",
    "| Spider          | ✓              | ✓             | ✓            | ✓            | ✓              | 138 different fields (e.g. car, stadium, country)   | [160, 53, 156, 115, 19, 50, 92, 131, 42, 43, 48, 51, 125]             |\n",
    "\n",
    "Table 9: Details of popular benchmarks\n",
    "\n",
    "| Benchmark             |   Year |   #queries |   #tables | Domains covered   |\n",
    "|-----------------------|--------|------------|-----------|-------------------|\n",
    "| ATIS [108]            |   1990 |       5871 |        25 | single field      |\n",
    "| Restaurant [127]      |   2000 |        250 |         3 | single field      |\n",
    "| GeoQuery [128]        |   2001 |        880 |         7 | single field      |\n",
    "| MAS [77]              |   2014 |        196 |        17 | single field      |\n",
    "| Scholar [62]          |   2017 |        816 |        10 | single field      |\n",
    "| IMDB [153]            |   2017 |        131 |        16 | single field      |\n",
    "| YELP [153]            |   2017 |        128 |         7 | single field      |\n",
    "| WikiSQL [167]         |   2017 |      80654 |     24241 | multiple fields   |\n",
    "| ParaphraseBench [133] |   2018 |        290 |         1 | single field      |\n",
    "| Advising [47]         |   2018 |       4387 |        15 | single field      |\n",
    "| Spider [160]          |   2018 |      10181 |      1020 | multiple fields   |\n",
    "\n",
    "queries and sorting queries. The average length of NLQs and SQLs in GeoQuery is about 8 and 16 words, respectively. Additionally, each query operates on an average of one table. Although the queries are relatively brief in length, they are highly composable, with nearly half of the SQL containing at least one nested sub-query. One of the English queries is as follows.\n",
    "\n",
    "## Q3: What is the largest city in states that border California?\n",
    "\n",
    "(iv) MAS [77] is generated from the Microsoft Academic Search database, which stores information such as academic papers, authors, journals, and conferences. The source of NLQs in MAS is the logical queries that are capable of being articulated in the search interface of the Microsoft Academic Search platform. The fields of MAS and Scholar are both academic in nature, but exhibit distinct patterns. One English query is as follows.\n",
    "\n",
    "## Q4: Return authors who have more papers than Bob in VLDB after 2000.\n",
    "\n",
    "(v) Scholar [62] consists of 816 NLQs for academic database search that are annotated with SQL. The average length of NLQs and SQLs in Scholar is approximately 7 and 29 words, respectively. Each query operates on an\n",
    "\n",
    "Table 10: Query categories and examples for ParaphraseBench\n",
    "\n",
    "| Category            | Example queries                                                       |\n",
    "|---------------------|-----------------------------------------------------------------------|\n",
    "| Naive               | What is the average length of stay of patients where age is 80?       |\n",
    "| Syntactic           | Where age is 80, what is the average length of stay of patients?      |\n",
    "| Morphological       | What is the averaged length of stay of patients where age equaled 80? |\n",
    "| Lexical             | What is the mean length of stay of patients where age is 80 years?    |\n",
    "| Semantic            | What is the average length of stay of patients older than 80?         |\n",
    "| Missing Information | What is the average stay of patients who are 80?                      |\n",
    "\n",
    "average of 3 tables. Iyer et al. [62] provide a database for performing these queries, which includes academic articles, journal details, author information, keywords, citations, and utilized datasets. One of the English queries is as follows.\n",
    "\n",
    "## Q5: Get all author having data set as DATASET TYPE.\n",
    "\n",
    "(vi) IMDB and YELP [153] are generated using data from the Internet Movie Database and Business Review Database, respectively. The NLQs are obtained from coworkers of the authors of the paper [153], who are only aware of the types of data available in the database and not the underlying database schema.\n",
    "\n",
    "(vii) WikiSQL [167], introduced in 2017, is a comprehensive and meticulously annotated collection of natural language to SQL mappings, and currently represents the most extensive data set for NL2SQL. WikiSQL contains SQL table instances extracted from 24241 HTML tables on Wikipedia, and 80654 natural language queries, each accompanied by an SQL. WikiSQL comprises genuine data extracted from the web, with queries involving a multitude of tables, but the queries do not involve complex operations such as GROUP BY and multi-table union queries. The majority of questions in WikiSQL are between 8 and 15 words in length, most SQLs are between 8 and 11 words, and most table columns are between 5 and 7. In addition, most natural language queries are of the what type, followed by which , name how many , , who . The execution accuracy of WikiSQL has significantly improved from the initial 59.4% to 93.0%, and the method has undergone a transformation from a simple seq2seq approach to a multi-tasking, transfer learning, and pre-training paradigm. A pair of questions and SQLs for the CFLDraft table can be formulated as follows.\n",
    "\n",
    "## Q6: How many CFL teams are from York College?\n",
    "\n",
    "SQL Q6: SELECT COUNT CFL Team FROM CFLDraft WHERE College = 'York'\n",
    "\n",
    "(viii) ParaphraseBench [133], a component of the DBPal paper [10], is a benchmark utilized to assess the robustness of NLIDBs. Unlike existing benchmarks, ParaphraseBench covers diverse language variants of user input NLQs and maps natural language to the anticipated SQL output. The benchmark is constructed upon a medical database that contains a single table for storing patient information. The language variants utilized in NLQs permit the classification of NLQs into six categories, as illustrated in Table 10.\n",
    "\n",
    "(ix) Advising [47] was proposed in 2018, and the NLQs were built on a database of course information from the University of Michigan containing fictitious student profiles. A portion of the queries are collected from the Facebook platform of the EECS department, and the remaining questions are formulated by computer science students well-versed in database topics that might be raised in academic consulting appointments. The queries in Advising are for student-advising tasks, including join queries and nested queries. One of the English queries is as follows.\n",
    "\n",
    "## Q7: For next semester, who is teaching EECS 123?\n",
    "\n",
    "(x) Spider [160] is a large NL2SQL data set introduced by Yale University in 2018, in order to solve the requirement for extensive and high-caliber datasets for a novel intricate cross-domain semantic parsing challenge.\n",
    "\n",
    "The data set contains 10181 natural language queries and 5693 corresponding complex SQLs, which are distributed across 200 independent databases, and the content covers 138 different domains. The average length of questions and SQL statements in Spider is approximately 13 and 21 words, respectively. While the number of questions and SQLs in Spider is not as extensive as that of WikiSQL, Spider contains all common SQL patterns and complex SQL usages, including advanced operations like HAVING, GROUP BY, ORDER BY, table joins, and nested queries, which makes Spider closely aligned with real-world scenarios. The following is an illustrative example of a complex problem and the corresponding SQL, which contains a nested query, a GROUP BY component, and multiple table joins.\n",
    "\n",
    "Q8: What are the name and budget of the departments with average instructor salary greater than the overall average?\n",
    "\n",
    "SQL Q8: SELECT T2.name, T2.budget FROM instructor as T1 JOIN department as T2 ON T1.department id = T2.id GROUP BY T1.department id HAVING avg (T1.salary) &gt; (SELECT avg (salary) FROM instructor)\n",
    "\n",
    "## 4.2 Generation of new benchmarks\n",
    "\n",
    "Modifying an existing NL2SQL benchmark to generate a new one is a common practice. The following steps describe the process in detail.\n",
    "\n",
    "(i) Researchers are required to conduct a meticulous analysis of the existing benchmarks, including an examination of the data structures, query types, and complexity. Through the analysis, they can gain insight into the constraints of the benchmark and identify potential avenues for enhancement.\n",
    "\n",
    "(ii) Designing a modification strategy is a critical step, which involves determining how to modify and extend the benchmark on the basis of the analysis results. The step may include adding new queries, changing the linguistic expression of queries, and introducing complex query types.\n",
    "\n",
    "(iii) In the process of implementing modifications, researchers are expected to execute the designed modification strategy with precision in order to ensure that the new benchmark meets the expected requirements.\n",
    "\n",
    "(iv) Evaluating the performance is a pivotal aspect of the process. The researchers employ the modified benchmark to train and test NL2SQL models, subsequently assessing the models' performance and generalization capabilities according to the new benchmark.\n",
    "\n",
    "Building on Spider [160], Kaoshik et al. [67] propose a new NL2SQL benchmark, named ACL-SQL, containing five tables and 3100 pairs of NLQ and SQL. By defining and annotating three types of questions on temporal aspects in Spider: (i) questions querying for temporal information , (ii) questions querying for temporal information with grouping or ordering , and (iii) questions with temporal conditions , Vo et al. [136] propose a new data set, TempQ4NLIDB, which can assist NLIDB systems based on machine learning approaches to improve their performance on temporal aspects. To address the dearth of publicly available benchmarks on ambiguous queries, Bhaskar et al. [11] generate a new benchmark called AmbiQT by modifying Spider with a combination of synonym generation and ChatGPT-based and standard rules-based perturbation. AmbiQT comprises in excess of 3000 examples, each of which can be interpreted as two valid SQLs due to lexical ambiguities (namely, unambiguous column and table names) or structural ambiguities (namely, the necessity of joins and the pre-computation of aggregations).\n",
    "\n",
    "In light of the limitations of existing benchmarks, including (i) the presence of data bias or linguistic expression limitations , and (ii) the limited coverage of domains and contexts that cannot fully represent real-world diversity , researchers have proposed generators for Text2SQL benchmarks. Weir et al. [149] present a synthesized data generator that synthesizes SQL patterns in the template syntax, including aggregations, simple nesting, and column joins. Each SQL pattern is matched with numerous different natural language (NL) patterns, allowing for the generation of a vast number of domain-specific NLQs and SQLs. Luo et al. [90] propose an NL2VIS synthesizer, named NL2SQL-to-NL2VIS, which is capable of generating multiple pairs of natural language and VIS from a single NL and SQL pair based on semantic joins between SQL and VIS queries. NL2SQL-to-NL2VIS\n",
    "\n",
    "can be utilized to create NL2VIS benchmarks from established NL2SQL benchmarks. Hu et al. [59] suggest a framework for synthesizing Text2SQL benchmarks. The framework involves first synthesizing SQL and then generating NLQs. At the stage of synthesizing SQL, a method is suggested for column sampling based on pattern distance weighting to prevent excessive complexity in concatenation. In the process of generating text from SQL, an intermediate representation is used to facilitate the transition from SQL to NLQ, thereby enhancing the quality of the generated NLQ.\n",
    "\n",
    "## 4.3 Evaluation metrics\n",
    "\n",
    "NLIDB is intended to assist users in efficiently querying and retrieving query results, and thus evaluating the response time and effectiveness of the system is essential. Response time measures how quickly the system can process a user's natural language queries and return the relevant results. Effectiveness measures how well the system translates natural languages into accurate and relevant executable database languages, which consists of two measures: (i) translatability and (ii) translation precision .\n",
    "\n",
    "DEFINITION 1 ( Translatability . ) Given the set E of executable languages generated by the system and the set N of input natural language queries, the translatability T is defined as follows.\n",
    "\n",
    "<!-- formula-not-decoded -->\n",
    "\n",
    "DEFINITION 2 ( Translation precision . ) Given the set ER of executable languages that meet the expected results, the set N of natural language queries entered into the system, the translation precision TP is defined as follows.\n",
    "\n",
    "<!-- formula-not-decoded -->\n",
    "\n",
    "Response time denotes the duration necessary for the system to transform the input natural language into the executable language of the database. This temporal interval represents the difference between the moment when the system furnishes the translated output and the moment when the natural language is received. Translatability is a measure of the likelihood of the system accurately translating a natural language into an executable language. This metric is quantified as the proportion of correctly translated queries out of the total number of queries submitted to the system. Translation precision refers to the likelihood that the final output of the translated executable language matches the expected outcome, and is quantified as the ratio of executable languages producing the desired results to the overall number of queries.\n",
    "\n",
    "The outcomes of evaluating a system may be different depending on the benchmark used. The size of the benchmark affects the accuracy of the semantic parsing part of the system. Complex queries in the benchmark can be used to assess the system's ability for generalization. In related papers, PRECISE achieves 95.0% translatability and translation precision on the Restaurant benchmark and 77.5% on the GeoQuery benchmark. ATHENA has a translatability and translation precision of 87.2% on the GeoQuery benchmark and 88.3% on the MAS benchmark. The translatability and translation precision of NALMO on the benchmark MOQ are 98.1% and 88.1%, respectively.\n",
    "\n",
    "## 5 System interfaces development\n",
    "\n",
    "We categorize recently developed NLIDBs according to the technical approach and the data stored in the backend. The methods of developing and using the system interfaces are then divided into two categories for analysis and summary:(i) used as an independent software and (ii) used as a module of a database management system . Finally, enhancements to the existing NLIDB system are presented in three aspects.\n",
    "\n",
    "## 5.1 Recently developed NLIDBs\n",
    "\n",
    "We are concerned with the NLIDBs, which have emerged since 2000. There are several ways to classify NLIDBs. Affolter et al. [2] divide recently developed systems into four categories.\n",
    "\n",
    "- (i) Keyword-based systems are represented by SODA [15]. The core of such a system lies in the search process, where the inverted index containing fundamental data and metadata from the database is utilized as the retrieval target. This process involves comparison with natural language, and identification of keywords referenced in the query. Although simple, the approach fails to identify the potential semantics that are not directly present in natural language. Such systems are unable to respond to aggregation queries and complex questions involving sub-queries.\n",
    "- (ii) Pattern-based systems , exemplified by NLQ/A [166] and QuestIO [27], are extensions of keyword-based systems that are capable of incorporating natural language patterns and mapping to pre-specified query sentence patterns.\n",
    "- (iii) Parsing-based systems are typified by NaLIR, a general interactive natural language interface designed for querying relational databases. NaLIR employs the existing natural language parser to acquire the semantic understanding of the given NLQ which is represented by a parse tree, and then converts the semantic understanding into database understanding and finally into SQL. Such systems incorporate a multitude of natural language processing methods, including the parsing of natural language sentences employing parse trees. One principal benefit of this method is the ability to map semantics into predefined SQL templates.\n",
    "- (iv) Grammar-based systems are represented by TR Discover [121] and MEANS [1]. The foundation of such systems consists of a predetermined set of grammar rules, which are used to constrain the questions that users can pose to the system in order to form formal NLQs that are straightforward to analyze. The primary advantage of this approach is that the systems are capable of providing users with guidance as they enter questions, and can respond to all questions that adhere to the established rules. In comparison to keyword-based, pattern-based and parsing-based systems, grammar-based systems are considered to be the most robust, despite relying significantly on predefined manual rules.\n",
    "\n",
    "In this survey, we categorize NLIDBs into seven distinct groups according to the data stored in the backend. The representative systems for each category are depicted in Figure 8. Among the various categories, natural language interfaces for relational data are the most prevalent and functional, and are subjected to ongoing research on an annual basis. Recently, research on NLIs for XML data has not advanced, remaining at the same stage as in 2007. The two main reasons are (i) an increasing preference for JSON as a format for data exchange over XML , and (ii) the suitability of NoSQL databases for handling unstructured or semi-structured data over XML databases . Since 2013, NLIs for natural language queries over RDF data, ontology data, graph data, spatial data, and spatio-temporal data have been developed. The executable languages transformed by these NLIs correspond to the databases used.\n",
    "\n",
    "NLIDB for relational data transforms natural language queries into SQL. IRNet [53] first identifies the entities contained in the NLQ, including columns, tables and values. Subsequently, a neural model based on syntax is used to synthesize an intermediate representation connecting natural language with SQL. Finally, IRNet derives SQLs on the basis of intermediate representations. Representative NLIDBs for XML databases are NaLIX [85] and DaNaLIX [81], which transform natural language queries into XQuery. NaLIX restricts natural language to a predefined subset of the grammar. DaNaLIX builds upon NaLIX and enables users to leverage domain knowledge for query transformation. TEQUILA [64] is a typical NLIDB for RDF data, which transforms natural language queries into SPARQL. TEQUILA employs a standard knowledge-based question and answer system to evaluate sub-questions independently. The results of the sub-questions are then combined for inference to compute the answer to the full question. QuestIO [27] works for querying structured data represented in ontology format. Built on the ontology and a knowledge base containing instances of the ontology's concepts, QuestIO accepts NLQ as input and produces SeRQL as output. Utilizing the language processing framework GATE, QuestIO\n",
    "\n",
    "Figure 8: Classification of NLIDBs based on data stored in the backend\n",
    "\n",
    "<!-- image -->\n",
    "\n",
    "combines fundamental concepts with keywords, blocks and phrases to deduce potential relationships among the concepts in the ontology. In the spatio-temporal domain, NLIDB can handle GIS-related queries, such as historical meteorological data at a specific location, and geographic position information at different moments. NeuroSPE [109] is a spatial extraction model designed to identify spatial relations within Chinese natural language text. The model extends a bidirectional gated recurrent neural network with a series of pre-trained models and is able to address specific challenges in a variety of natural language text, including the absence of direct context and the occurrence of abbreviations, special languages, and symbols. NALMO [144, 143] is a natural language interface designed for moving objects that allows users to submit queries of five types, including (i) time interval queries , (ii) range queries , (iii) nearest neighbor queries , (iv) trajectory similarity queries , and (v) join queries .\n",
    "\n",
    "Several systems have been created that can be used across various back-end data stores, with the objective of enhancing the generality of NLIDB. TR Discover [121] is one such system which transforms NLQ into SPARQL or SQL. TR Discover generates FOL representations by analyzing natural language using a feature-based contextindependent grammar consisting of entries in the vocabulary for leaf nodes and rules governing the phrase structure for non-terminal nodes. The FOL representation is then parsed into a parse tree through the utilization of a firstorder logic parser. The parse tree is traversed sequentially and transformed into SPARQL or SQL. FINESSE [63], an extension to ATHENA, is a system that seamlessly connects to multiple structured data stores. FINESSE can access various structured backends (e.g., RDF stores and Graph stores) by automatically transforming the intermediate query language OQL into the corresponding structured query language specific to the backends (e.g., SPARQL and Gremlin).\n",
    "\n",
    "## 5.2 Development and usage of system interfaces\n",
    "\n",
    "The combination of the aforementioned three components, including (i) natural language preprocessing , (ii) natural language understanding and (iii) natural language translation , constitutes a comprehensive system architecture. Then the theoretical knowledge is implemented in the form of a system. There are two primary methods of development and usage:\n",
    "\n",
    "(i) A stand-alone software. In this scenario, the system generally comprises a separate visual interface and a database, and the architecture is shown in Figure 9(a). A visual interface allows users to write natural language\n",
    "\n",
    "<!-- image -->\n",
    "\n",
    "(a)\n",
    "\n",
    "A stand-alone software\n",
    "\n",
    "<!-- image -->\n",
    "\n",
    "(b)\n",
    "\n",
    "A plug-in for DBMS\n",
    "\n",
    "Figure 9: The architecture of the database system with NLI\n",
    "\n",
    "problems that are interactively translated into executable language. By submitting the executable language in the corresponding database management system, the query results can be obtained. The paper [78] presents the JavaScript-driven interface of NaLIR, which interacts with a master server implemented in Java. NL2CM is implemented in Java 7, whose web user interface is constructed in PHP 5.3 and jQuery 1.x. The paper [62] develops a web interface designed to receive NLQs from users directed towards academic databases and display translated SQLs. The interface also shows several example utterances to assist users in comprehending the domain. The tool that comes with the NLMO system is a web application written in Java.\n",
    "\n",
    "(ii) A plug-in for the database management system. In this instance, the system exists in a format analogous to a Python custom module and interacts with the user through the visual interface of the database management system. The system architecture is illustrated in Figure 9(b). The user inputs NLQ by invoking the interface provided by the system. Thereafter, the database management system automatically calls the NLI module to process the NLQ, and displays the translated executable language on the visual interface. One of the most typical systems is NALMO, which is developed on a laptop running Ubuntu 14.04. The final interface form in SECONDO is represented as an algebraic module with an operator. The users can use the operator on the moving objects databases in SECONDO to perform the corresponding NLQ translation of moving objects.\n",
    "\n",
    "## 5.3 Enhancement of NLIDB systems\n",
    "\n",
    "Although existing NLIDBs have been able to achieve the transformation from natural language to executable database language, the research on NLIDB is a long process and the systems need to be optimized step by step because natural language has rich expressions, ambiguous semantic knowledge and intricate correlations [61]. Enhancements to the existing NLIDB systems are mainly in the following three areas: (i) interpreting answers and non-answers to queries , (ii) improving the effectiveness of the system , and (iii) securing the system against potential vulnerabilities .\n",
    "\n",
    "## 5.3.1 Interpreting answers and non-answers to queries\n",
    "\n",
    "Researchers have enhanced the functionalities of existing systems with regards to providing explanations for both query answers and non-answers. Users of NLIDB do not usually have the relevant expertise and may have difficulty in understanding the results or verifying their correctness. In this work, papers [31, 32, 33, 34] complement these efforts by providing NL explanations for query answers. The authors propose a system named NLProv, which employs the original NLQ structure to transform the provenance information into natural language. The obtained provenance information is then presented to the user in the form of natural language answers, through a four-step process:\n",
    "\n",
    "- · The user inputs a query using natural language that is transmitted to the improved NaLIR. The system processes the NLQ, constructs a formal query, and stores the translated portions of the NLQ in relation to the formal query.\n",
    "- · NLProv employs the SelP system [36] to evaluate formal queries and records the provenance of each query, indicating the correlation between dependency tree nodes and specific provenance sections.\n",
    "- · The source information is decomposed and then compiled into an NL answer with explanation.\n",
    "- · The system presents the factorized answer to the user. In cases where the answer is excessively detailed and difficult to comprehend, users have the option to access summaries at various levels of nesting.\n",
    "\n",
    "The paper [34] proposes a general solution for NLProv that is not specific to NaLIR. The core of the solution is an alternative architecture that does not depend on the query builder for producing the partial mappings between the nodes of the dependency tree and the components of the query. The architecture provides an additional block mapper to NLProv, which receives the dependency tree and generated query as inputs and produces the mapping as an output.\n",
    "\n",
    "Users may fail to obtain the expected results when using NLIDBs, leading to surprise or confusion. NLProveNAns [35] enriches NaLIR by supporting interpretations of non-answer. NLProveNAns can provide two explanations, corresponding to two different why-not source models: (i) a concise explanation rooted in the picky boundary model and (ii) a comprehensive explanation derived from the polynomial model . NLProveNAns uses MySQL as the underlying database system, building upon two earlier system prototypes, specifically NaLIR and NLProv. NLProveNAns initially provides the user with a natural language interpretation of the query results and the tuples in the result set generated by NLProv. The user then formulates a ' why-not ' query. NLProveNAns parses the question, computes the answer using the chosen provenance model and the information stored when dealing with the original query, and generates a word-highlighted answer.\n",
    "\n",
    "## 5.3.2 Improving the effectiveness of the system\n",
    "\n",
    "Numerous researchers have provided user interaction components for NLIDB systems to improve effectiveness. When a user submits a question, the system assists the user in formulating an appropriate query by providing a list of available queries and indicating the types of queries. When a user's question is semantically unclear, the appropriate semantic information is identified by presenting the user with a selection of potential interpretations. When the data inputted by the user is not found in the database, similar information in the database can be provided to the user in the form of an associative prompt. Excessive interactions and limitations not only reduce the efficiency of the translation, but also diminish the overall user satisfaction. Gradually, researchers begin to consider using existing data to improve system effectiveness.\n",
    "\n",
    "A key challenge to improving system effectiveness lies in closing the semantic gap between natural language and the fundamental data in the database. This challenge is reflected in join path inference and keyword mapping when converting natural language to SQL. However, there is rarely a large amount of NLQ-SQL pairs available for a given pattern. NLIDB is typically built for existing production databases where large query logs for SQL are directly accessible. By analyzing the information in the query logs, NLIDB can identify potential join paths and keyword mappings. TEMPLAR [9] augments existing pipeline-based NLIDBs using query log information, and the architecture is shown in Figure 10. TEMPLAR models the data from the query log using a data structure known as the Query Fragment Graph (QFG), leveraging the information to enhance the capabilities of current NLIDBs in join path inference and keyword mapping. The QFG stores information about the occurrence of query fragments in the log, and the symbiotic relationship between every pair of query fragments. Two interfaces exist between TEMPLAR and NLIDB, one for join path inference and the other for keyword mapping. The experimental evaluation in the paper [9] proves the effectiveness of TEMPLAR, which greatly improves the translatability of NaLIR and Pipeline by using query logs for SQL.\n",
    "\n",
    "Taking the NLQ ' Find papers from 2000 until 2010. ' from the Microsoft Academic Search database as an\n",
    "\n",
    "Figure 10: The architecture of the NLIDB enhanced by TEMPLAR\n",
    "\n",
    "<!-- image -->\n",
    "\n",
    "example, the translation process of NaLIR enhanced with TEMPLAR is as follows.\n",
    "\n",
    "In the initial step, the NLQ is parsed using NaLIR to identify the keywords associated with the database elements and the relevant parser metadata. In this instance, the keywords identified by NaLIR are papers and from 2000 until 2010 . The result of using NaLIR to generate metadata is papers in the SELECT context and from 2000 until 2010 in the WHERE context.\n",
    "\n",
    "In the second step, the keywords are transmitted to the Keyword Mapper that utilizes the keyword metadata and pertinent information from the database to associate each keyword with potential query segments and assign a score to these segments. In this example, the candidate mappings for papers include (journal.name, SELECT) and (publication.title, SELECT) , and from 2000 until 2010 is mapped to (publication.year ≥ 2000 AND publication.year ≤ 2010, WHERE) . The Keyword Mapper transmits the two most likely candidate configurations back to\n",
    "\n",
    "## NaLIR as follows.\n",
    "\n",
    "- · [(journal.name, SELECT); (publication.year &gt; = 2000 AND publication.year &lt; = 2010, WHERE)]\n",
    "- · [(publication.title, SELECT); (publication.year &gt; = 2000 AND publication.year &lt; =\n",
    "\n",
    "2010, WHERE)]\n",
    "\n",
    "In the third step, NaLIR sends the known relationship of every candidate configuration to the Join Path Generator to generate the most probable join path. In this example, the Join Path Generator generates the join paths journal-publication and publication for the two configurations, respectively.\n",
    "\n",
    "In the final step, NaLIR utilizes the join paths returned by the Join Path Generator to construct and return the SQL for each candidate configuration. In this example, the final translated SQLs are as follows.\n",
    "\n",
    "- · SELECT j.name FROM journal j, publication p\n",
    "- WHERE p.year &gt; = 2000 AND p.year &lt; =\n",
    "- 2010 AND j.jid = p.jid\n",
    "- · SELECT title FROM publication WHERE year &gt; = 2000 AND year &lt; = 2010\n",
    "\n",
    "## 5.3.3 Securing the system against potential vulnerabilities\n",
    "\n",
    "Research on the security vulnerabilities arising from malicious user interactions is relatively limited. Zhang et al. [164] propose a backdoor-based SQL injection framework for Text2SQL systems named TrojanSQL, using two injection attacks: (i) boolean-based and (ii) union-based . Boolean-based injection is used for conditional queries\n",
    "\n",
    "with WHERE clauses and invalidates the original query condition by performing boolean operations on existing conditional judgments to bypass the original query condition. Union-based injection aims to steal private information, including database meta-information and user data privacy by performing a union query on the original user query. Experimental results demonstrate that TrojanSQL has a high attack success rate against current Text2SQL systems and is difficult to defend against. Zhang et al. [164] provide security practice recommendations for NLIDB developers to reduce the risk of SQL injection attacks:\n",
    "\n",
    "- · The utilization of only officially recognized or peer-reviewed data sets for model training is recommended.\n",
    "- · The selection of a verified and reputable source for initializing model weights is advised.\n",
    "- · The implementation of additional layers of security or filtering should be considered when using model linking techniques.\n",
    "- · Rigorous testing should be performed prior to the integration of NLIDB APIs provided by third parties into an application.\n",
    "\n",
    "## 6 Discussions about Text2SQL with LLM, SQL2Text and Speech2SQL\n",
    "\n",
    "We discuss deep language understanding and database interaction techniques related to NLIDB, including the use of LLM for Text2SQL tasks, the creation of natural language interpretations from SQL, and the transformation of speech queries into SQL.\n",
    "\n",
    "## 6.1 Text2SQL with LLM\n",
    "\n",
    "The advent of the Transformer architecture [134] has resulted in considerable success of LLMs in natural language processing tasks. The models effectively capture the deep structure and semantic information of language through pre-training and fine-tuning [94]. Decoder-only, encoder-only and encoder-decoder are the principal structures of LLMs.\n",
    "\n",
    "(i) The decoder-only model , represented by GPT [18, 98], exclusively comprises a decoder and generates output sequences progressively through an autoregressive approach. The model is suitable for generative tasks such as text generation and dialogue systems [110]. However, the model exhibits limited effectiveness when processing long texts due to the autoregressive nature. Additionally, the model does not directly handle input information, posing a challenge of unidirectional information transmission.\n",
    "\n",
    "(ii) The encoder-only model , represented by BERT [37], contains only an encoder and extracts context through bidirectional training. This architecture is applicable to tasks involving context comprehension and supervised learning. Lacking a direct output generation mechanism, the model is unsuitable for generative tasks. In addition, the model cannot handle variable-length outputs in seq2seq tasks.\n",
    "\n",
    "(iii) The encoder-decoder model , represented by T5 [111], consists of an encoder and a decoder. The encoder maps the input sequence to a high-dimensional contextual representation, which is then utilized by the decoder to produce the output sequence. The architecture excels in tasks requiring global information transfer, such as machine translation and summary generation [76]. However, the computational resource demands of the model are high, and the complexity of information transfer may lead to performance degradation in certain tasks.\n",
    "\n",
    "LLMs contribute to the development of NLIDB. Notably, the growing popularity of GPT [18, 98] opens new possibilities for NLP in NLIDB systems. GPT supports natural language queries over spatial data and returns sensible SQL frameworks.\n",
    "\n",
    "EXAMPLE1. Taking the NLQ 'Can you tell me what POIs are available in Jiangning District?' as an example, the SQL generated by GPT is as follows.\n",
    "\n",
    "## SELECT POI.name\n",
    "\n",
    "FROM POI JOIN district ON ST Within(POI.geom, district.geom)\n",
    "\n",
    "WHERE district.name = 'Jiangning District';\n",
    "\n",
    "The query employs the ST Within function to ascertain whether the location of each POI is within Jiangning District. GPT extracts the entities (POI and district) and the query type (range query).\n",
    "\n",
    "However, GPT is primarily designed for traditional relational data and has limited ability to represent spatial data. While adept at processing simple objects(e.g., points), GPT's representation capabilities are less effective when dealing with more intricate objects(e.g., lines and regions).\n",
    "\n",
    "EXAMPLE 2. Taking the NLQ 'What cinemas are there on Sterndamm street?' as an example, the SQL generated by GPT is as follows.\n",
    "\n",
    "## SELECT name\n",
    "\n",
    "FROM cinemas\n",
    "\n",
    "WHERE ST Intersects (location, ST GeomFromText (' LINESTRING (13.531836 52.437831, 13.536510 52.434202 )', 4326));\n",
    "\n",
    "GPT is capable of capturing the pivotal semantic details contained within the query, including cinemas, Sterndamm street and the spatial correlation between them. However, the representation of Sterndamm street in the executable language is not accurate and Sterndamm street comprises multiple segments. Upon receiving the prompt 'Sterndamm street is stored in the spatial relation streets', GPT generates a reasonable SQL:\n",
    "\n",
    "## SELECT name\n",
    "\n",
    "FROM cinemas\n",
    "\n",
    "WHERE ST Intersects (location, (SELECT ST Buffer (geom, 0.0001) FROM streets WHERE name =\n",
    "\n",
    "'Sterndamm'));\n",
    "\n",
    "The query utilizes the ST Buffer function to create a buffer with a size of 0.0001 degrees (approximately 11 meters) around Sterndamm street and subsequently employs the ST Intersects function to examine whether the location of each cinema intersects with the buffer.\n",
    "\n",
    "The advent of intricate deep learning architectures has prompted a focus on accurately interpreting natural language and generating structured language by optimizing LLMs. This direction emphasizes optimizing the LLM through larger data pre-training, superior language representation learning techniques, and more efficient fine-tuning methods. Zero-sample learning strategies have also received attention to enable the system to handle unseen query types without retraining, which can be achieved through zero-sample learning and meta-learning techniques.\n",
    "\n",
    "## 6.2 SQL2Text\n",
    "\n",
    "The purpose of SQL2Text is to transform complex SQL into natural language description. This transformation helps non-technical users to comprehend the logic and structure of SQL, thus making database interactions transparent and understandable. Koutrika et al. [73] utilize a graph-based approach for transforming SQL into natural language. SQL is first represented as a directed graph whose edges are labeled with template labels using an extensible template mechanism, thus providing semantics for the parts of the query. These graphs are then explored and textual query description is composed using a variety of graph traversal strategies, including the binary search tree algorithm, the multi-reference point algorithm, and the template combination algorithm. Eleftherakis et al. [40] address SQL2Text by extending the graph-based model of Logos to translate a wider range of queries (e.g. SELECT TOP, LIMIT, IN, and LIKE). The SQL is first analyzed to generate a parse tree storing the essential information utilized to construct the query graph, and then the textual description of the SQL is created through\n",
    "\n",
    "the application of the multi-reference point traversal strategy. Camara et al. [20] employ LLM to generate explanations of SQL. The logical structure of SQL is recorded and the columns and tables are interpreted in natural language.\n",
    "\n",
    "Although progress has been made in this direction, there remains ample opportunity for enhancement. Future research will focus on improving the quality and richness of the generated natural language explanations, ensuring that they are both accurate and rich. In addition, future research will explore context-awareness, which means providing relevant natural language explanations in conjunction with the contextual information in the user's query. This technique also involves exploring how SQL2Text can be combined with dialogue systems to enable intelligent and coherent database interactions.\n",
    "\n",
    "## 6.3 Speech2SQL\n",
    "\n",
    "Speech2SQL technology is designed to transform speech input into SQL, making the process of database querying as simple and intuitive as speaking, thus significantly reducing the barrier to database interaction. SpeakQL [21, 119, 117, 118] converts speech SQL into queries that are displayed on the screen, where users can perform interactive query corrections using a screen-based touch interface or a single click. SpeakQL utilizes automatic speech recognition (ASR) tools to record speech SQL which will be output as text. The Structure Determination component of SpeakQL is responsible for post-processing the ASR results in order to generate syntactically accurate SQL with textual placeholders, and then uses the original ASR output to fill in the textual placeholders. SpeakNav [165, 12] is a system that combines natural language understanding with route search related to navigation. Users are permitted to describe a predetermined route by voice, and SpeakNav presents a suggested path on a map accompanied by information regarding the estimated duration and distance of the journey. MUVE [147, 148] converts NLQs formulated in speech to SQL using a greedy heuristic approach that does not ensure an optimal solution, but produces a solution that is close to optimal. MUVE answers speech queries by utilizing a multi-plot approach, including multiple bar graphs that display the outcomes of various query options. SpeechSQLNet [122] is an end-to-end neural architecture designed to convert speech into SQL directly, obviating the necessity for an external ASR. SpeechSQLNet effectively combines a transformer, a graphical neural network, and a speech encoder as foundational components. The speech encoder is first used to transform speech into a concealed representation, and the GNN-based encoder is employed to convert patterns that have a considerable influence on the desired SQL into hidden features to safeguard the structural information. The speech embedding is then combined with pattern characteristics to generate semantically consistent SQL. Wav2SQL [86] is also an end-to-end Speech2SQL parser that utilizes self-supervised learning to address the challenge of limited data availability and generate diverse representations. Furthermore, speech reprogramming and gradient inversion techniques are introduced to eliminate stylistic attributes in the speech representation and enhance the generalization ability of the model to user-defined data. VoiceQuerySystem [123] is a speech-based database query system that generates SQL from NLQ speech using two methods:\n",
    "\n",
    "- · Cascade approach involves converting speech-based natural language queries to text using a proprietary ASR module, followed by the generation of SQL through IRNet.\n",
    "- · End-to-end approach directly converts speech to SQL without the need for text as an intermediate medium, by using SpeechSQLNet.\n",
    "\n",
    "Despite the considerable efforts invested in speech recognition and interaction technologies, there remain significant challenges that require further attention. Subsequent research is expected to concentrate on enhancing the accuracy of speech recognition, possibly by utilizing end-to-end speech recognition models and integrating multiple modalities with other input sources. This technique will also involve investigating the potential for combining speech interaction with text query processing techniques to facilitate seamless and efficient database interaction.\n",
    "\n",
    "## 7 Future research and conclusions\n",
    "\n",
    "We investigate unresolved issues and potential directions for future research in the area of NLIDB and provide the conclusions of this paper.\n",
    "\n",
    "## 7.1 Open problems\n",
    "\n",
    "Despite the considerable advancements made by NLIDB, numerous challenges and issues remain to be addressed. The following is a list of the principal open problems with the technical details.\n",
    "\n",
    "Natural language disambiguation. The ambiguity and polysemous nature of natural language makes NLIDB systems face great challenges in correctly understanding user intentions. Future research should focus on the following aspects.\n",
    "\n",
    "- (i) Contextual understanding. Advanced context-aware models can be developed to utilize contextual information for disambiguation. Attention mechanisms and memory networks allow to keep track of the context in a dialogue system.\n",
    "- (ii) Multi-round dialogue. Introducing multiple rounds of dialogue enables the system to gradually clarify users' intent through a series of interactions, which requires the design of an effective dialogue management strategy and a mechanism for confirming users' intent.\n",
    "\n",
    "(iii) Semantic parsing. Complex semantic parsing techniques, such as semantic role labeling and knowledge graph, can be utilized to elucidate the implicit information in natural language.\n",
    "\n",
    "Query optimization. Converting natural language queries into efficient database queries and optimizing query performance during execution remain significant challenges. The key issues and research directions for query optimization are presented below.\n",
    "\n",
    "(i) Index Selection. Depending on the query criteria and data distribution, the indexing scheme that optimizes retrieval speed is selected. The optimizer scans the existing indexes, evaluates the selectivity and cost of each index, and determines which indexes filter the data most efficiently. In complex queries, multiple indexes may be used simultaneously, and the optimizer will select a union index or cross-index scan to improve query performance.\n",
    "\n",
    "(ii) Query rewriting is a method of simplifying the execution plan and improving query efficiency. Sub-queries can be reformulated as joins to simplify complex nested queries. Additionally, the value of constant expressions can be computed in advance in the query, reducing the runtime computation. Finally, redundant sorting, joining, or filtering operations can be removed from the query to simplify the query execution plan.\n",
    "\n",
    "(iii) Execution plan selection. The cost of each execution plan is evaluated using statistical information (e.g., table size and index distribution) and a cost model (rule-based or cost-based). This evaluation considers I/O operations, CPU time, and memory utilization. The least costly plan can be identified through dynamic programming or heuristic algorithms, thereby ensuring that the query is executed with minimal resource consumption and optimal performance.\n",
    "\n",
    "(iv) Join optimization. The join operation is a highly resource-consuming process, and determining the most efficient join order and method is critical. The selection of suitable join algorithms (e.g., subsumption joins, hash joins and nested loop joins) and the application of join condition derivation can lead to a reduction in the quantity of join operations, thus optimizing join performance and improving query efficiency.\n",
    "\n",
    "Corpus construction. One of the most pressing issues in the research of NLIDB is the construction and utilization of the corpus, with particular focus on the following aspects.\n",
    "\n",
    "(i) In order to guarantee the generality and adaptability of the natural language interface system, one needs to collect data from diverse sources. Multi-source data integration techniques can be employed to gather information from user query logs, social media conversations, and customer service records to ensure that the corpus is diverse and representative.\n",
    "\n",
    "- (ii) A high-quality corpus relies on accurate annotation, which requires the integration of manual and automated tools. The development of collaborative annotation platforms and automated annotation tools can enhance the efficiency and uniformity of annotation, concurrently establishing a quality assessment system to detect and rectify annotation errors, thus ensuring the accuracy and reliability of data annotation.\n",
    "\n",
    "(iii) Protecting user privacy and data security is of paramount importance when constructing and utilizing the corpus. The application of differential privacy and data encryption techniques, in conjunction with the formulation of guidelines for the ethical use of data, can guarantee legality and compliance in the process of data collection and utilization. Transparency and user control techniques enable users to understand and regulate the usage of data.\n",
    "\n",
    "(iv) The construction and evaluation of the corpus necessitate a unified and standardized framework to facilitate the comparison of research results and the sharing of data. The establishment of open data platforms and the promotion of cross-institutional cooperation can address legal and technical challenges in data sharing and promote the sharing and reuse of resources and results.\n",
    "\n",
    "## 7.2 Conclusions\n",
    "\n",
    "This paper offers a comprehensive review of recently proposed NLIDBs. We summarize the translation process from natural language to database executable language in three stages: (i) natural language preprocessing , (ii) natural language understanding , and (iii) natural language translation . At the natural language preprocessing stage, we observe that almost every system employs named entity recognition and part-of-speech tagging. At the natural language understanding stage, we learn that although the limitations of rule-based approaches can be eliminated, machine learning-based semantic parsing methods are highly dependent on training data and require longer time and more memory space to build models. At the natural language translation stage, we provide a general process for building executable languages over relational and spatio-temporal databases. Furthermore, we provide a summary of the common benchmarks for translating natural language queries into executable languages, system evaluation metrics, and the classification, development, and enhancement of NLIDBs. Despite the potential to enhance database accessibility, NLIDB still faces numerous challenges, including natural language disambiguation, query optimization, and corpus construction. Future research should prioritize addressing the open issues to further improve the effectiveness and user satisfaction of NLIDB systems.\n",
    "\n",
    "## References\n",
    "\n",
    "- [1] Asma Ben Abacha and Pierre Zweigenbaum. MEANS: A medical question-answering system combining NLP techniques and semantic web technologies. Inf. Process. Manag. , 51(5):570-594, 2015.\n",
    "- [2] Katrin Affolter, Kurt Stockinger, and Abraham Bernstein. A comparative survey of recent natural language interfaces for databases. VLDB J. , 28(5):793-819, 2019.\n",
    "- [3] Karam Ahkouk and Mustapha Machkour. Towards an interface for translating natural language questions to SQL: a conceptual framework from a systematic review. Int. J. Reason. based Intell. Syst. , 12(4):264-275, 2020.\n",
    "- [4] Muhammed Jassem Al-Muhammed and Deryle W. Lonsdale. Ontology-aware dynamically adaptable freeform natural language agent interface for querying databases. Knowl. Based Syst. , 239:108012, 2022.\n",
    "- [5] Yael Amsterdamer, Anna Kukliansky, and Tova Milo. A natural language interface for querying general and individual knowledge. Proc. VLDB Endow. , 8(12):1430-1441, 2015.\n",
    "- [6] Yael Amsterdamer, Anna Kukliansky, and Tova Milo. Nl 2 cm: A natural language interface to crowd mining. In SIGMOD , pages 1433-1438, 2015.\n",
    "- [7] Ion Androutsopoulos, Graeme D. Ritchie, and Peter Thanisch. Natural language interfaces to databases an introduction. Nat. Lang. Eng. , 1(1):29-81, 1995.\n",
    "\n",
    "| [8]   | Yoav Artzi and Luke S. Zettlemoyer. Bootstrapping semantic parsers from conversations. In EMNLP , pages 421-432, 2011.                                                                                                                |\n",
    "|-------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| [9]   | Christopher Baik, H. V. Jagadish, and Yunyao Li. Bridging the semantic gap with SQL query logs in natural language interfaces to databases. In IEEE ICDE , pages 374-385, 2019.                                                       |\n",
    "| [10]  | Fuat Basik, Benjamin H¨ttasch, a Amir Ilkhechi, Arif Usta, Shekar Ramaswamy, Prasetya Utama, Nathaniel Weir, Carsten Binnig, and Ugur Cetintemel. ¸ Dbpal: A learned nl-interface for databases. In SIGMOD , pages 1765-1768, 2018.   |\n",
    "| [11]  | Adithya Bhaskar, Tushar Tomar, Ashutosh Sathe, and Sunita Sarawagi. Benchmarking and improving text-to-sql generation under ambiguity. In EMNLP , pages 7053-7074, 2023.                                                              |\n",
    "| [12]  | Lei Bi, Juan Cao, Guohui Li, Nguyen Quoc Viet Hung, Christian S. Jensen, and Bolong Zheng. Speaknav: Avoice-based navigation system via route description language understanding. In ICDE , pages 2669-2672, 2021.                    |\n",
    "| [13]  | Steven Bird. NLTK: the natural language toolkit. In ACL , 2006.                                                                                                                                                                       |\n",
    "| [14]  | Adrian N. Bishop, Jeremie Houssineau, Daniel Angley, and Branko Ristic. Spatio-temporal tracking from natural language statements using outer probability theory. Inf. Sci. , 463-464:56-74, 2018.                                    |\n",
    "| [15]  | Lukas Blunschi, Claudio Jossen, Donald Kossmann, Magdalini Mori, and Kurt Stockinger. SODA: gener- ating SQL for business users. Proc. VLDB Endow. , 5(10):932-943, 2012.                                                             |\n",
    "| [16]  | Joel Booth, Barbara Di Eugenio, Isabel F. Cruz, and Ouri Wolfson. Robust natural language processing for urban trip planning. Appl. Artif. Intell. , 29(9):859-903, 2015.                                                             |\n",
    "| [17]  | Joel Booth, A. Prasad Sistla, Ouri Wolfson, and Isabel F. Cruz. Adata model for trip planning in multimodal transportation systems. In EDBT , volume 360 of ACM International Conference Proceeding Series , pages 994-1005, 2009.    |\n",
    "| [18]  | Tom B. Brown, Benjamin Mann, Nick Ryder, and et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901, 2020.                                                                    |\n",
    "| [19]  | Ursin Brunner and Kurt Stockinger. Valuenet: A natural language-to-sql system that learns from database information. In ICDE , pages 2177-2182, 2021.                                                                                 |\n",
    "| [20]  | Vanessa Cˆmara, a Rayol Mendonca-Neto, Andr´ e Silva, and Luiz Cordovil Jr. A large language model approach to sql-to-text generation. In ICCE , pages 1-4, 2024.                                                                     |\n",
    "| [21]  | Dharmil Chandarana, Vraj Shah, Arun Kumar, and Lawrence K. Saul. Speakql: Towards speech-driven multi-modal querying. In HILDA@SIGMOD , pages 11:1-11:6, 2017.                                                                        |\n",
    "| [22]  | Chih-Yung Chang, Yuan-Lin Liang, Shih-Jung Wu, and Diptendu Sinha Roy. Sv 2 -sql: a text-to-sql trans- formation mechanism based on BERT models for slot filling, value extraction, and verification. Multim. Syst. , 30(1):16, 2024. |\n",
    "| [23]  | Peng Chen, Hui Li, Sourav S. Bhowmick, Shafiq R. Joty, and Weiguo Wang. LANTERN: boredom- conscious natural language description generation of query execution plans for database education. In SIGMOD , pages 2413-2416, 2022.       |\n",
    "| [24]  | Yi-Hui Chen, Eric Jui-Lin Lu, and Ting-An Ou. Intelligent SPARQL query generation for natural language processing systems. IEEE Access , 9:158638-158650, 2021.                                                                       |\n",
    "| [25]  | Jianpeng Cheng, Siva Reddy, Vijay A. Saraswat, and Mirella Lapata. Learning structured natural language representations for semantic parsing. In ACL , pages 44-55, 2017.                                                             |\n",
    "| [26]  | Danica Damljanovic, Milan Agatonovic, and Hamish Cunningham. Freya: An interactive way of querying linked data using natural language. In ESWC , volume 7117 of Lecture Notes in Computer Science , pages 125-138, 2011.              |\n",
    "| [27]  | Danica Damljanovic, Valentin Tablan, and Kalina Bontcheva. Atext-based query interface to OWLontolo- LREC                                                                                                                             |\n",
    "\n",
    "| [28]   | Alaka Das and Rakesh Chandra Balabantaray. Mynlidb: A natural language interface to database. In ICIT , pages 234-238, 2019.                                                                                                                                          |\n",
    "|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| [29]   | C. J. Date. A critique of the SQL database language. SIGMOD Rec. , 14(3):8-54, 1984.                                                                                                                                                                                  |\n",
    "| [30]   | Ephrem Tadesse Degu and Rosa Tsegaye Aga. Natural language interface for covid-19 amharic database using LSTM encoder decoder architecture with attention. In ICT4DA , pages 95-100, 2021.                                                                            |\n",
    "| [31]   | Daniel Deutch, Nave Frost, and Amir Gilad. Nlprov: Natural language provenance. Proc. VLDB Endow. , 9(13):1537-1540, 2016.                                                                                                                                            |\n",
    "| [32]   | Daniel Deutch, Nave Frost, and Amir Gilad. Provenance for natural language queries. Proc. VLDB Endow. , 10(5):577-588, 2017.                                                                                                                                          |\n",
    "| [33]   | Daniel Deutch, Nave Frost, and Amir Gilad. Natural language explanations for query results. SIGMOD Rec. , 47(1):42-49, 2018.                                                                                                                                          |\n",
    "| [34]   | Daniel Deutch, Nave Frost, and Amir Gilad. Explaining natural language query results. VLDB J. , 29(1):485-508, 2020.                                                                                                                                                  |\n",
    "| [35]   | Daniel Deutch, Nave Frost, Amir Gilad, and Tomer Haimovich. Nlprovenans: Natural language provenance for non-answers. Proc. VLDB Endow. , 11(12):1986-1989, 2018.                                                                                                     |\n",
    "| [36]   | Daniel Deutch, Amir Gilad, and Yuval Moskovitch. Selective provenance for datalog programs using top-k queries. Proc. VLDB Endow. , 8(12):1394-1405, 2015.                                                                                                            |\n",
    "| [37]   | Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirec- tional transformers for language understanding. In NAACL-HLT , pages 4171-4186, 2019.                                                                           |\n",
    "| [38]   | Li Dong and Mirella Lapata. Language to logical form with neural attention. In ACL , 2016.                                                                                                                                                                            |\n",
    "| [39]   | Li Dong and Mirella Lapata. Coarse-to-fine decoding for neural semantic parsing. In ACL , pages 731-742, 2018.                                                                                                                                                        |\n",
    "| [40]   | Stavroula Eleftherakis, Orest Gkini, and Georgia Koutrika. Let the database talk back: Natural language explanations for SQL. In SEA-Data , volume 2929 of CEUR Workshop Proceedings , pages 14-19, 2021.                                                             |\n",
    "| [41]   | Tatiana N. Erekhinskaya, Dmitriy Strebkov, Sujal Patel, Mithun Balakrishna, Marta Tatu, and Dan I. Moldovan. Ten ways of leveraging ontologies for natural language processing and its enterprise appli- cations. In SBD@SIGMOD , pages 8:1-8:6, 2020.                |\n",
    "| [42]   | Yuankai Fan, Zhenying He, Tonghui Ren, Dianjun Guo, Lin Chen, Ruisi Zhu, Guanduo Chen, Yinan Jing, Kai Zhang, and X. Sean Wang. Gar: Agenerate-and-rank approach for natural language to SQL translation. In ICDE , pages 110-122, 2023.                              |\n",
    "| [43]   | Yuankai Fan, Tonghui Ren, Dianjun Guo, Zhigang Zhao, Zhenying He, X. Sean Wang, Yu Wang, and Tao Sui. An integrated interactive framework for natural language to SQL translation. In WISE , volume 14306 of Lecture Notes in Computer Science , pages 643-658, 2023. |\n",
    "| [44]   | Yuankai Fan, Tonghui Ren, Zhenying He, X. Sean Wang, Ye Zhang, and Xingang Li. Gensql: Agenerative natural language interface to database systems. In ICDE , pages 3603-3606, 2023.                                                                                   |\n",
    "| [45]   | Alessandro Fantechi, Stefania Gnesi, Samuele Livi, and Laura Semini. A spacy-based tool for extracting variability from NL requirements. In SPLC , pages 32-35, 2021.                                                                                                 |\n",
    "| [46]   | S´bastien e Ferr´. e Sparklis: An expressive query builder for SPARQL endpoints with guidance in natural language. Semantic Web , 8(3):405-418, 2017.                                                                                                                 |\n",
    "| [47]   | Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir R. Radev. Improving text-to-sql evaluation methodology. In ACL , pages 351-360, 2018.                                                         |\n",
    "| [48]   | Han Fu, Chang Liu, Bin Wu, Feifei Li, Jian Tan, and Jianling Sun. Catsql: Towards real world natural language to SQL applications. Proc. VLDB Endow. , 16(6):1534-1547, 2023.                                                                                         |\n",
    "\n",
    "| [49]   | Kaitlyn Fulford and Aspen Olmsted. Mobile natural language database interface for accessing relational data. In i-Society , pages 86-87, 2017.                                                                                                                                                                                                           |\n",
    "|--------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| [50]   | Yujian Gan, Xinyun Chen, Jinxia Xie, Matthew Purver, John R. Woodward, John H. Drake, and Qiaofu Zhang. Natural SQL: making SQL easier to infer from natural language specifications. In EMNLP , pages 2030-2042, 2021.                                                                                                                                  |\n",
    "| [51]   | Robert Giaquinto, Dejiao Zhang, Benjamin Kleiner, Yang Li, Ming Tan, Parminder Bhatia, Ramesh Nalla- pati, and Xiaofei Ma. Multitask pretraining with structured knowledge for text-to-sql generation. In ACL , pages 11067-11083, 2023.                                                                                                                 |\n",
    "| [52]   | Alessandra Giordani and Alessandro Moschitti. Translating questions to SQL queries with generative parsers discriminatively reranked. In COLING , pages 401-410, 2012.                                                                                                                                                                                   |\n",
    "| [53]   | Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang Lou, Ting Liu, and Dongmei Zhang. Towards complex text-to-sql in cross-domain database with intermediate representation. In ACL , pages 4524-4535, 2019.                                                                                                                                          |\n",
    "| [54]   | Izzeddin Gur, Semih Yavuz, Yu Su, and Xifeng Yan. Dialsql: Dialogue based structured query generation. In ACL , pages 1339-1349, 2018.                                                                                                                                                                                                                   |\n",
    "| [55]   | Ralf Hartmut G¨ting. u An introduction to spatial database systems. VLDB J. , 3(4):357-399, 1994.                                                                                                                                                                                                                                                        |\n",
    "| [56]   | Ralf Hartmut G¨ting, u Thomas Behr, and Christian D¨ntgen. u SECONDO: A platform for moving objects database research and for publishing and integrating research implementations. IEEE Data Eng. Bull. , 33(2):56-63, 2010.                                                                                                                             |\n",
    "| [57]   | Ditiman Hazarika, Gopal Konwar, Shuvam Deb, and Dibya Jyoti Bora. Sentiment analysis on twitter by using textblob for natural language processing. In ICRMAT , volume 24 of Annals of Computer Science and Information Systems , pages 63-67, 2020.                                                                                                      |\n",
    "| [58]   | Jos´ e Henarejos-Blasco, Jos´ e Antonio Garc´a-D´az, ı ı ´ Oscar Apolinario-Arzube, and Rafael Valencia-Garc´a. ı Cnl-rdf-query: a controlled natural language interface for querying ontologies and relational databases. In EATIS , pages 35:1-35:5, 2020.                                                                                             |\n",
    "| [59]   | Yiqun Hu, Yiyun Zhao, Jiarong Jiang, Wuwei Lan, Henghui Zhu, Anuj Chauhan, Alexander Hanbo Li, Lin Pan, Jun Wang, Chung-Wei Hang, Sheng Zhang, Jiang Guo, Mingwen Dong, Joseph Lilien, Patrick Ng, Zhiguo Wang, Vittorio Castelli, and Bing Xiang. Importance of synthesizing high-quality data for text-to-sql parsing. In ACL , pages 1327-1343, 2023. |\n",
    "| [60]   | Ruizhe Huang and Lei Zou. Natural language question answering over RDF data. In SIGMOD , pages 1289-1290, 2013.                                                                                                                                                                                                                                          |\n",
    "| [61]   | Zachary G. Ives. Technical perspective: : Natural language explanations for query results. SIGMOD Rec. , 47(1):41, 2018.                                                                                                                                                                                                                                 |\n",
    "| [62]   | Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer. Learning a neural semantic parser from user feedback. In ACL , pages 963-973, 2017.                                                                                                                                                                          |\n",
    "| [63]   | Manasa Jammi, Jaydeep Sen, Ashish R. Mittal, Sagar Verma, Vardaan Pahuja, Rema Ananthanarayanan, Pranay Lohia, Hima Karanam, Diptikalyan Saha, and Karthik Sankaranarayanan. Tooling framework for instantiating natural language querying system. Proc. VLDB Endow. , 11(12):2014-2017, 2018.                                                           |\n",
    "| [64]   | Zhen Jia, Abdalghani Abujabal, Rishiraj Saha Roy, Jannik Str¨tgen, o and Gerhard Weikum. TEQUILA: temporal question answering over knowledge bases. In CIKM , pages 1807-1810, 2018.                                                                                                                                                                     |\n",
    "| [65]   | Jiffy Joseph, Janu R Panicker, and Meera M. An efficient natural language interface to xml database. In ICIS , pages 207-212, 2016.                                                                                                                                                                                                                      |\n",
    "| [66]   | Aishwarya Kamath and Rajarshi Das. A survey on semantic parsing. In AKBC , 2019.                                                                                                                                                                                                                                                                         |\n",
    "| [67]   | Ronak Kaoshik, Rohit Patil, Prakash R, Shaurya Agarawal, Naman Jain, and Mayank Singh. ACL-SQL:                                                                                                                                                                                                                                                          |\n",
    "\n",
    "| [68]   | George Katsogiannis-Meimarakis and Georgia Koutrika. Asurvey on deep learning approaches for text-to- sql. VLDB J. , 32(4):905-936, 2023.                                                                                                   |\n",
    "|--------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| [69]   | Esther Kaufmann, Abraham Bernstein, and Renato Zumstein. Querix: Anatural language interface to query ontologies based on clarification dialogs. In ISWC , 2006.                                                                            |\n",
    "| [70]   | Hyeonji Kim, Byeong-Hoon So, Wook-Shin Han, and Hongrae Lee. Natural language to SQL: where are we today? Proc. VLDB Endow. , 13(10):1737-1750, 2020.                                                                                       |\n",
    "| [71]   | Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.                                                                                                                                                |\n",
    "| [72]   | Georgia Koutrika. Natural language data interfaces: Adata access odyssey (invited talk). In ICDT , volume 290 of LIPIcs , pages 1:1-1:22, 2024.                                                                                             |\n",
    "| [73]   | Georgia Koutrika, Alkis Simitsis, and Yannis E. Ioannidis. Explaining structured queries in natural lan- guage. In ICDE , pages 333-344, 2010.                                                                                              |\n",
    "| [74]   | Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gardner. Neural semantic parsing with type constraints for semi-structured tables. In EMNLP , pages 1516-1526, 2017.                                                                         |\n",
    "| [75]   | Claude Lehmann, Dennis Gehrig, Stefan Holdener, Carlo Saladin, Jo˜o a Pedro Monteiro, and Kurt Stockinger. Building natural language interfaces for databases in practice. In SSDBM , pages 20:1-20:4, 2022.                                |\n",
    "| [76]   | Mike Lewis, Yinhan Liu, Naman Goyal, and et al. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In ACL , pages 7871-7880, 2020.                                          |\n",
    "| [77]   | Fei Li and H. V. Jagadish. Constructing an interactive natural language interface for relational databases. Proc. VLDB Endow. , 8(1):73-84, 2014.                                                                                           |\n",
    "| [78]   | Fei Li and H. V. Jagadish. Nalir: an interactive natural language interface for querying relational databases. In SIGMOD , pages 709-712, 2014.                                                                                             |\n",
    "| [79]   | Fei Li and H. V. Jagadish. Understanding natural language queries over relational databases. SIGMOD Rec. , 45(1):6-13, 2016.                                                                                                                |\n",
    "| [80]   | Jingjing Li, Wenlu Wang, Wei-Shinn Ku, Yingtao Tian, and Haixun Wang. Spatialnli: A spatial domain natural language interface to databases using spatial comprehension. In ACMSIGSPATIAL , pages 339-348, 2019.                             |\n",
    "| [81]   | Yunyao Li, Ishan Chaudhuri, Huahai Yang, Satinder Singh, and H. V. Jagadish. Danalix: a domain-adaptive natural language interface for querying XML. In SIGMOD , pages 1165-1168, 2007.                                                     |\n",
    "| [82]   | Yunyao Li and Davood Rafiei. Natural language data management and interfaces: Recent development and open challenges. In ACM SIGMOD , pages 1765-1770, 2017.                                                                                |\n",
    "| [83]   | Yunyao Li and Davood Rafiei. Natural Language Data Management and Interfaces . Synthesis Lectures on Data Management. 2018.                                                                                                                 |\n",
    "| [84]   | Yunyao Li, Huahai Yang, and H. V. Jagadish. Nalix: an interactive natural language interface for querying XML. In SIGMOD , pages 900-902, 2005.                                                                                             |\n",
    "| [85]   | Yunyao Li, Huahai Yang, and H. V. Jagadish. Nalix: A generic natural language search environment for XML data. ACM Trans. Database Syst. , 32(4):30, 2007.                                                                                  |\n",
    "| [86]   | Huadai Liu, Rongjie Huang, Jinzheng He, Gang Sun, Ran Shen, Xize Cheng, and Zhou Zhao. Wav2sql: Direct generalizable speech-to-sql parsing. CoRR , abs/2305.12552, 2023.                                                                    |\n",
    "| [87]   | Jian Liu, Qian Cui, Hongwei Cao, Tianyuan Shi, and Min Zhou. Auto-conversion from natural language to structured query language using neural networks embedded with pre-training and fine-tuning mechanism. In CAC , pages 6651-6654, 2020. |\n",
    "| [88]   | Mengyi Liu, Xieyang Wang, and Jianqiu Xu. NALSD: A natural language interface for spatial databases. In SSTD , pages 175-179, 2023.                                                                                                         |\n",
    "\n",
    "| [89]   | Mengyi Liu, Xieyang Wang, Jianqiu Xu, and Hua Lu. Nalspatial: An effective natural language transfor- mation framework for queries over spatial data. In SIGSPATIAL/GIS , pages 57:1-57:4, 2023.                                                                                  |\n",
    "|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| [90]   | Yuyu Luo, Nan Tang, Guoliang Li, Chengliang Chai, Wenbo Li, and Xuedi Qin. Synthesizing natural language to visualization (NL2VIS) benchmarks from NL2SQL benchmarks. In SIGMOD , pages 1235- 1247, 2021.                                                                         |\n",
    "| [91]   | Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky. The stanford corenlp natural language processing toolkit. In ACL , pages 55-60, 2014.                                                                                  |\n",
    "| [92]   | Youssef Mellah, Abdelkader Rhouati, El Hassane Ettifouri, Toumi Bouchentouf, and Mohammed Ghaouth Belkasmi. COMBINE: A pipeline for SQL generation from natural language. In ICACDS , volume 1441 of Communications in Computer and Information Science , pages 97-106, 2021.     |\n",
    "| [93]   | Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representa- tions of words and phrases and their compositionality. In NIPS , pages 3111-3119, 2013.                                                                                    |\n",
    "| [94]   | Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Comput. Surv. , 56(2):30:1-30:40, 2024. |\n",
    "| [95]   | Mohamed F. Mokbel, Mahmoud Attia Sakr, Li Xiong, Andreas Z¨fle, u and et al. Mobility data science (dagstuhl seminar 22021). Dagstuhl Reports , 12(1):1-34, 2022.                                                                                                                 |\n",
    "| [96]   | Kevin Mote. Natural language processing - A survey. CoRR , abs/1209.6238, 2012.                                                                                                                                                                                                   |\n",
    "| [97]   | Linyong Nan, Yilun Zhao, Weijin Zou, Narutatsu Ri, Jaesung Tae, Ellen Zhang, Arman Cohan, and Dragomir Radev. Enhancing text-to-sql capabilities of large language models: A study on prompt design strategies. In EMNLP , pages 14935-14956, 2023.                               |\n",
    "| [98]   | Long Ouyang, Jeffrey Wu, Xu Jiang, and et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730-27744, 2022.                                                                                      |\n",
    "| [99]   | Fatma Ozcan, Abdul Quamar, Jaydeep Sen, Chuan Lei, and Vasilis Efthymiou. State of the art and open challenges in natural language interfaces to data. In SIGMOD , pages 2629-2636, 2020.                                                                                         |\n",
    "| [100]  | Parth Parikh, Oishik Chatterjee, Muskan Jain, Aman Harsh, Gaurav Shahani, Rathin Biswas, and Kavi Arya. Auto-query - A simple natural language to SQL query generator for an e-learning platform. In EDUCON , pages 936-940, 2022.                                                |\n",
    "| [101]  | Bijan Parsia. Querying the web with SPARQL. In Reasoning Web , volume 4126 of Lecture Notes in Computer Science , pages 53-67, 2006.                                                                                                                                              |\n",
    "| [102]  | Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In ACL , pages 1470-1480, 2015.                                                                                                                                                       |\n",
    "| [103]  | Rodolfo A. Pazos, Jos´ e A. Mart´nez ı F., Juan Javier Gonz´lez a Barbosa, and Andr´s e A. Ver´stegui a O. Al- gorithm for processing queries that involve boolean columns for a natural language interface to databases. Computaci´n o y Sistemas , 24(1), 2020.                 |\n",
    "| [104]  | Rodolfo A. Pazos, Jos´ e A. Mart´nez ı F., and Alan G. Aguirre L. Processing natural language queries via a natural language interface to databases with design anomalies. Polibits , 62:43-50, 2020.                                                                             |\n",
    "| [105]  | Hoifung Poon and Pedro M. Domingos. Unsupervised semantic parsing. In EMNLP , pages 1-10, 2009.                                                                                                                                                                                   |\n",
    "| [106]  | Ana-Maria Popescu, Alex Armanasu, Oren Etzioni, David Ko, and Alexander Yates. Modern natural lan- guage interfaces to databases: Composing statistical parsing with semantic tractability. In COLING , 2004.                                                                     |\n",
    "| [107]  | Ana-Maria Popescu, Oren Etzioni, and Henry A. Kautz. Towards a theory of natural language interfaces to databases. In IUI , pages 149-157, 2003.                                                                                                                                  |\n",
    "| [108]  | Patti J. Price. Evaluation of spoken language systems: the ATIS domain. In Speech and Natural Language ,                                                                                                                                                                          |\n",
    "\n",
    "| [109]   | Qinjun Qiu, Zhong Xie, Kai Ma, Liufeng Tao, and Shiyu Zheng. Neurospe: A neuro-net spatial relation extractor for natural language text fusing gazetteers and pretrained models. Trans. GIS , 27(5):1526-1549, 2023.                                                                                                                                                                                    |\n",
    "|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| [110]   | Xipeng Qiu, Tianxiang Sun, Yige Xu, and et al. Pre-trained models for natural language processing: A survey. Sci. China Technol. Sci. , 63(10):1872-1897, 2020.                                                                                                                                                                                                                                         |\n",
    "| [111]   | Colin Raffel, Noam Shazeer, Adam Roberts, and et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. , 21:140:1-140:67, 2020.                                                                                                                                                                                                                   |\n",
    "| [112]   | Ohad Rubin and Jonathan Berant. Smbop: Semi-autoregressive bottom-up semantic parsing. In NAACL- HLT , pages 311-324, 2021.                                                                                                                                                                                                                                                                             |\n",
    "| [113]   | Diptikalyan Saha, Avrilia Floratou, Karthik Sankaranarayanan, Umar Farooq Minhas, Ashish R. Mittal, and Fatma ¨ Ozcan. ATHENA: an ontology-driven system for natural language querying over relational data stores. Proc. VLDB Endow. , 9(12):1209-1220, 2016.                                                                                                                                          |\n",
    "| [114]   | Xavier Schmitt, Sylvain Kubler, J´r´my e e Robert, Mike Papadakis, and Yves Le Traon. A replicable com- parison study of NER software: Stanfordnlp, nltk, opennlp, spacy, gate. In SNAMS , pages 338-343, 2019.                                                                                                                                                                                         |\n",
    "| [115]   | Jaydeep Sen, Chuan Lei, Abdul Quamar, Fatma ¨ Ozcan, Vasilis Efthymiou, Ayushi Dalmia, Greg Stager, Ashish R. Mittal, Diptikalyan Saha, and Karthik Sankaranarayanan. ATHENA++: natural language query- ing for complex nested SQL queries. Proc. VLDB Endow. , 13(11):2747-2759, 2020.                                                                                                                 |\n",
    "| [116]   | Jaydeep Sen, Fatma Ozcan, Abdul Quamar, Greg Stager, Ashish R. Mittal, Manasa Jammi, Chuan Lei, Dip- tikalyan Saha, and Karthik Sankaranarayanan. Natural language querying of complex business intelligence queries. In SIGMOD , pages 1997-2000, 2019.                                                                                                                                                |\n",
    "| [117]   | Vraj Shah. Speakql: Towards speech-driven multimodal querying. In SIGMOD , pages 1847-1849, 2019.                                                                                                                                                                                                                                                                                                       |\n",
    "| [118]   | Vraj Shah, Side Li, Arun Kumar, and Lawrence K. Saul. Speakql: Towards speech-driven multimodal querying of structured data. In SIGMOD , pages 2363-2374, 2020.                                                                                                                                                                                                                                         |\n",
    "| [119]   | Vraj Shah, Side Li, Kevin Yang, Arun Kumar, and Lawrence K. Saul. Demonstration of speakql: Speech- driven multimodal querying of structured data. In SIGMOD , pages 2001-2004, 2019.                                                                                                                                                                                                                   |\n",
    "| [120]   | Grigori Sidorov, Rodolfo A. Pazos Rangel, Jos´ e A. Mart´nez ı F., Juan Mart´n ı Carpio, and Alan G. Aguirre L. Configuration module for treating design anomalies in databases for a natural language interface to databases. In Intuitionistic and Type-2 Fuzzy Logic Enhancements in Neural and Optimization Algorithms , volume 862 of Studies in Computational Intelligence , pages 703-714. 2020. |\n",
    "| [121]   | Dezhao Song, Frank Schilder, Charese Smiley, Chris Brew, Tom Zielund, Hiroko Bretz, Robert Martin, Chris Dale, John Duprey, Tim Miller, and Johanna Harrison. TR discover: A natural language interface for querying and analyzing interlinked datasets. In ISWC , pages 21-37, 2015.                                                                                                                   |\n",
    "| [122]   | Yuanfeng Song, Raymond Chi-Wing Wong, Xuefang Zhao, and Di Jiang. Speech-to-sql: Towards speech- driven SQL query generation from natural language question. CoRR , abs/2201.01209, 2022.                                                                                                                                                                                                               |\n",
    "| [123]   | Yuanfeng Song, Raymond Chi-Wing Wong, Xuefang Zhao, and Di Jiang. Voicequerysystem: A voice- driven database querying system using natural language questions. In SIGMOD , pages 2385-2388, 2022.                                                                                                                                                                                                       |\n",
    "| [124]   | Niculae Stratica, Leila Kosseim, and Bipin C. Desai. Using semantic templates for a natural language interface to the CINDI virtual library. Data Knowl. Eng. , 55(1):4-19, 2005.                                                                                                                                                                                                                       |\n",
    "| [125]   | Shuo Sun, Yuze Gao, Yuchen Zhang, Jian Su, Bin Chen, Yingzhan Lin, and Shuqi Sun. An exploratory study on model compression for text-to-sql. In ACL , pages 11647-11654, 2023.                                                                                                                                                                                                                          |\n",
    "| [126]   | Shuo Sun, Yuchen Zhang, Jiahuan Yan, Yuze Gao, Donovan Ong, Bin Chen, and Jian Su. Battle of the large language models: Dolly vs llama vs vicuna vs guanaco vs bard vs chatgpt - A text-to-sql parsing comparison. In EMNLP , pages 11225-11238, 2023.                                                                                                                                                  |\n",
    "| [127]   | Lappoon R. Tang and Raymond J. Mooney. Automated construction of database interfaces: Intergrating                                                                                                                                                                                                                                                                                                      |\n",
    "\n",
    "- [127] Lappoon R. Tang and Raymond J. Mooney. Automated construction of database interfaces: Intergrating statistical and relational learning for semantic parsing. In EMNLP , pages 133-141, 2000.\n",
    "\n",
    "| [128]   | Lappoon R. Tang and Raymond J. Mooney. Using multiple clause constructors in inductive logic program- ming for semantic parsing. In EMCL , volume 2167 of Lecture Notes in Computer Science , pages 466-477, 2001.                                                                     |\n",
    "|---------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| [129]   | Peihao Tong, Qifan Zhang, and Junjie Yao. Leveraging domain context for question answering over knowl- edge graph. Data Sci. Eng. , 4(4):323-335, 2019.                                                                                                                                |\n",
    "| [130]   | Immanuel Trummer. Database tuning using natural language processing. SIGMOD Rec. , 50(3):27-28, 2021.                                                                                                                                                                                  |\n",
    "| [131]   | Arif Usta, Akifhan Karakayali, and ¨ Ozg¨r u Ulusoy. Dbtagger: Multi-task learning for keyword mapping in nlidbs using bi-directional recurrent neural networks. Proc. VLDB Endow. , 14(5):813-821, 2021.                                                                              |\n",
    "| [132]   | Arif Usta, Akifhan Karakayali, and ¨ Ozg¨r u Ulusoy. xdbtagger: explainable natural language interface to databases using keyword mappings and schema graph. VLDB J. , 33(2):301-321, 2024.                                                                                            |\n",
    "| [133]   | Prasetya Utama, Nathaniel Weir, Fuat Basik, Carsten Binnig, Ugur Cetintemel, ¸ Benjamin H¨ttasch, a Amir Ilkhechi, Shekar Ramaswamy, and Arif Usta. Anend-to-end neural natural language interface for databases. CoRR , abs/1804.00401, 2018.                                         |\n",
    "| [134]   | Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS , pages 5998-6008, 2017.                                                                                              |\n",
    "| [135]   | Moses Visperas, Aunhel John Adoptante, Christalline Joie Borjal, Ma. Teresita Abia, Jasper Kyle Catapang, and Elmer C. Peramo. On modern text-to-sql semantic parsing methodologies for natural language interface to databases: A comparative study. In ICAIIC , pages 390-396, 2023. |\n",
    "| [136]   | Ngoc Phuoc An Vo, Octavian Popescu, Irene Manotas, and Vadim Sheinin. Tackling temporal questions in natural language interface to databases. In EMNLP , pages 179-187, 2022.                                                                                                          |\n",
    "| [137]   | Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. RAT-SQL: relation-aware schema encoding and linking for text-to-sql parsers. In ACL , pages 7567-7578, 2020.                                                                                       |\n",
    "| [138]   | Runze Wang, Zhen-Hua Ling, Jing-Bo Zhou, and Yu Hu. A multiple-integration encoder for multi-turn text-to-sql semantic parsing. IEEE ACM Trans. Audio Speech Lang. Process. , 29:1503-1513, 2021.                                                                                      |\n",
    "| [139]   | Weiguo Wang, Sourav S. Bhowmick, Hui Li, Shafiq R. Joty, Siyuan Liu, and Peng Chen. Towards en- hancing database education: Natural language generation meets query execution plans. In SIGMOD , pages 1933-1945, 2021.                                                                |\n",
    "| [140]   | Wenlu Wang. A cross-domain natural language interface to databases using adversarial text method. In VLDB , volume 2399 of CEUR Workshop Proceedings . CEUR-WS.org, 2019.                                                                                                              |\n",
    "| [141]   | Wenlu Wang, Jingjing Li, Wei-Shinn Ku, and Haixun Wang. Multilingual spatial domain natural language interface to databases. GeoInformatica , 28(1):29-52, 2024.                                                                                                                       |\n",
    "| [142]   | Wenlu Wang, Yingtao Tian, Haixun Wang, and Wei-Shinn Ku. A natural language interface for database: Achieving transfer-learnability using adversarial method for question understanding. In ICDE , pages 97- 108, 2020.                                                                |\n",
    "| [143]   | Xieyang Wang, Mengyi Liu, Jianqiu Xu, and Hua Lu. NALMO: transforming queries in natural language for moving objects databases. GeoInformatica , 27(3):427-460, 2023.                                                                                                                  |\n",
    "| [144]   | Xieyang Wang, Jianqiu Xu, and Hua Lu. NALMO: A natural language interface for moving objects databases. In SSTD , pages 1-11, 2021.                                                                                                                                                    |\n",
    "| [145]   | Xieyang Wang, Jianqiu Xu, and Yaxin Wang. NLMO: towards a natural language tool for querying moving objects. In MDM , pages 228-229, 2020.                                                                                                                                             |\n",
    "| [146]   | Yushi Wang, Jonathan Berant, and Percy Liang. Building a semantic parser overnight. In ACL , pages 1332-1342, 2015.                                                                                                                                                                    |\n",
    "| [147]   | Ziyun Wei, Immanuel Trummer, and Connor Anderson. Demonstrating robust voice querying with MUVE: optimally visualizing results of phonetically similar queries. In SIGMOD , pages 2798-2802, 2021.                                                                                     |\n",
    "\n",
    "| [148]   | Ziyun Wei, Immanuel Trummer, and Connor Anderson. Robust voice querying with MUVE: optimally visualizing results of phonetically similar queries. Proc. VLDB Endow. , 14(11):2397-2409, 2021.                                                                                                                   |\n",
    "|---------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| [149]   | Nathaniel Weir and Prasetya Utama. Bootstrapping an end-to-end natural language interface for databases. In SIGMOD , pages 1862-1864, 2019.                                                                                                                                                                     |\n",
    "| [150]   | Yuk Wah Wong and Raymond J. Mooney. Learning for semantic parsing with statistical machine trans- lation. In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics , 2006.                                                                         |\n",
    "| [151]   | Chunyang Xiao, Marc Dymetman, and Claire Gardent. Sequence-based structured prediction for semantic parsing. In ACL , 2016.                                                                                                                                                                                     |\n",
    "| [152]   | Xiaojun Xu, Chang Liu, and Dawn Song. Sqlnet: Generating structured queries from natural language without reinforcement learning. CoRR , abs/1711.04436, 2017.                                                                                                                                                  |\n",
    "| [153]   | Navid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and Thomas Dillig. Sqlizer: query synthesis from natural language. Proc. ACM Program. Lang. , 1(OOPSLA):63:1-63:26, 2017.                                                                                                                                         |\n",
    "| [154]   | Yuquan Yang, Qifan Zhang, and Junjie Yao. Task-driven neural natural language interface to database. In WISE , volume 14306 of Lecture Notes in Computer Science , pages 659-673, 2023.                                                                                                                         |\n",
    "| [155]   | Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. In NeurIPS , pages 5754-5764, 2019.                                                                                                   |\n",
    "| [156]   | Ziyu Yao, Yu Su, Huan Sun, and Wen-tau Yih. Model-based interactive semantic parsing: A unified framework and A text-to-sql case study. In EMNLP-IJCNLP , pages 5446-5457, 2019.                                                                                                                                |\n",
    "| [157]   | Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code generation. In ACL , pages 440-450, 2017.                                                                                                                                                                                    |\n",
    "| [158]   | Tao Yu, Zifan Li, Zilin Zhang, Rui Zhang, and Dragomir R. Radev. Typesql: Knowledge-based type-aware neural text-to-sql generation. In NAACL-HLT , pages 588-594, 2018.                                                                                                                                         |\n",
    "| [159]   | Tao Yu, Michihiro Yasunaga, Kai Yang, Rui Zhang, Dongxu Wang, Zifan Li, and Dragomir R. Radev. Syntaxsqlnet: Syntax tree networks for complex and cross-domain text-to-sql task. In EMNLP , pages 1653-1663, 2018.                                                                                              |\n",
    "| [160]   | Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir R. Radev. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In EMNLP , pages 3911-3921, 2018. |\n",
    "| [161]   | John M. Zelle and Raymond J. Mooney. Learning to parse database queries using inductive logic program- ming. In AAAI IAAI , pages 1050-1055, 1996.                                                                                                                                                              |\n",
    "| [162]   | Gideon Zenz, Xuan Zhou, Enrico Minack, Wolf Siberski, and Wolfgang Nejdl. From keywords to semantic queries - incremental query construction on the semantic web. J. Web Semant. , 7(3):166-176, 2009.                                                                                                          |\n",
    "| [163]   | Luke S. Zettlemoyer and Michael Collins. Learning to map sentences to logical form: Structured classifi- cation with probabilistic categorial grammars. In UAI , pages 658-666, 2005.                                                                                                                           |\n",
    "| [164]   | Jinchuan Zhang, Yan Zhou, Binyuan Hui, Yaxin Liu, Ziming Li, and Songlin Hu. Trojansql: SQL injection against natural language interface to database. In EMNLP , pages 4344-4359, 2023.                                                                                                                         |\n",
    "| [165]   | Bolong Zheng, Lei Bi, Juan Cao, Hua Chai, Jun Fang, Lu Chen, Yunjun Gao, Xiaofang Zhou, and Chris- tian S. Jensen. Speaknav: Voice-based route description language understanding for template driven path search. Proc. VLDB Endow. , 14(12):3056-3068, 2021.                                                  |\n",
    "| [166]   | Weiguo Zheng, Hong Cheng, Lei Zou, Jeffrey Xu Yu, and Kangfei Zhao. Natural language ques- tion/answering: Let users talk with the knowledge graph. In ACM CIKM , pages 217-226, 2017.                                                                                                                          |\n",
    "| [167]   | Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. CoRR , abs/1709.00103, 2017.                                                                                                                                        |\n",
    "| [168]   | Lei Zou, Ruizhe Huang, Haixun Wang, Jeffrey Xu Yu, Wenqiang He, and Dongyan Zhao. Natural language question answering over RDF: a graph data driven approach. In SIGMOD , pages 313-324, 2014.                                                                                                                  |\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae78c365-5f89-4d7c-a433-0d30de36665a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## NLI4DB：数据库的自然语言接口系统综述\n",
      "\n",
      "## 刘梦一 和 徐建秋 *\n",
      "\n",
      "南京航空航天大学，中国南京 {liumengyi, jianqiu}@nuaa.edu.cn\n",
      "\n",
      "## 摘要\n",
      "\n",
      "随着在生活各个领域查询数据库需求的不断增长，研究人员对数据库的自然语言接口（NLIDB）给予了极大的关注。本文对最近提出的NLIDBs进行了全面调查。我们首先简要介绍了自然语言处理技术、可执行数据库语言和自然语言与可执行语言之间的中间表示，然后提供了从自然语言到可执行数据库语言的翻译过程概述。翻译过程分为三个阶段：(i) 自然语言预处理，(ii) 自然语言理解，以及 (iii) 自然语言翻译。传统方法和数据驱动方法被用于预处理阶段。传统方法依赖于预定义规则和语法，涉及正则表达式、依存分析和命名实体识别等技术。数据驱动方法依赖于大规模数据和机器学习模型，使用词嵌入和模式链接等技术。自然语言理解方法分为三类：(i) 基于规则，(ii) 基于机器学习，和 (iii) 混合方法。随后，我们描述了针对关系和时空数据库构建可执行语言的一般过程。接下来，介绍了将自然语言转换为可执行语言的常见基准和评估指标，并探讨了生成新基准的方法。最后，我们总结了NLIDB系统的分类、发展和改进，并讨论了与NLIDB相关的深度语言理解和数据库交互技术，包括 (i) 使用大语言模型进行Text2SQL任务，(ii) 从SQL生成自然语言解释，以及 (iii) 将语音查询转换为SQL。\n",
      "\n",
      "关键词：数据库的自然语言接口，语义解析，结构化语言，查询处理\n",
      "\n",
      "## 1 引言\n",
      "\n",
      "在当今数据驱动的世界中，数据库是许多应用的核心，从社交媒体平台到金融系统。然而，访问和查询这些庞大的信息库通常需要专门的查询语言知识，如SQL，这对非专业用户来说是一个重大障碍，限制了他们充分利用手头数据的能力。自然语言接口（NLI）的出现有可能消除用户与终端之间的互动障碍 [130]。自然语言处理（NLP）与数据库技术的结合代表了一条未来研究的有趣途径。有些系统促进了自然语言向结构化语言的转换 [141, 22]，提供查询执行计划的自然语言描述 [139, 23]，并将SQL转换为自然语言 [40, 132]。\n",
      "\n",
      "想象一个世界，无论技术熟练程度如何，任何人都可以轻松地用日常语言与复杂数据库互动。这一愿景通过开发数据库自然语言接口（NLIDB）正在成为现实，NLIDB旨在将自然语言查询（NLQ）转换为可执行语言，如图1所示。用户倾向于喜欢一种允许他们确认生成的结构化语言的准确性和精确性的交互界面 [95]。NLIDB使用户能够避免掌握结构化查询语言和数据库模式的必要性，从而显著简化用户的努力并增强利用数据库的好处 [70]。最初的NLIDBs，包括BASEBALL、LUNAR、LADDER、Chat-80和ASK，相继发布 [2]。之后，NLIDBs主要应用于关系数据库（例如GENSQL [44] 和CatSQL [48]）、空间领域（例如SpatialNLI [80, 141] 和NALSpatial [89]）、RDF问答（例如Querix [69] 和TEQUILA [64]）和XML数据库（例如NaLIX [84, 85] 和DaNaLIX [81]）。\n",
      "\n",
      "尽管经过多年的研发，NLIDB领域仍充满挑战 [9, 82]。自然语言固有的歧义和变异性使得NLIDB难以确保准确的查询解释。此外，理解不同数据库的结构和语义增加了另一层复杂性。此外，在保持高精度的同时实现实时性能仍然是一个持续的挑战。虽然大型语言模型（LLMs）为使用自然语言查询数据库提供了新的途径，但训练和推理这些模型需要大量的计算资源，这在资源有限的情况下可能难以实施 [97]。此外，LLMs的决策过程通常是不透明且缺乏可解释性的，这使得很难确定生成的查询结果是否符合用户的意图 [126]。这些问题强调了继续研究和开发以完善NLIDB的必要性。\n",
      "\n",
      "鉴于这些观察，本系统综述探讨了当前NLIDB的状态，考察了已提出的各种连接自然语言与数据库查询的方法，命名为NLI4DB。本调查的目的是提供一个全面的概述，作为研究人员的宝贵参考和实践者实施有效NLIDB解决方案的实际指南。NLI4DB对NLIDB主题进行了深入检查，将其工作分类为子主题，并对每个子主题进行了深入分析。从自然语言到可执行语言的翻译过程分为三个阶段：(i) 自然语言预处理，(ii) 自然语言理解，和 (iii) 自然语言翻译。这三个阶段的划分提供了物理独立性，通过将数据的物理排列与查询的语义分离 [113]。翻译技术如图2所示。\n",
      "\n",
      "(i) 自然语言预处理通常涉及为特定领域构建专用的数据字典，使用词干提取和同义词技术。然后对输入的自然语言进行词性标注和分词。预处理阶段使用的技术包括传统方法和数据驱动方法。传统方法依赖于特定领域的文本处理所使用的预定义规则和语法，涉及正则表达式、依存分析和命名实体识别（NER）等技术。数据驱动方法依赖于大规模数据和机器学习模型，用于复杂或可变文本的处理，使用词嵌入和模式链接等技术。\n",
      "\n",
      "(ii) 自然语言理解方法分为三类：(i) 基于规则，(ii) 基于机器学习，和 (iii) 混合方法。基于规则的系统只能处理特定领域的知识库，其语义理解过程要么定义语义可达性的概念，要么将NLQ转换为可以描述语义和关系的中间表示。基于机器学习的系统使用各种技术来解析文本，包括无监督方法、问答监督学习、统计机器翻译技术、带循环神经网络的编码器-解码器框架以及确定性算法和机器学习的组合。混合方法结合规则和机器学习技术以最大化其优势。\n",
      "\n",
      "(iii) 自然语言翻译使用独特的算法将处理后的关键语义信息映射到相应的结构化语言组件。为了构建SQL，匹配NLQ的数据库元素被放置在SELECT、FROM和WHERE部分的适当位置。如果查询涉及多个关系，则需要在WHERE和FROM子句中分别包含连接条件和参与关系的名称。在构建时空数据库的可执行语言过程中，首先识别输入NLQ的查询类型。然后根据查询类型确定构建可执行语言所需的运算符。最后，将自然语言理解阶段获得的关键语义信息整合形成可执行语言。\n",
      "\n",
      "与NLIDB相关的现有综述[2]主要集中在整个自然语言接口系统的比较分析上。Affolter等人[2]将NLIs分为四组：(i) 关键词基础系统，(ii) 模式基础系统，(iii) 解析基础系统，和 (iv) 语法基础系统。对于每组，他们提供了一个代表性系统的概述，并详细描述了最具说明力的一个。此外，他们在论文设计的样本世界基础上系统地比较了24个最近开发的NLIDBs。每个系统都使用10个示例问题进行评估，以展示其优缺点。\n",
      "\n",
      "与Affolter等人[2]相比，我们将系统翻译过程分为三个步骤，并专注于每个步骤的比较分析。我们调查了最近开发的NLIDBs，并将翻译过程分为三个阶段：(i) 自然语言预处理，(ii) 自然语言理解，和 (iii) 自然语言翻译。我们将自然语言预处理技术分为传统方法和数据驱动方法。然后分析自然语言理解方法的三个类别：(i) 基于规则，(ii) 基于机器学习，和 (iii) 混合方法。接下来，我们提供了一个针对关系和时空数据库构建可执行语言的一般过程概述。最后，我们介绍了NLIDBs的分类、发展和改进，并讨论了与NLIDB相关的深度语言理解和数据库交互技术，包括 (i) 使用LLM进行Text2SQL任务，(ii) 从SQL生成自然语言解释，和 (iii) 将语音查询转换为SQL。\n",
      "\n",
      "表1：常用符号\n",
      "\n",
      "| 名称                                  | 缩写   |\n",
      "|---------------------------------------|--------|\n",
      "| 数据库的自然语言接口                | NLIDB  |\n",
      "| 自然语言接口                        | NLI    |\n",
      "| 自然语言查询                        | NLQ    |\n",
      "| 自然语言处理                        | NLP    |\n",
      "| 命名实体识别                        | NER    |\n",
      "| 大型语言模型                        | LLM    |\n",
      "| 一阶逻辑                            | FOL    |\n",
      "| 自动语音识别                        | ASR    |\n",
      "| 序列到序列                          | seq2seq|\n",
      "\n",
      "其余内容结构如下。第2节提供了关于NLIDB的背景知识，包括自然语言处理技术、可执行数据库语言和中间表示语言。第3节描述了可执行数据库语言生成的三个阶段：(i) 自然语言预处理，(ii) 自然语言理解和 (iii) 自然语言翻译。第4节总结了11个流行的将NLQ转换为SQL的基准和3个评估指标，包括响应时间、可翻译性和翻译精度，并探讨了生成新基准的方法。第5节分析了NLIDBs的分类、发展和改进。第6节讨论了与NLIDB相关的深度语言理解和数据库交互技术，包括 (i) 使用LLM进行Text2SQL任务，(ii) 从SQL生成自然语言解释，和 (iii) 将语音查询转换为SQL。第7节探讨了NLIDB的开放问题并总结了调查。表1总结了常用的符号。\n",
      "\n",
      "## 2 背景：NLP技术和查询语言\n",
      "\n",
      "我们介绍与NLIDB相关的背景知识，包括自然语言处理技术、可执行数据库语言和中间表示语言。\n",
      "\n",
      "## 2.1 自然语言处理技术\n",
      "\n",
      "NLP是一门跨学科学科，整合了语言学、计算机科学和数学等多个领域，目标是使计算机能够理解、处理和生成自然语言文本或语音。通过分段、词汇注释和句法分析，NLP提供了对文本的结构化处理，以实现语义理解和信息提取。NLP的应用领域涵盖了机器翻译、情感分析、信息检索和对话系统，为人们提供了智能和便捷的语言交互方式。\n",
      "\n",
      "简要历史。最早的自然语言处理研究是机器翻译。1950年，艾伦·图灵提出了最终测试来判断真正“智能”机器的到来，这通常被视为NLP思想的开端 [96]。从1950年代到1970年代，规则方法被用来处理自然语言，基于语法规则和形式逻辑。1970年代，统计方法逐渐取代了规则方法。在此阶段，NLP基于数学模型和统计取得了实质性突破，并应用于实际应用。从2008年至今，研究人员引入了深度学习到NLP中，以应对图像识别和语音识别方面的成就。\n",
      "\n",
      "NLIDBs常用的NLP技术如下。\n",
      "\n",
      "- (i) 词性标注是指为分割文本中的每个词分配正确的词性标签，确定每个词是名词、动词还是形容词。在NLIDB中，词性标注有助于识别自然语言查询中单个词的语法角色，从而准确理解用户意图。例如，对于自然语言查询 “2000年至2010年间布拉德·皮特主演的所有电影”，使用Stanford CoreNLP进行词性标注的结果如图3(a)所示。在图中，DT = 冠词；NNS = 复数名词；VBG = 动词的现在分词或动名词；NNP = 单数专有名词；IN = 介词或连词；CD = 基数词。\n",
      "\n",
      "- (ii) 词形还原是将单词的不同形式还原为其原始形式的过程。在NLIDB中，词形还原有助于统一自然语言查询中的各种时态和形态的单词为基本形式，以便与数据库内容匹配。例如，对于自然语言查询 “2000年至2010年间布拉德·皮特主演的所有电影”，使用Stanford CoreNLP进行词形还原的结果如图3(b)所示。\n",
      "- (iii) 命名实体识别是在自然语言文本中识别具有特定含义的实体的过程 [114]。通常，识别的实体可以分为三大类（实体、时间、数字）和七个子类（PERSON、ORGANIZATION、LOCATION、TIME、DATE、MONEY和PERCENT）。NLIDB中的NER使识别自然语言查询中涉及的实体成为可能，以定位查询的主题和范围。例如，对于自然语言查询 “2000年至2010年间布拉德·皮特主演的所有电影”，使用Stanford CoreNLP进行NER的结果如图3(c)所示。\n",
      "- (iv) 依存分析涉及分析自然语言句子中单词之间的依存关系。单词之间的二元不对称关系称为依存关系，描述为从主语（被修饰的对象）到修饰词的箭头。NLIDB中的依存分析有助于理解NLQ中单词之间的语法关系，从而使查询的结构和意义得到准确理解。例如，对于自然语言查询 “2000年至2010年间布拉德·皮特主演的所有电影”，使用Stanford CoreNLP进行依存分析的结果如图3(d)所示。在图中，punct = 标点符号；obl = 斜向名词；obj = 宾语；det = 冠词；acl = 名词的从句修饰；case = 格标记。\n",
      "\n",
      "随着NLP技术的快速发展，出现了许多NLP工具 [114]。这些工具可以执行基本任务，包括依存分析、命名实体识别、词形还原和词性标注，每种都有其独特的优势和劣势。以下是建立的开源自然语言处理工具列表。\n",
      "\n",
      "- (i) NLTK 是一个使用Python作为编程语言的自然语言处理工具包。NLTK功能齐全，实现了自然语言处理中的许多功能组件，如命名实体识别、句法分析、词性标注和文本分类 [13]。NLTK诞生于学术领域，适合学习和研究。缺点是NLTK的处理速度比其他工具慢。\n",
      "- (ii) spaCy 是一种商业开源软件，是一种使用Python和Cython语言编写的工业级自然语言处理软件 [45]。spaCy继NLTK之后，包括预训练的统计模型和词向量。spaCy可以将文本分解为文章、单词和标点符号等语义单元，并支持命名实体识别。spaCy的特点是快速而准确的语法分析，功能范围从简单的词性标注到高级的深度学习。\n",
      "- (iii) Stanford CoreNLP 是斯坦福大学使用Java编程语言开发的工具集。Stanford CoreNLP支持多种自然语言，并为无需Java的编程语言提供了丰富的接口 [91]。Stanford CoreNLP是由高水平研究机构创建的高效工具，广泛应用于科学研究和实验，但在生产系统中可能会产生额外成本。Stanford CoreNLP可能不是行业的最佳选择。\n",
      "- (iv) TextBlob 是NLTK的扩展，提供了一种更简单的方式来使用NLTK的功能 [57]。TextBlob支持情感分析、分词、词性标注和文本分类。其中一个优点是TextBlob可以在性能要求不太高的生产环境中使用。TextBlob可以在广泛的场景中应用，特别适用于小型项目。\n",
      "\n",
      "## 2.2 可执行数据库语言\n",
      "\n",
      "NLIDB的输出是一种可执行数据库语言，我们展示了关系数据、RDF数据和空间数据上的可执行语言。\n",
      "\n",
      "### 2.2.1 关系数据查询语言\n",
      "\n",
      "关系数据的标准可执行查询语言是SQL。这种语言是一种通用的、极其强大的关系数据库语言，其功能不仅限于查询，还包括创建数据库模式、插入和修改数据以及定义和控制数据库安全完整性 [29]。随着SQL成为国际标准语言，众多数据库制造商发布了兼容SQL的软件，包括数据库管理系统和接口。因此，SQL成为大多数数据库的通用数据访问语言和标准接口，促进了不同数据库系统之间的互操作性共享基础。SQL已成为数据库领域的主要语言，意义重大。\n",
      "\n",
      "SQL提供了用于查询数据的SELECT语句，其使用灵活且功能丰富。SELECT语句可以执行简单的单表查询，也可以执行复杂的连接查询和嵌套查询，其一般格式为：\n",
      "\n",
      "SELECT [ALL DISTINCT] | <目标列表达式> [别名] [, <目标列表达式> [别名]] FROM <表名或视图名> [, <表名或视图名>] | (SELECT语句) [AS] <别名> [ WHERE <条件表达式> ] [ASC DESC]]; [ GROUP BY <列名1> [ HAVING <条件表达式> ]] [ ORDER BY <列名2> |\n",
      "\n",
      "SELECT语句的目的是根据FROM子句中的基本表、视图或派生表，依据WHERE子句中的条件表达式找到满足条件的元组。然后根据SELECT子句中的目标列表达式选择元组中的属性值以形成结果表。当存在GROUP BY子句时，输出按照<列名1>的值组织，其中具有相同属性列值的元组被分组在一起。通常对每个组应用聚合函数。当GROUP BY子句伴随HAVING子句时，输出仅包括满足指定条件的组。如果存在ORDER BY子句，则根据<列名2>的值按升序或降序对结果表进行排序。\n",
      "\n",
      "### 2.2.2 RDF数据查询语言\n",
      "\n",
      "RDF的完整定义是资源描述框架，这是一个用于表示互联网资源信息的数据模型。该数据模型通常描述由三部分组成的事实，称为三元组，包括 (i) 主体 ，(ii) 谓词 ，和 (iii) 宾语 。RDF图包含多个三元组。RDF文档使用XML编写，以提供标准化的方法来描述信息。RDF旨在供计算机应用程序读取和理解，而不是用于向网络用户进行视觉展示。\n",
      "\n",
      "SPARQL是一种专为RDF设计的查询语言和数据检索协议，代表SPARQL Protocol and RDF Query Language [24]。SPARQL是一种针对RDF图的查询语言，其中数据库被表示为一组“主体-谓词-宾语”三元组。尽管RDF数据具有推理性，SPARQL并不具备推理性查询功能。SPARQL专为管理存储在RDF格式中的数据而设计，既能检索也能操作。SPARQL由以下组件组成。\n",
      "\n",
      "- · PREFIX子句用于声明前缀，以简化URI的使用。前缀声明是可选的。\n",
      "- · SELECT子句用于指定查询返回的变量。\n",
      "- · WHERE子句用于匹配RDF图中的数据。该子句包含一个或多个三元组模式，用于指示查询的条件。\n",
      "- · FILTER子句设计用于有条件地过滤查询结果。该子句可以包含布尔表达式以限制WHERE子句匹配的结果集。\n",
      "\n",
      "SPARQL的基本查询类型如下 [101]。\n",
      "\n",
      "SELECT查询是最常用的查询类型，其功能是选择变量并返回结果集。通常会生成一个表格作为SELECT查询的结果，其中包含满足查询标准的变量及其对应值。\n",
      "\n",
      "CONSTRUCT查询用于利用查询模式生成新的RDF图。与表格结果不同，CONSTRUCT查询生成的RDF图是从查询模式匹配的数据构造而成的。\n",
      "\n",
      "ASK查询旨在确定是否存在满足查询模式的RDF数据。ASK查询以布尔值（true或false）的形式提供响应，以指示是否存在匹配项。\n",
      "\n",
      "DESCRIBE查询用于获取资源的详细描述。描述由查询引擎决定，通常包括直接与资源相关的三元组。\n",
      "\n",
      "每种查询类型都使用WHERE子句来限制查询范围。然而，在DESCRIBE查询的上下文中，包含WHERE子句并不是强制性的。例如，以下查询从数据集中检索年龄超过24岁的人：\n",
      "\n",
      "PREFIX info: <http://somewhere/peopleInfo#> SELECT ?resource WHERE { ?resource info:age ?age . FILTER (?age >= 24)\n",
      "\n",
      "### 2.2.3 空间数据查询语言\n",
      "\n",
      "人们对地理信息系统在生产和生活中各方面的日益依赖，导致了各行业对空间数据查询需求的显著增加。空间应用的普及引起了对空间数据库的高度关注 [55]。数据库中用于表示和操作空间对象的基本数据类型包括点、线和区域。常见的空间数据查询操作符如表2所示。\n",
      "\n",
      "成熟的空间数据存储和管理系统包括Esri的ArcGIS、PostGIS、Google Earth Engine、GRASS GIS和SECONDO [56]。例如，SECONDO是一个免费可用的平台，旨在组织和检查空间和时间数据。SECONDO的基本命令如下。\n",
      "\n",
      "query <值表达式>. 该命令评估给定的值表达式，然后将结果显示给用户。\n",
      "\n",
      "let <标识符> = <值表达式>. 该命令首先以类似于前面命令的方式评估提供的值表达式。与前面的命令不同的是，评估结果不会立即显示，而是存储在一个名为标识符的对象中。如果对象已经在数据库中存在，则该命令将导致错误。\n",
      "\n",
      "delete <标识符>. 该命令从当前数据库中删除名为标识符的对象，通常与第二个命令一起使用。\n",
      "\n",
      "当专家或系统开发人员为SECONDO编写可执行查询语言时，需要全面了解数据流和操作符之间的复杂关系。rel2stream操作符将关系转换为元组流，如图4所示。相反，stream2rel操作符将元组流转换为关系。在SECONDO的基本操作符中，filter操作符是最常使用的。类似于SQL中的SELECT关键字，filter操作符的功能是从满足特定条件的数据中提取信息。SELECT关键字在二维表结构上操作，通过指定列和条件来查询、过滤和投影数据。filter操作符作用于元组流，后跟一个过滤条件。符合条件的元组被收集并输出为一个流。例如，以下可执行语言将在SECONDO的关系city中输出所有有关南京的信息。\n",
      "\n",
      "query city rel2stream filter [.Name = 'Nanjing'] stream2rel;\n",
      "\n",
      "在查询执行过程中，rel2stream操作符首先将关系city转换为元组流，然后filter操作符从流中提取名为 'Nanjing' 的元组，最后stream2rel操作符将元组从流转换为关系。\n",
      "\n",
      "### 2.3 中间表示语言\n",
      "\n",
      "NLIDB中的中间表示语言旨在适应自然语言和可执行数据库语言之间的语义差异和多样性，从而提高系统的翻译准确性、灵活性和可维护性 [7]。中间表示充当自然语言和可执行语言之间的翻译器，将复杂的自然语言结构映射到统一的语义表示，以便于后续有效的查询处理和执行。通过将NLQ与底层数据库查询语言解耦，中间表示语言使NLIDB系统变得灵活、便携，并能适应各种数据库类型和查询需求。中间表示的设计考虑了以下几个因素：\n",
      "\n",
      "- (i) 中间表示应传达用户希望提交给数据库的查询请求，而不是用户输入的完整意义。\n",
      "- (ii) 为了便于后续翻译成数据库的可执行语言，中间表示应是无歧义的。\n",
      "- (iii) 为了方便重新开发，中间表示应该是可重用的。\n",
      "\n",
      "流行的中间表示有解析树 [78]、一阶逻辑 [121]、OQL [113]、查询草图 [153]、SemQL [53] 和 NatSQL [50]。\n",
      "\n",
      "解析树。自然语言查询的句法结构与解析树的设计密切相关。树结构通常用于表示查询的层次和结构性关系。解析树中的每个节点表示一个语法单位（例如短语、词组和词汇），边表示这些语法单位之间的语法关系（例如修饰和连接）。可以在解析树的节点和边上标记语义信息，以识别查询中存在的语义角色和约束，为后续查询处理提供重要信息。\n",
      "\n",
      "一阶逻辑。将NLQ转换为一阶逻辑（FOL）时，首先将自然语言中的单词和短语映射到FOL中的谓词、常量、变量和逻辑连接词，以表示查询中的实体、属性和关系。然后，基于NLQ的句法结构，构建FOL表示的句法树或句法图，以捕捉查询中的语义关系和逻辑结构。最后，识别查询中的主题、条件和操作，并将其转换为FOL中的逻辑表达式，以表示查询的约束和操作要求。\n",
      "\n",
      "当将自然语言查询 “查找所有年龄大于30岁的员工的名字和薪水。” 转换为一阶逻辑表示时，定义谓词和常量如下。\n",
      "\n",
      "Employee x ( ) : x 是一名员工\n",
      "\n",
      "Name x,n ( ) : 员工 x 的名字是 n\n",
      "\n",
      "Age x, a ( ) : 员工 x 的年龄是 a\n",
      "\n",
      "Salary x, s ( ) : 员工 x 的薪水是 s\n",
      "\n",
      "查询条件表示为 ∀ x Employee x ( ( ) ∧ Age x, a ( ) ∧ a > 30) 。查询结果表示为 ∃ n, s ( Name x,n ( ) ∧ Salary x, s ( )) 。通过结合查询的条件和结果，获得完整的FOL表示。\n",
      "\n",
      "OQL基于本体知识图，其中自然语言查询中的单词和短语与本体知识图中的概念、属性和关系相关联。通过语义表示和查询模式捕获自然语言查询的语义信息，以有效地与数据库交互。OQL语法允许表达复杂的聚合、联合和嵌套查询。OQL查询在查询的FROM子句中为每个概念分配别名。\n",
      "\n",
      "查询草图是一种带有自然语言提示的SQL形式。以NLQ “查找OOPSLA 2010的论文数量。” 为例，查询草图如下。\n",
      "\n",
      "SELECT count(?[papers]) FROM ??[papers] WHERE ? = 'OOPSLA 2010';\n",
      "\n",
      "在查询草图中，符号 '??' 和 '?' 分别表示未指定的表和未指定的列。方括号内的单词表示相应空缺处的提示。例如，草图中的第一个提示建议符号 '?' 具有与术语“papers”相似的语义意义。\n",
      "\n",
      "SemQL 设计为一种树结构，不仅在合成过程中约束搜索空间，还保持与SQL相同的结构特征。在SemQL查询中，移除了SQL中的GROUP BY、HAVING和FROM子句，并且WHERE和HAVING子句中的条件始终在Filter子树中表示。此外，在后续推理阶段，利用领域知识从SemQL查询中确定性地推断实现细节。例如，SQL中GROUP BY子句中包含的列通常也出现在SELECT子句中。\n",
      "\n",
      "NatSQL保留了SQL的核心功能，同时简化了SQL的结构以更接近自然语言的语法。NatSQL仅保留SELECT、WHERE和FROM子句，省略JOIN ON、HAVING和GROUP BY子句。此外，NatSQL不需要嵌套子查询或聚合运算符，采用单一的SELECT子句。对于自然语言查询“库存中有超过5位演员且少于3部影片的影片是哪一部？”，对应的SQL和NatSQL如下。\n",
      "\n",
      "SQL: SELECT T1.title FROM film AS T1 JOIN film_actor AS T2 ON T1.film_id = T2.film_id GROUP BY T1.film_id HAVING count(*) > 5 INTERSECT SELECT T1.title FROM film AS T1 JOIN inventory AS T2 ON T1.film_id = T2.film_id GROUP BY T1.film_id HAVING count(*) < 3;\n",
      "\n",
      "NatSQL: SELECT film.title WHERE count(film_actor.*) > 5 and count(inventory.*) < 3;\n",
      "\n",
      "表3：NLIDBs的自然语言预处理\n",
      "\n",
      "| NLIDB            | 年份 | 底层数据类型 | 分词 | 词性标注 | NER | 字典生成 | 正则表达式 | 依存分析 | 词嵌入 | 模式链接 |\n",
      "|------------------|------|--------------|------|----------|-----|----------|------------|----------|---------|----------|\n",
      "| PRECISE [107]   | 2003 | 关系数据     | ✓    | ✓        | ✓   | ✓        |            |          |         |          |\n",
      "| Querix [69]     | 2006 | 本体         | ✓    | ✓        |     | ✓        |            |          |         |          |\n",
      "| QuestIO [27]    | 2008 | 本体         | ✓    | ✓        |     | ✓        |            |          |         |          |\n",
      "| gAnswer [60]    | 2013 | RDF数据      |      |          |     | ✓        |            |          |         |          |\n",
      "| MEANS [1]       | 2015 | RDF数据      | ✓    |          | ✓   |          |            |          |         |          |\n",
      "| NL2CM [6, 5]    | 2015 | RDF数据      | ✓    | ✓        |     |          |            | ✓         |         |         |\n",
      "| NL2TRANQUYL [16]| 2015 | 关系数据     |      |          |     |          |            | ✓         |         |         |\n",
      "| ATHENA [113]    | 2016 | 关系数据     | ✓    | ✓        | ✓   |          |            | ✓         |         |         |\n",
      "| SQLizer [153]   | 2017 | 关系数据     | ✓    | ✓        | ✓   |          |            |           |         |         |\n",
      "| TEQUILA [64]    | 2018 | RDF数据      | ✓    | ✓        | ✓   |          |            |           |         |         |\n",
      "| MyNLIDB [28]    | 2019 | 关系数据     | ✓    | ✓        |     |          |            | ✓         |         |         |\n",
      "| IRNet [53]      | 2019 | 关系数据     |      |          |     |          |            |           | ✓       | ✓        |\n",
      "| NLMO [145]      | 2020 | 移动物体     | ✓    | ✓        | ✓   | ✓        | ✓          |           |         |         |\n",
      "| NALMO [144, 143]| 2021 | 移动物体     | ✓    | ✓        | ✓   | ✓        | ✓          |           |         |         |\n",
      "| NALSD [88]      | 2023 | 空间数据     | ✓    | ✓        | ✓   | ✓        |            |           |         |         |\n",
      "| NALSpatial [89]  | 2023 | 空间数据     | ✓    | ✓        | ✓   | ✓        |            |           |         |         |\n",
      "| xDBTagger [132] | 2024 | 关系数据     | ✓    | ✓        |     |          |            |           | ✓       | ✓        |\n",
      "\n",
      "## 3 可执行数据库语言的生成\n",
      "\n",
      "可执行数据库语言的生成可分为三个阶段：(i) 自然语言预处理，(ii) 自然语言理解，和 (iii) 自然语言翻译。在第一阶段，系统对原始自然语言查询进行初步分析，以为后续的自然语言理解阶段做准备。在第二阶段，系统对预处理后的自然语言查询进行语义解析和理解，以提取查询的语义细节和意图。在第三阶段，系统将理解的自然语言转换为可在数据库内执行的语言。\n",
      "\n",
      "### 3.1 自然语言预处理\n",
      "\n",
      "在自然语言查询的语义理解和翻译之前，使用传统方法和数据驱动方法进行预处理。为了预处理自然语言查询，最近开发的NLIDBs使用的技术如表3所示。\n",
      "\n",
      "许多NLIDBs的预处理过程从为领域构建专用的数据字典开始。领域知识提取过程对系统的可移植性有着深远的影响。此外，语义解析组件需要借助字典准确理解NLQ，领域知识提取过程将影响NLIDB的可用性。提取技术的主要目标是减少系统用户的负担，同时增强自动生成字典的能力。提取过程主要依赖于词干提取和同义词技术。系统然后需要对输入的自然语言进行分词和词性标注。此过程需要使用自然语言处理工具。选择工具时，应首先考虑分词和词性标注结果的高准确性，其次是处理速度。此外，查询必须面向数据库信息，相关的\n",
      "\n",
      "表4：自然语言查询解析规则\n",
      "\n",
      "| 规则                 | 典型的NLIDBs                                                                                                                                  |\n",
      "|----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| 解析树              | PRECISE [107], NaLIX [84, 85], Querix [69], DaNaLIX [81], gAnswer [60], NaLIR [78, 77, 79], NL2TRANQUYL [16], 未命名方法 [65], MyNLIDB [28] |\n",
      "| 本体               | QuestIO [27], ATHENA [113], FINESSE [63], 未命名方法 [41], CNL-RDF-Query [58], ATHENA++ [115], 未命名方法 [4]                          |\n",
      "| 语义图             | 未命名方法 [168], MEANS [1], NL2CM [6, 5]                                                                                                   |\n",
      "| 模板匹配           | SQLizer [153], 未命名方法 [3], LogicalBeam [11]                                                                                             |\n",
      "| 模式匹配           | SODA [15]                                                                                                                                       |\n",
      "| 上下文无关语法     | TR Discover [121]                                                                                                                               |\n",
      "| 语义语法           | 未命名方法 [49]                                                                                                                             |\n",
      "\n",
      "查询请求中使用的语句应紧密关联将使用的数据库。因此，词性标注通常与命名实体识别和数据字典结合使用。\n",
      "\n",
      "传统的预处理方法依赖于预定义的规则和语法，涉及的技术包括NER、正则表达式和依存分析。NLMO使用自然语言处理工具包spaCy进行分段和实体识别，并为时间信息提取设置正则表达式。ATHENA利用TIMEX注释器检测文本中提到的所有时间间隔，并利用Stanford Numeric Expressions注释器识别所有包含数值的令牌。ATHENA使用Stanford Dependency Parser识别GROUP BY子句上下文中的依赖关系。PRECISE使用Charniak解析器进行精确的解析和从生成的解析树中提取令牌关系。NL2CM使用依存分析和词性标注技术。NL2TRANQUYL使用Stanford Parser分析输入的自然语言，生成成分和依存解析。\n",
      "\n",
      "数据驱动的预处理方法依赖于大规模数据和机器学习模型，使用的技术包括词嵌入和模式链接。Word2Vec和GloVe是能够将单词表示为顺序向量空间中的点的词嵌入模型，从而捕捉单词之间的语义关系。这些向量可用于计算语义相似度和提取特征。xDBTagger使用预训练的词嵌入模型将令牌转换为300维的向量表示。IRNet通过将自然语言与数据库模式连接来进行模式链接，旨在识别自然语言中引用的具体列和表。然后根据问题中提到的方式为列分配不同的类型。\n",
      "\n",
      "### 3.2 自然语言理解\n",
      "\n",
      "理解自然语言的三种主要技术方法是 (i) 基于规则，(ii) 基于机器学习，和 (iii) 混合方法。基于这些技术，总结了最近开发的NLIDBs的自然语言理解过程。我们提供了三个时间线来描述基于规则、基于机器学习和混合方法的研究进展，如图5所示。\n",
      "\n",
      "#### 3.2.1 基于规则的方法\n",
      "\n",
      "成熟的NLI的语义解析主要基于规则。系统需要特定的规则来解析自然语言查询，包括解析树、本体、语义图、模板匹配、模式匹配、上下文无关语法和语义语法，如表4所示。基于规则的系统只能处理固定领域的知识库，通常不能移植到其他知识库。为了提高语义理解的准确性，系统通常受到支持自然语言特征（如语法和词汇）的能力限制 [144]。PRECISE [107] 阐明了语义可达性的概念，并定义了一个可以准确转换为SQL的自然语言的特定子集。但是，PRECISE会拒绝无法进行语义处理的自然语言查询。NaLIX [84, 85] 根据预定语法将自然语言查询限制为受监管的子集。DaNaLIX基于NaLIX构建，并使用领域知识进行查询翻译。领域知识封装在一系列规定中，这些规定将解析树中的领域意义术语映射到通用系统（如NaLIX）可理解的术语。DaNaLIX中的领域适配器评估当前的领域专业知识，并使用相关规则修改解析树。NaLIR [78, 77, 79] 识别在预处理步骤中可能对应于SQL组件的语言解析树中的节点，并将语义覆盖表示为解析树的一个子集。这样的树明确对应于SQL，充当查询树，调解NLQ和SQL之间。为了理解个体和集体知识的集成挑战，NL2CM首先使用RDF表示个体和"
     ]
    }
   ],
   "source": [
    "from illufly.llm import ChatAgent\n",
    "\n",
    "a = ChatAgent(model=\"qwen-long\", imitator=\"QWEN\")\n",
    "task = f'帮我翻译为中文，不需要引文：\\n{result}'\n",
    "async for chunk in a.chat(messages=task):\n",
    "    print(chunk['output_text'], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd609ce6-ca84-4758-947f-a66b156775c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Hybrid chunking¶\n",
      "\n",
      "## Overview¶\n",
      "\n",
      "Hybrid chunking applies tokenization-aware refinements on top of document-based hierarchical chunking.\n",
      "\n",
      "For more details, see here.\n",
      "\n",
      "## Setup¶\n",
      "\n",
      "In [1]:\n",
      "\n",
      "```\n",
      "%pip install -qU docling transformers\n",
      "```\n",
      "\n",
      "%pip install -qU docling transformers\n",
      "\n",
      "```\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "```\n",
      "\n",
      "## Conversion¶\n",
      "\n",
      "In [2]:\n",
      "\n",
      "```\n",
      "from docling.document_converter import DocumentConverter\n",
      "\n",
      "DOC_SOURCE = \"../../tests/data/md/wiki.md\"\n",
      "\n",
      "doc = DocumentConverter().convert(source=DOC_SOURCE).document\n",
      "```\n",
      "\n",
      "from docling.document\\_converter import DocumentConverter\n",
      "\n",
      "DOC\\_SOURCE = \"../../tests/data/md/wiki.md\"\n",
      "\n",
      "doc = DocumentConverter().convert(source=DOC\\_SOURCE).document\n",
      "\n",
      "## Chunking¶\n",
      "\n",
      "### Basic usage¶\n",
      "\n",
      "For a basic usage scenario, we can just instantiate a HybridChunker, which will use\n",
      "the default parameters.\n",
      "\n",
      "In [3]:\n",
      "\n",
      "```\n",
      "from docling.chunking import HybridChunker\n",
      "\n",
      "chunker = HybridChunker()\n",
      "chunk_iter = chunker.chunk(dl_doc=doc)\n",
      "```\n",
      "\n",
      "from docling.chunking import HybridChunker\n",
      "\n",
      "chunker = HybridChunker()\n",
      "chunk\\_iter = chunker.chunk(dl\\_doc=doc)\n",
      "\n",
      "```\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n",
      "```\n",
      "\n",
      "👉 NOTE: As you see above, using the HybridChunker can sometimes lead to a warning from the transformers library, however this is a \"false alarm\" — for details check here.\n",
      "\n",
      "Note that the text you would typically want to embed is the context-enriched one as\n",
      "returned by the serialize() method:\n",
      "\n",
      "In [4]:\n",
      "\n",
      "```\n",
      "for i, chunk in enumerate(chunk_iter):\n",
      "    print(f\"=== {i} ===\")\n",
      "    print(f\"chunk.text:\\n{repr(f'{chunk.text[:300]}…')}\")\n",
      "\n",
      "    enriched_text = chunker.serialize(chunk=chunk)\n",
      "    print(f\"chunker.serialize(chunk):\\n{repr(f'{enriched_text[:300]}…')}\")\n",
      "\n",
      "    print()\n",
      "```\n",
      "\n",
      "for i, chunk in enumerate(chunk\\_iter):\n",
      "    print(f\"=== {i} ===\")\n",
      "    print(f\"chunk.text:\\n{repr(f'{chunk.text[:300]}…')}\")\n",
      "\n",
      "    enriched\\_text = chunker.serialize(chunk=chunk)\n",
      "    print(f\"chunker.serialize(chunk):\\n{repr(f'{enriched\\_text[:300]}…')}\")\n",
      "\n",
      "    print()\n",
      "\n",
      "```\n",
      "=== 0 ===\n",
      "chunk.text:\n",
      "'International Business Machines Corporation (using the trademark IBM), nicknamed Big Blue, is an American multinational technology company headquartered in Armonk, New York and present in over 175 countries.\\nIt is a publicly traded company and one of the 30 companies in the Dow Jones Industrial Aver…'\n",
      "chunker.serialize(chunk):\n",
      "'IBM\\nInternational Business Machines Corporation (using the trademark IBM), nicknamed Big Blue, is an American multinational technology company headquartered in Armonk, New York and present in over 175 countries.\\nIt is a publicly traded company and one of the 30 companies in the Dow Jones Industrial …'\n",
      "\n",
      "=== 1 ===\n",
      "chunk.text:\n",
      "'IBM originated with several technological innovations developed and commercialized in the late 19th century. Julius E. Pitrap patented the computing scale in 1885;[17] Alexander Dey invented the dial recorder (1888);[18] Herman Hollerith patented the Electric Tabulating Machine (1889);[19] and Willa…'\n",
      "chunker.serialize(chunk):\n",
      "'IBM\\n1910s–1950s\\nIBM originated with several technological innovations developed and commercialized in the late 19th century. Julius E. Pitrap patented the computing scale in 1885;[17] Alexander Dey invented the dial recorder (1888);[18] Herman Hollerith patented the Electric Tabulating Machine (1889…'\n",
      "\n",
      "=== 2 ===\n",
      "chunk.text:\n",
      "'Collectively, the companies manufactured a wide array of machinery for sale and lease, ranging from commercial scales and industrial time recorders, meat and cheese slicers, to tabulators and punched cards. Thomas J. Watson, Sr., fired from the National Cash Register Company by John Henry Patterson,…'\n",
      "chunker.serialize(chunk):\n",
      "'IBM\\n1910s–1950s\\nCollectively, the companies manufactured a wide array of machinery for sale and lease, ranging from commercial scales and industrial time recorders, meat and cheese slicers, to tabulators and punched cards. Thomas J. Watson, Sr., fired from the National Cash Register Company by John …'\n",
      "\n",
      "=== 3 ===\n",
      "chunk.text:\n",
      "'In 1961, IBM developed the SABRE reservation system for American Airlines and introduced the highly successful Selectric typewriter.…'\n",
      "chunker.serialize(chunk):\n",
      "'IBM\\n1960s–1980s\\nIn 1961, IBM developed the SABRE reservation system for American Airlines and introduced the highly successful Selectric typewriter.…'\n",
      "```\n",
      "\n",
      "### Advanced usage¶\n",
      "\n",
      "For more control on the chunking, we can parametrize through the HybridChunker\n",
      "arguments illustrated below.\n",
      "\n",
      "Notice how tokenizer and embed\\_model further below are single-sourced from\n",
      "EMBED\\_MODEL\\_ID.\n",
      "This is important for making sure the chunker and the embedding model are using the same\n",
      "tokenizer.\n",
      "\n",
      "In [5]:\n",
      "\n",
      "```\n",
      "from transformers import AutoTokenizer\n",
      "\n",
      "from docling.chunking import HybridChunker\n",
      "\n",
      "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
      "MAX_TOKENS = 64  # set to a small number for illustrative purposes\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_ID)\n",
      "\n",
      "chunker = HybridChunker(\n",
      "    tokenizer=tokenizer,  # instance or model name, defaults to \"sentence-transformers/all-MiniLM-L6-v2\"\n",
      "    max_tokens=MAX_TOKENS,  # optional, by default derived from `tokenizer`\n",
      "    merge_peers=True,  # optional, defaults to True\n",
      ")\n",
      "chunk_iter = chunker.chunk(dl_doc=doc)\n",
      "chunks = list(chunk_iter)\n",
      "```\n",
      "\n",
      "from transformers import AutoTokenizer\n",
      "\n",
      "from docling.chunking import HybridChunker\n",
      "\n",
      "EMBED\\_MODEL\\_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
      "MAX\\_TOKENS = 64  # set to a small number for illustrative purposes\n",
      "\n",
      "tokenizer = AutoTokenizer.from\\_pretrained(EMBED\\_MODEL\\_ID)\n",
      "\n",
      "chunker = HybridChunker(\n",
      "    tokenizer=tokenizer,  # instance or model name, defaults to \"sentence-transformers/all-MiniLM-L6-v2\"\n",
      "    max\\_tokens=MAX\\_TOKENS,  # optional, by default derived from `tokenizer`\n",
      "    merge\\_peers=True,  # optional, defaults to True\n",
      ")\n",
      "chunk\\_iter = chunker.chunk(dl\\_doc=doc)\n",
      "chunks = list(chunk\\_iter)\n",
      "\n",
      "Points to notice looking at the output chunks below:\n",
      "\n",
      "- Where possible, we fit the limit of 64 tokens for the metadata-enriched serialization form (see chunk 2)\n",
      "- Where neeeded, we stop before the limit, e.g. see cases of 63 as it would otherwise run into a comma (see chunk 6)\n",
      "- Where possible, we merge undersized peer chunks (see chunk 0)\n",
      "- \"Tail\" chunks trailing right after merges may still be undersized (see chunk 8)\n",
      "\n",
      "In [6]:\n",
      "\n",
      "```\n",
      "for i, chunk in enumerate(chunks):\n",
      "    print(f\"=== {i} ===\")\n",
      "    txt_tokens = len(tokenizer.tokenize(chunk.text))\n",
      "    print(f\"chunk.text ({txt_tokens} tokens):\\n{repr(chunk.text)}\")\n",
      "\n",
      "    ser_txt = chunker.serialize(chunk=chunk)\n",
      "    ser_tokens = len(tokenizer.tokenize(ser_txt))\n",
      "    print(f\"chunker.serialize(chunk) ({ser_tokens} tokens):\\n{repr(ser_txt)}\")\n",
      "\n",
      "    print()\n",
      "```\n",
      "\n",
      "for i, chunk in enumerate(chunks):\n",
      "    print(f\"=== {i} ===\")\n",
      "    txt\\_tokens = len(tokenizer.tokenize(chunk.text))\n",
      "    print(f\"chunk.text ({txt\\_tokens} tokens):\\n{repr(chunk.text)}\")\n",
      "\n",
      "    ser\\_txt = chunker.serialize(chunk=chunk)\n",
      "    ser\\_tokens = len(tokenizer.tokenize(ser\\_txt))\n",
      "    print(f\"chunker.serialize(chunk) ({ser\\_tokens} tokens):\\n{repr(ser\\_txt)}\")\n",
      "\n",
      "    print()\n",
      "\n",
      "```\n",
      "=== 0 ===\n",
      "chunk.text (55 tokens):\n",
      "'International Business Machines Corporation (using the trademark IBM), nicknamed Big Blue, is an American multinational technology company headquartered in Armonk, New York and present in over 175 countries.\\nIt is a publicly traded company and one of the 30 companies in the Dow Jones Industrial Average.'\n",
      "chunker.serialize(chunk) (56 tokens):\n",
      "'IBM\\nInternational Business Machines Corporation (using the trademark IBM), nicknamed Big Blue, is an American multinational technology company headquartered in Armonk, New York and present in over 175 countries.\\nIt is a publicly traded company and one of the 30 companies in the Dow Jones Industrial Average.'\n",
      "\n",
      "=== 1 ===\n",
      "chunk.text (45 tokens):\n",
      "'IBM is the largest industrial research organization in the world, with 19 research facilities across a dozen countries, having held the record for most annual U.S. patents generated by a business for 29 consecutive years from 1993 to 2021.'\n",
      "chunker.serialize(chunk) (46 tokens):\n",
      "'IBM\\nIBM is the largest industrial research organization in the world, with 19 research facilities across a dozen countries, having held the record for most annual U.S. patents generated by a business for 29 consecutive years from 1993 to 2021.'\n",
      "\n",
      "=== 2 ===\n",
      "chunk.text (63 tokens):\n",
      "'IBM was founded in 1911 as the Computing-Tabulating-Recording Company (CTR), a holding company of manufacturers of record-keeping and measuring systems. It was renamed \"International Business Machines\" in 1924 and soon became the leading manufacturer of punch-card tabulating systems. During the 1960s and 1970s, the'\n",
      "chunker.serialize(chunk) (64 tokens):\n",
      "'IBM\\nIBM was founded in 1911 as the Computing-Tabulating-Recording Company (CTR), a holding company of manufacturers of record-keeping and measuring systems. It was renamed \"International Business Machines\" in 1924 and soon became the leading manufacturer of punch-card tabulating systems. During the 1960s and 1970s, the'\n",
      "\n",
      "=== 3 ===\n",
      "chunk.text (44 tokens):\n",
      "\"IBM mainframe, exemplified by the System/360, was the world's dominant computing platform, with the company producing 80 percent of computers in the U.S. and 70 percent of computers worldwide.[11]\"\n",
      "chunker.serialize(chunk) (45 tokens):\n",
      "\"IBM\\nIBM mainframe, exemplified by the System/360, was the world's dominant computing platform, with the company producing 80 percent of computers in the U.S. and 70 percent of computers worldwide.[11]\"\n",
      "\n",
      "=== 4 ===\n",
      "chunk.text (63 tokens):\n",
      "'IBM debuted in the microcomputer market in 1981 with the IBM Personal Computer, — its DOS software provided by Microsoft, — which became the basis for the majority of personal computers to the present day.[12] The company later also found success in the portable space with the ThinkPad. Since the 1990s,'\n",
      "chunker.serialize(chunk) (64 tokens):\n",
      "'IBM\\nIBM debuted in the microcomputer market in 1981 with the IBM Personal Computer, — its DOS software provided by Microsoft, — which became the basis for the majority of personal computers to the present day.[12] The company later also found success in the portable space with the ThinkPad. Since the 1990s,'\n",
      "\n",
      "=== 5 ===\n",
      "chunk.text (61 tokens):\n",
      "'IBM has concentrated on computer services, software, supercomputers, and scientific research; it sold its microcomputer division to Lenovo in 2005. IBM continues to develop mainframes, and its supercomputers have consistently ranked among the most powerful in the world in the 21st century.'\n",
      "chunker.serialize(chunk) (62 tokens):\n",
      "'IBM\\nIBM has concentrated on computer services, software, supercomputers, and scientific research; it sold its microcomputer division to Lenovo in 2005. IBM continues to develop mainframes, and its supercomputers have consistently ranked among the most powerful in the world in the 21st century.'\n",
      "\n",
      "=== 6 ===\n",
      "chunk.text (62 tokens):\n",
      "\"As one of the world's oldest and largest technology companies, IBM has been responsible for several technological innovations, including the automated teller machine (ATM), dynamic random-access memory (DRAM), the floppy disk, the hard disk drive, the magnetic stripe card, the relational database, the SQL programming\"\n",
      "chunker.serialize(chunk) (63 tokens):\n",
      "\"IBM\\nAs one of the world's oldest and largest technology companies, IBM has been responsible for several technological innovations, including the automated teller machine (ATM), dynamic random-access memory (DRAM), the floppy disk, the hard disk drive, the magnetic stripe card, the relational database, the SQL programming\"\n",
      "\n",
      "=== 7 ===\n",
      "chunk.text (63 tokens):\n",
      "'language, and the UPC barcode. The company has made inroads in advanced computer chips, quantum computing, artificial intelligence, and data infrastructure.[13][14][15] IBM employees and alumni have won various recognitions for their scientific research and inventions, including six Nobel Prizes and six Turing'\n",
      "chunker.serialize(chunk) (64 tokens):\n",
      "'IBM\\nlanguage, and the UPC barcode. The company has made inroads in advanced computer chips, quantum computing, artificial intelligence, and data infrastructure.[13][14][15] IBM employees and alumni have won various recognitions for their scientific research and inventions, including six Nobel Prizes and six Turing'\n",
      "\n",
      "=== 8 ===\n",
      "chunk.text (5 tokens):\n",
      "'Awards.[16]'\n",
      "chunker.serialize(chunk) (6 tokens):\n",
      "'IBM\\nAwards.[16]'\n",
      "\n",
      "=== 9 ===\n",
      "chunk.text (56 tokens):\n",
      "'IBM originated with several technological innovations developed and commercialized in the late 19th century. Julius E. Pitrap patented the computing scale in 1885;[17] Alexander Dey invented the dial recorder (1888);[18] Herman Hollerith patented the Electric Tabulating Machine'\n",
      "chunker.serialize(chunk) (60 tokens):\n",
      "'IBM\\n1910s–1950s\\nIBM originated with several technological innovations developed and commercialized in the late 19th century. Julius E. Pitrap patented the computing scale in 1885;[17] Alexander Dey invented the dial recorder (1888);[18] Herman Hollerith patented the Electric Tabulating Machine'\n",
      "\n",
      "=== 10 ===\n",
      "chunk.text (60 tokens):\n",
      "\"(1889);[19] and Willard Bundy invented a time clock to record workers' arrival and departure times on a paper tape (1889).[20] On June 16, 1911, their four companies were amalgamated in New York State by Charles Ranlett Flint forming a fifth company, the\"\n",
      "chunker.serialize(chunk) (64 tokens):\n",
      "\"IBM\\n1910s–1950s\\n(1889);[19] and Willard Bundy invented a time clock to record workers' arrival and departure times on a paper tape (1889).[20] On June 16, 1911, their four companies were amalgamated in New York State by Charles Ranlett Flint forming a fifth company, the\"\n",
      "\n",
      "=== 11 ===\n",
      "chunk.text (59 tokens):\n",
      "'Computing-Tabulating-Recording Company (CTR) based in Endicott, New York.[1][21] The five companies had 1,300 employees and offices and plants in Endicott and Binghamton, New York; Dayton, Ohio; Detroit, Michigan; Washington,'\n",
      "chunker.serialize(chunk) (63 tokens):\n",
      "'IBM\\n1910s–1950s\\nComputing-Tabulating-Recording Company (CTR) based in Endicott, New York.[1][21] The five companies had 1,300 employees and offices and plants in Endicott and Binghamton, New York; Dayton, Ohio; Detroit, Michigan; Washington,'\n",
      "\n",
      "=== 12 ===\n",
      "chunk.text (13 tokens):\n",
      "'D.C.; and Toronto, Canada.[22]'\n",
      "chunker.serialize(chunk) (17 tokens):\n",
      "'IBM\\n1910s–1950s\\nD.C.; and Toronto, Canada.[22]'\n",
      "\n",
      "=== 13 ===\n",
      "chunk.text (60 tokens):\n",
      "'Collectively, the companies manufactured a wide array of machinery for sale and lease, ranging from commercial scales and industrial time recorders, meat and cheese slicers, to tabulators and punched cards. Thomas J. Watson, Sr., fired from the National Cash Register Company by John Henry Patterson, called'\n",
      "chunker.serialize(chunk) (64 tokens):\n",
      "'IBM\\n1910s–1950s\\nCollectively, the companies manufactured a wide array of machinery for sale and lease, ranging from commercial scales and industrial time recorders, meat and cheese slicers, to tabulators and punched cards. Thomas J. Watson, Sr., fired from the National Cash Register Company by John Henry Patterson, called'\n",
      "\n",
      "=== 14 ===\n",
      "chunk.text (59 tokens):\n",
      "\"on Flint and, in 1914, was offered a position at CTR.[23] Watson joined CTR as general manager and then, 11 months later, was made President when antitrust cases relating to his time at NCR were resolved.[24] Having learned Patterson's pioneering business\"\n",
      "chunker.serialize(chunk) (63 tokens):\n",
      "\"IBM\\n1910s–1950s\\non Flint and, in 1914, was offered a position at CTR.[23] Watson joined CTR as general manager and then, 11 months later, was made President when antitrust cases relating to his time at NCR were resolved.[24] Having learned Patterson's pioneering business\"\n",
      "\n",
      "=== 15 ===\n",
      "chunk.text (23 tokens):\n",
      "\"practices, Watson proceeded to put the stamp of NCR onto CTR's companies.[23]:\\n105\"\n",
      "chunker.serialize(chunk) (27 tokens):\n",
      "\"IBM\\n1910s–1950s\\npractices, Watson proceeded to put the stamp of NCR onto CTR's companies.[23]:\\n105\"\n",
      "\n",
      "=== 16 ===\n",
      "chunk.text (59 tokens):\n",
      "'He implemented sales conventions, \"generous sales incentives, a focus on customer service, an insistence on well-groomed, dark-suited salesmen and had an evangelical fervor for instilling company pride and loyalty in every worker\".[25][26] His favorite slogan,'\n",
      "chunker.serialize(chunk) (63 tokens):\n",
      "'IBM\\n1910s–1950s\\nHe implemented sales conventions, \"generous sales incentives, a focus on customer service, an insistence on well-groomed, dark-suited salesmen and had an evangelical fervor for instilling company pride and loyalty in every worker\".[25][26] His favorite slogan,'\n",
      "\n",
      "=== 17 ===\n",
      "chunk.text (60 tokens):\n",
      "'\"THINK\", became a mantra for each company\\'s employees.[25] During Watson\\'s first four years, revenues reached $9 million ($158 million today) and the company\\'s operations expanded to Europe, South America, Asia and Australia.[25] Watson never liked the'\n",
      "chunker.serialize(chunk) (64 tokens):\n",
      "'IBM\\n1910s–1950s\\n\"THINK\", became a mantra for each company\\'s employees.[25] During Watson\\'s first four years, revenues reached $9 million ($158 million today) and the company\\'s operations expanded to Europe, South America, Asia and Australia.[25] Watson never liked the'\n",
      "\n",
      "=== 18 ===\n",
      "chunk.text (57 tokens):\n",
      "'clumsy hyphenated name \"Computing-Tabulating-Recording Company\" and chose to replace it with the more expansive title \"International Business Machines\" which had previously been used as the name of CTR\\'s Canadian Division;[27] the name was changed on February 14,'\n",
      "chunker.serialize(chunk) (61 tokens):\n",
      "'IBM\\n1910s–1950s\\nclumsy hyphenated name \"Computing-Tabulating-Recording Company\" and chose to replace it with the more expansive title \"International Business Machines\" which had previously been used as the name of CTR\\'s Canadian Division;[27] the name was changed on February 14,'\n",
      "\n",
      "=== 19 ===\n",
      "chunk.text (21 tokens):\n",
      "'1924.[28] By 1933, most of the subsidiaries had been merged into one company, IBM.'\n",
      "chunker.serialize(chunk) (25 tokens):\n",
      "'IBM\\n1910s–1950s\\n1924.[28] By 1933, most of the subsidiaries had been merged into one company, IBM.'\n",
      "\n",
      "=== 20 ===\n",
      "chunk.text (22 tokens):\n",
      "'In 1961, IBM developed the SABRE reservation system for American Airlines and introduced the highly successful Selectric typewriter.'\n",
      "chunker.serialize(chunk) (26 tokens):\n",
      "'IBM\\n1960s–1980s\\nIn 1961, IBM developed the SABRE reservation system for American Airlines and introduced the highly successful Selectric typewriter.'\n",
      "```\n",
      "\n",
      "Conversion of custom XML\n",
      "\n",
      "RAG with Haystack\n",
      "\n",
      "Made with\n"
     ]
    }
   ],
   "source": [
    "result = converter.convert(\n",
    "    \"https://docling-project.github.io/docling/examples/hybrid_chunking/\"\n",
    ")\n",
    "print(result.document.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12054044-40ed-4a43-85f3-6a4c7546feae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# 模型列表\\n\\n百炼提供了丰富多样的模型选择，它集成了通义系列大模型和第三方大模型，涵盖文本、图像、音视频等不同模态。\\n\\n## 旗舰模型\\n\\n| 旗舰模型                  |  通义千问-Max适合复杂任务，能力最强   |  通义千问-Plus效果、速度、成本均衡   |  通义千问-Turbo适合简单任务，速度快、成本极低   |  通义千问-Long适合大规模文本分析，效果与速度均衡、成本较低   |\\n|---------------------------|---------------------------------------|--------------------------------------|-------------------------------------------------|--------------------------------------------------------------|\\n| 最大上下文长度（Token数） | 32,768                                | 131,072                              | 1,000,000                                       | 10,000,000                                                   |\\n| 最低输入价格（每千Token） | 0.0024元                              | 0.0008元                             | 0.0003元                                        | 0.0005元                                                     |\\n| 最低输出价格（每千Token） | 0.0096元                              | 0.002元                              | 0.0006元                                        | 0.002元                                                      |\\n\\n## 模型总览\\n\\n| 类别           | 模型          | 说明                                                                                                                                                                                                                                                                                                                                                               |\\n|----------------|---------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| 文本生成       | 通义千问      | 通义千问大语言模型：商业版（QwQ、通义千问-Max、通义千问-Plus、通义千问-Turbo）、开源版（QwQ、Qwen2.5、Qwen2、Qwen1.5、Qwen）、超长文档模型通义千问-Long多模态模型：视觉理解模型通义千问VL、音频理解模型通义千问Audio、全模态模型通义千问Omni数学模型：通义千问数学模型代码模型：通义千问Coder翻译模型：通义千问翻译模型                                            |\\n| 文本生成       | 第三方模型    | DeepSeek、Llama、百川、ChatGLM、零一万物等。                                                                                                                                                                                                                                                                                                                       |\\n| 图像生成       | 通用文生图    | 可生成图像或编辑图像，适用于生成证件照、电商主图、模特图、各种风格人像图（动漫、国风、二次元等），也可用于抠图、生成背景、更改图片元素等。                                                                                                                                                                                                                         |\\n| 图像生成       | 图像编辑      | 通用图像编辑：集合多个功能于一身，如去水印、图像风格化、图像修复等。涂鸦作画人像风格重绘：图像画面扩展人物实例分割、图像擦除补全                                                                                                                                                                                                                                   |\\n| 图像生成       | 素材创作      | 电商素材创作：虚拟模特、鞋靴模特、ai试衣、图像背景背景生成：商品主图背景生海报创作：创意海报生成、创意文字生成人物创作：人物写真生成                                                                                                                                                                                                                               |\\n| 图像生成       | 第三方模型    | Stable Diffusion和FLUX。                                                                                                                                                                                                                                                                                                                                           |\\n| 语音合成与识别 | 语音合成      | CosyVoice和Sambert可实现文本转语音，适用于智能语音客服、有声读物、车载导航、教育辅导等场景。                                                                                                                                                                                                                                                                       |\\n| 语音合成与识别 | 语音识别/翻译 | Gummy、Paraformer和SenseVoice可实现语音转文本，适用于实时会议记录、实时直播字幕、电话客服等场景。此外，Gummy还支持语音翻译。                                                                                                                                                                                                                                       |\\n| 视频编辑与生成 | 文生视频      | 文生视频：一句话生成视频，视频风格丰富，画质细腻。                                                                                                                                                                                                                                                                                                                 |\\n| 视频编辑与生成 | 图生视频      | 图生视频：将输入图片作为视频首帧，并根据提示词生成视频。图+动作模板生成舞蹈视频：舞动人像AnimateAnyone基于人物图片和动作视频生成舞蹈视频。图+音频生成对口型视频悦动人像EMO基于人物图片和音频，适合唱演场景。灵动人像LivePortrait基于人物图片和音频，适合语音播报场景。图+表情模板生成表情包视频：表情包Emoji基于人脸图片和预设的人脸动态模板，生成人脸表情包视频。 |\\n| 视频编辑与生成 | 视频编辑      | 视频口型替换：声动人像VideoRetalk基于人物视频和音频，适合短视频制作、视频翻译等场景。视频风格转换：视频风格重绘可将视频转换为日式漫画、美式漫画等风格。                                                                                                                                                                                                            |\\n| 向量           | 文本向量      | 将文本转换成一组可以代表文字的数字，用于搜索、聚类、推荐、分类等。                                                                                                                                                                                                                                                                                                 |\\n| 向量           | 多模态向量    | 将文本、图像、语音转换成一组数字，用于音视频分类、图像分类、图文检索等。                                                                                                                                                                                                                                                                                           |\\n| 行业           | 通义法睿      | 适用于法律咨询、案例分析和法规解读等。                                                                                                                                                                                                                                                                                                                             |\\n| 行业           | 意图理解      | 意图理解模型能够在毫秒级时间内解析用户意图，并选择合适工具来解决用户问题。                                                                                                                                                                                                                                                                                         |\\n\\n## 文本生成-通义千问\\n\\n以下是通义千问模型的商业版。相较于开源版，商业版具有最新的能力和改进。\\n\\n### QwQ\\n\\n基于 Qwen2.5 模型训练的 QwQ 推理模型，通过强化学习大幅度提升了模型推理能力。模型数学代码等核心指标（AIME 24/25、LiveCodeBench）以及部分通用指标（IFEval、LiveBench等）达到DeepSeek-R1 满血版水平。使用方法\\n\\n| 模型名称                             | 版本   | 上下文长度   | 最大输入    | 最大思维链长度   | 最大回复长度   | 输入成本                    | 输出成本                  | 免费额度（注）                         |\\n|--------------------------------------|--------|--------------|-------------|------------------|----------------|-----------------------------|---------------------------|----------------------------------------|\\n| 模型名称                             | 版本   | （Token数）  | （Token数） | （Token数）      | （Token数）    | （每千Token）               | （每千Token）             | 免费额度（注）                         |\\n| qwq-plus当前等同 qwq-plus-2025-03-05 | 稳定版 | 131,072      | 98,304      | 32,768           | 8,192          | 0.0016元Batch调用：0.0008元 | 0.004元Batch调用：0.002元 | 各100万 Token有效期：百炼开通后180天内 |\\n| qwq-plus-latest始终等同最新快照版    | 最新版 | 131,072      | 98,304      | 32,768           | 8,192          | 0.0016元                    | 0.004元                   | 各100万 Token有效期：百炼开通后180天内 |\\n| qwq-plus-2025-03-05又称qwq-plus-0305 | 快照版 | 131,072      | 98,304      | 32,768           | 8,192          | 0.0016元                    | 0.004元                   | 各100万 Token有效期：百炼开通后180天内 |\\n\\n### 通义千问-Max\\n\\n通义千问系列效果最好的模型，适合复杂、多步骤的任务。使用方法 | API参考 | 在线体验\\n\\n#### 公共云\\n\\n| 模型名称                                          | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本                    | 输出成本                    | 免费额度（注）                        |\\n|---------------------------------------------------|--------|--------------|-------------|-------------|-----------------------------|-----------------------------|---------------------------------------|\\n| 模型名称                                          | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token）               | （每千Token）               | 免费额度（注）                        |\\n| qwen-max当前等同qwen-max-2024-09-19               | 稳定版 | 32,768       | 30,720      | 8,192       | 0.0024元Batch调用：0.0012元 | 0.0096元Batch调用：0.0048元 | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-max-latest始终等同最新快照版                 | 最新版 | 32,768       | 30,720      | 8,192       | 0.0024元Batch调用：0.0012元 | 0.0096元Batch调用：0.0048元 | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-max-2025-01-25又称qwen-max-0125、Qwen2.5-Max | 快照版 | 32,768       | 30,720      | 8,192       | 0.0024元                    | 0.0096元                    | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-max-2024-09-19又称qwen-max-0919              | 快照版 | 32,768       | 30,720      | 8,192       | 0.02元                      | 0.06元                      | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-max-2024-04-28又称qwen-max-0428              | 快照版 | 8,000        | 6,000       | 2,000       | 0.04元                      | 0.12元                      | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-max-2024-04-03又称qwen-max-0403              | 快照版 | 8,000        | 6,000       | 2,000       | 0.04元                      | 0.12元                      | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-max-2024-01-07又称qwen-max-0107              | 快照版 | 8,000        | 6,000       | 2,000       | 0.04元                      | 0.12元                      | 各100万Token有效期：百炼开通后180天内 |\\n\\n#### 金融云\\n\\n| 模型名称   | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本                  | 输出成本                  | 免费额度                            |\\n|------------|--------|--------------|-------------|-------------|---------------------------|---------------------------|-------------------------------------|\\n| 模型名称   | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token）             | （每千Token）             | 免费额度                            |\\n| qwen-max   | 稳定版 | 8,000        | 6,000       | 2,000       | 0.038元Batch调用：0.019元 | 0.114元Batch调用：0.057元 | 100万Token有效期：百炼开通后180天内 |\\n\\n### 通义千问-Plus\\n\\n能力均衡，推理效果、成本和速度介于通义千问-Max和通义千问-Turbo之间，适合中等复杂任务。使用方法 | API参考 | 在线体验\\n\\n#### 公共云\\n\\n| 模型名称                               | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本                    | 输出成本                  | 免费额度（注）                        |\\n|----------------------------------------|--------|--------------|-------------|-------------|-----------------------------|---------------------------|---------------------------------------|\\n| 模型名称                               | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token）               | （每千Token）             | 免费额度（注）                        |\\n| qwen-plus当前等同qwen-plus-2025-01-25  | 稳定版 | 131,072      | 129,024     | 8,192       | 0.0008元Batch调用：0.0004元 | 0.002元Batch调用：0.001元 | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-plus-latest始终等同最新快照版     | 最新版 | 131,072      | 129,024     | 8,192       | 0.0008元Batch调用：0.0004元 | 0.002元Batch调用：0.001元 | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-plus-2025-01-25又称qwen-plus-0125 | 快照版 | 131,072      | 129,024     | 8,192       | 0.0008元                    | 0.002元                   | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-plus-2025-01-12又称qwen-plus-0112 | 快照版 | 131,072      | 129,024     | 8,192       | 0.0008元                    | 0.002元                   | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-plus-2024-12-20又称qwen-plus-1220 | 快照版 | 131,072      | 129,024     | 8,192       | 0.0008元                    | 0.002元                   | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-plus-2024-11-27又称qwen-plus-1127 | 快照版 | 131,072      | 129,024     | 8,192       | 0.0008元                    | 0.002元                   | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-plus-2024-11-25又称qwen-plus-1125 | 快照版 | 131,072      | 129,024     | 8,192       | 0.0008元                    | 0.002元                   | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-plus-2024-09-19又称qwen-plus-0919 | 快照版 | 131,072      | 129,024     | 8,192       | 0.0008元                    | 0.002元                   | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-plus-2024-08-06又称qwen-plus-0806 | 快照版 | 131,072      | 128,000     | 8,192       | 0.004元                     | 0.012元                   | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-plus-2024-07-23又称qwen-plus-0723\\u200b | 快照版 | 32,000       | 30,000      | 8,000       | 0.004元                     | 0.012元                   | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-plus-2024-06-24又称qwen-plus-0624 | 快照版 | 32,000       | 30,000      | 8,000       | 0.004元                     | 0.012元                   | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-plus-2024-02-06又称qwen-plus-0206 | 快照版 | 32,000       | 30,000      | 8,000       | 0.004元                     | 0.012元                   | 各100万Token有效期：百炼开通后180天内 |\\n\\n#### 金融云\\n\\n| 模型名称   | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本                      | 输出成本                    | 免费额度                            |\\n|------------|--------|--------------|-------------|-------------|-------------------------------|-----------------------------|-------------------------------------|\\n| 模型名称   | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token）                 | （每千Token）               | 免费额度                            |\\n| qwen-plus  | 稳定版 | 131,072      | 128,000     | 8,192       | 0.00152元Batch调用：0.00076元 | 0.0038元Batch调用：0.0019元 | 100万Token有效期：百炼开通后180天内 |\\n\\n### 通义千问-Turbo\\n\\n通义千问系列速度最快、成本极低的模型，适合简单任务。使用方法 | API参考 | 在线体验\\n\\n| 模型名称                                 | 版本   | 上下文长度   | 最大输入    | 最大输出   | 输入成本                     | 输出成本                    | 免费额度（注）                        |\\n|------------------------------------------|--------|--------------|-------------|------------|------------------------------|-----------------------------|---------------------------------------|\\n| 模型名称                                 | 版本   | （Token数）  | （Token数） |            | （每千Token）                | （每千Token）               | 免费额度（注）                        |\\n| qwen-turbo当前等同 qwen-turbo-2025-02-11 | 稳定版 | 1,000,000    | 1,000,000   | 8,192      | 0.0003元Batch调用：0.00015元 | 0.0006元Batch调用：0.0003元 | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-turbo-latest始终等同最新快照版      | 最新版 | 1,000,000    | 1,000,000   | 8,192      | 0.0003元Batch调用：0.00015元 | 0.0006元Batch调用：0.0003元 | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-turbo-2025-02-11又称qwen-turbo-0211 | 快照版 | 1,000,000    | 1,000,000   | 8,192      | 0.0003元                     | 0.0006元                    | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-turbo-2024-11-01又称qwen-turbo-1101 | 快照版 | 1,000,000    | 1,000,000   | 8,192      | 0.0003元                     | 0.0006元                    | 1000万Token有效期：百炼开通后180天内  |\\n| qwen-turbo-2024-09-19又称qwen-turbo-0919 | 快照版 | 131,072      | 129,024     | 8,192      | 0.0003元                     | 0.0006元                    | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-turbo-2024-06-24又称qwen-turbo-0624 | 快照版 | 8,000        | 6,000       | 2,000      | 0.002元                      | 0.006元                     | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-turbo-2024-02-06又称qwen-turbo-0206 | 快照版 | 8,000        | 6,000       | 2,000      | 0.002元                      | 0.006元                     | 各100万Token有效期：百炼开通后180天内 |\\n\\n### 通义千问-Long\\n\\n通义千问系列上下文窗口最长，能力均衡且成本较低的模型，适合长文本分析、信息抽取、总结摘要和分类打标等任务。使用方法 | 在线体验\\n\\n## 公共云\\n\\n| 模型名称                               | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本                     | 输出成本                  | 免费额度（注）                      |\\n|----------------------------------------|--------|--------------|-------------|-------------|------------------------------|---------------------------|-------------------------------------|\\n| 模型名称                               | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token）                | （每千Token）             | 免费额度（注）                      |\\n| qwen-long                              | 稳定版 | 10,000,000   | 10,000,000  | 8,192       | 0.0005元Batch调用：0.00025元 | 0.002元Batch调用：0.001元 | 100万Token有效期：百炼开通后180天内 |\\n| qwen-long-latest始终等同最新快照版     | 最新版 | 10,000,000   | 10,000,000  | 8,192       | 0.0005元Batch调用：0.00025元 | 0.002元Batch调用：0.001元 | 100万Token有效期：百炼开通后180天内 |\\n| qwen-long-2025-01-25又称qwen-long-0125 | 快照版 | 10,000,000   | 10,000,000  | 8,192       | 0.0005元                     | 0.002元                   | 100万Token有效期：百炼开通后180天内 |\\n\\n## 金融云\\n\\n| 模型名称   | 上下文长度   | 最大输入    | 最大输出    | 输入成本                     | 输出成本                  | 免费额度（注）                      |\\n|------------|--------------|-------------|-------------|------------------------------|---------------------------|-------------------------------------|\\n| 模型名称   | （Token数）  | （Token数） | （Token数） | （每千Token）                | （每千Token）             | 免费额度（注）                      |\\n| qwen-long  | 10,000,000   | 10,000,000  | 8,192       | 0.0005元Batch调用：0.00025元 | 0.002元Batch调用：0.001元 | 100万Token有效期：百炼开通后180天内 |\\n\\n### 通义千问Omni\\n\\n通义千问全新多模态理解生成大模型，支持文本、图像、语音与视频输入，并输出文本与音频，提供了4种自然对话音色。使用方法｜API 参考\\n\\n| 模型名称                                           | 版本   | 上下文长度   | 最大输入    | 最大输出    | 免费额度（注）                                      |\\n|----------------------------------------------------|--------|--------------|-------------|-------------|-----------------------------------------------------|\\n| 模型名称                                           | 版本   | （Token数）  | （Token数） | （Token数） | 免费额度（注）                                      |\\n| qwen-omni-turbo当前等同qwen-omni-turbo-2025-03-26  | 稳定版 | 32,768       | 30,720      | 2,048       | 各100万Token（不区分模态）有效期：百炼开通后180天内 |\\n| qwen-omni-turbo-latest始终等同最新快照版           | 最新版 | 32,768       | 30,720      | 2,048       | 各100万Token（不区分模态）有效期：百炼开通后180天内 |\\n| qwen-omni-turbo-2025-03-26又称qwen-omni-turbo-0326 | 快照版 | 32,768       | 30,720      | 2,048       | 各100万Token（不区分模态）有效期：百炼开通后180天内 |\\n| qwen-omni-turbo-2025-01-19又称qwen-omni-turbo-0119 | 快照版 | 32,768       | 30,720      | 2,048       | 各100万Token（不区分模态）有效期：百炼开通后180天内 |\\n\\n免费额度用完后，输入与输出的计费规则如下：\\n\\n### QVQ\\n\\n通义千问QVQ是视觉推理模型，支持视觉输入及思维链输出，在数学、编程、视觉分析、创作以及通用任务上都表现了更强的能力。使用方法\\n\\n| 模型名称                           | 版本   | 上下文长度   | 最大输入            | 最大思维链长度   | 最大回复长度   | 输入成本      | 输出成本      | 免费额度（注）                         |\\n|------------------------------------|--------|--------------|---------------------|------------------|----------------|---------------|---------------|----------------------------------------|\\n| 模型名称                           | 版本   | （Token数）  | （Token数）         | （Token数）      | （Token数）    | （每千Token） | （每千Token） | 免费额度（注）                         |\\n| qvq-max当前等同 qvq-max-2025-03-25 | 稳定版 | 122,880      | 98,304单图最大16384 | 16,384           | 8,192          | 0.008元       | 0.032元       | 各100万 Token有效期：百炼开通后180天内 |\\n| qvq-max-latest始终等同最新快照版   | 最新版 | 122,880      | 98,304单图最大16384 | 16,384           | 8,192          | 0.008元       | 0.032元       | 各100万 Token有效期：百炼开通后180天内 |\\n| qvq-max-2025-03-25又称qvq-max-0325 | 快照版 | 122,880      | 98,304单图最大16384 | 16,384           | 8,192          | 0.008元       | 0.032元       | 各100万 Token有效期：百炼开通后180天内 |\\n\\n### 通义千问VL\\n\\n通义千问VL是具有视觉（图像）理解能力的文本生成模型，不仅能进行OCR（图片文字识别），还能进一步总结和推理，例如从商品照片中提取属性，根据习题图进行解题等。使用方法 | API参考 | 在线体验\\n\\n| 模型名称                                                                                                                   | 版本   | 上下文长度   | 最大输入             | 最大输出    | 输入成本                     | 输出成本                     | 免费额度（注）                        |\\n|----------------------------------------------------------------------------------------------------------------------------|--------|--------------|----------------------|-------------|------------------------------|------------------------------|---------------------------------------|\\n| 模型名称                                                                                                                   | 版本   | （Token数）  | （Token数）          | （Token数） | （每千Token）                | （每千Token）                | 免费额度（注）                        |\\n| qwen-vl-max相比qwen-vl-plus再次提升视觉推理和指令遵循能力，在更多复杂任务中提供最佳性能。当前等同qwen-vl-max-2024-11-19    | 稳定版 | 32,768       | 30,720单图最大16384  | 2,048       | 0.003元Batch调用：0.0015元   | 0.009元Batch调用：0.0045元   | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-vl-max-latest始终等同最新快照版                                                                                       | 最新版 | 131,072      | 129,024单图最大16384 | 8,192       | 0.003元Batch调用：0.0015元   | 0.009元Batch调用：0.0045元   | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-vl-max-2025-01-25又称qwen-vl-max-0125此版本属于Qwen2.5-VL系列模型，扩展上下文至128k，显著增强图像和视频的理解能力。   | 快照版 | 131,072      | 129,024单图最大16384 | 8,192       | 0.003元                      | 0.009元                      | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-vl-max-2024-12-30又称qwen-vl-max-1230                                                                                 | 快照版 | 32,768       | 30,720单图最大16384  | 2,048       | 0.003元                      | 0.009元                      | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-vl-max-2024-11-19又称qwen-vl-max-1119                                                                                 | 快照版 | 32,768       | 30,720单图最大16384  | 2,048       | 0.003元                      | 0.009元                      | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-vl-max-2024-10-30又称qwen-vl-max-1030                                                                                 | 快照版 | 32,768       | 30,720单图最大16384  | 2,048       | 0.02元                       | 0.02元                       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-vl-max-2024-08-09又称qwen-vl-max-0809此版本扩展上下文至32k，增强图像理解能力，能更好地识别图片中的多语种和手写体。    | 快照版 | 32,768       | 30,720单图最大16384  | 2,048       | 0.02元                       | 0.02元                       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-vl-max-2024-02-01又称qwen-vl-max-0201                                                                                 | 快照版 | 8,000        | 6,000单图最大1280    | 2,000       | 0.02元                       | 0.02元                       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-vl-plus大幅提升细节识别和文字识别能力，支持超百万像素分辨率和任意宽高比的图像。在广泛的视觉任务中提供卓越性能。       | 稳定版 | 8,000        | 6,000单图最大1280    | 2,000       | 0.0015元Batch调用：0.00075元 | 0.0045元Batch调用：0.00225元 | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-vl-plus-latest始终等同最新快照版                                                                                      | 最新版 | 131,072      | 129,024单图最大16384 | 8,192       | 0.0015元                     | 0.0045元                     | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-vl-plus-2025-01-25又称qwen-vl-plus-0125此版本属于Qwen2.5-VL系列模型，扩展上下文至128k，显著增强图像和视频的理解能力。 | 快照版 | 131,072      | 129,024单图最大16384 | 8,192       | 0.0015元                     | 0.0045元                     | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-vl-plus-2025-01-02又称qwen-vl-plus-0102                                                                               | 快照版 | 32,768       | 30,720单图最大16384  | 2,048       | 0.0015元                     | 0.0045元                     | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-vl-plus-2024-08-09又称qwen-vl-plus-0809                                                                               | 快照版 | 32,768       | 30,720单图最大16384  | 2,048       | 0.0015元                     | 0.0045元                     | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-vl-plus-2023-12-01                                                                                                    | 快照版 | 8,000        | 6,000                | 2,000       | 0.008元                      | 0.008元                      | 各100万Token有效期：百炼开通后180天内 |\\n\\n### 通义千问OCR\\n\\n通义千问OCR模型是专用于文字提取的模型。相较于通义千问VL模型，它更专注于文档、表格、试题、手写体文字等类型图像的文字提取能力。它能够识别多种语言，包括英语、法语、日语、韩语、德语、俄语和意大利语等。使用方法 | API参考｜在线体验\\n\\n| 模型名称                                   | 版本   | 上下文长度   | 最大输入           | 最大输出    | 输入输出单价   | 免费额度（注）                        |\\n|--------------------------------------------|--------|--------------|--------------------|-------------|----------------|---------------------------------------|\\n| 模型名称                                   | 版本   | （Token数）  | （Token数）        | （Token数） | （每千Token）  | 免费额度（注）                        |\\n| qwen-vl-ocr当前等同qwen-vl-ocr-2024-10-28  | 稳定版 | 34096        | 30000单图最大30000 | 4096        | 0.005元        | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-vl-ocr-latest始终等同最新快照版       | 最新版 | 34096        | 30000单图最大30000 | 4096        | 0.005元        | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-vl-ocr-2024-10-28又称qwen-vl-ocr-1028 | 快照版 | 34096        | 30000单图最大30000 | 4096        | 0.005元        | 各100万Token有效期：百炼开通后180天内 |\\n\\n### 通义千问Audio\\n\\n通义千问Audio是音频理解模型，支持输入多种音频（人类语音、自然音、音乐、歌声）和文本，并输出文本。该模型不仅能对输入的音频进行转录，还具备更深层次的语义理解、情感分析、音频事件检测、语音聊天等能力。使用方法\\n\\n| 模型名称                                                                                                         | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本                                                     | 输出成本                                                     | 免费额度（注）                       |\\n|------------------------------------------------------------------------------------------------------------------|--------|--------------|-------------|-------------|--------------------------------------------------------------|--------------------------------------------------------------|--------------------------------------|\\n| 模型名称                                                                                                         | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token）                                                | （每千Token）                                                | 免费额度（注）                       |\\n| qwen-audio-turbo当前等同qwen-audio-turbo-2024-08-07                                                              | 稳定版 | 8,000        | 6,000       | 1,500       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\\n| qwen-audio-turbo-latest始终等同最新快照版                                                                        | 最新版 | 8,192        | 6,144       | 2,048       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\\n| qwen-audio-turbo-2024-12-04又称qwen-audio-turbo-1204较上个快照版本大幅提升语音识别准确率，且新增了语音聊天能力。 | 快照版 | 8,192        | 6,144       | 2,048       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\\n| qwen-audio-turbo-2024-08-07又称qwen-audio-turbo-0807                                                             | 快照版 | 8,000        | 6,000       | 1,500       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\\n\\n### 通义千问ASR（Beta版本）\\n\\n通义千问ASR是基于Qwen-Audio训练，专用于语音识别的模型。目前支持的语言有：中文和英文。使用方法\\n\\n通义千问Audio与通义千问ASR（Beta版本）的区别\\n\\n- 功能对比：\\n- 功能对比：\\n    - 通义千问Audio模型是对话模型，不仅能够进行语音识别，还具备更深层次的语义理解、语音聊天等能力，支持设置提示词。\\n    - 通义千问ASR模型是专用于语音识别的模型，不支持设置提示词（包括System\\xa0Prompt和User Prompt）。\\n- 准确率对比：\\n- 准确率对比：\\n    - 在语音识别准确率上，通义千问ASR模型高于通义千问Audio模型。\\n- 音频时长对比：\\n- 音频时长对比：\\n    - 通义千问Audio模型：30秒内。\\n    - 通义千问ASR模型：3分钟以内。\\n- 支持识别的语言对比 ：\\n- 支持识别的语言对比：\\n    - 通义千问Audio模型：中文、英文、粤语、法语、意大利语、西班牙语、德语和日语。\\n    - 通义千问ASR模型：中文、英文。目前通义千问ASR是Beta版本，后续版本中将会陆续支持更多语言的识别。\\n\\n| 模型名称                                          | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本                                                     | 输出成本                                                     | 免费额度（注）                       |\\n|---------------------------------------------------|--------|--------------|-------------|-------------|--------------------------------------------------------------|--------------------------------------------------------------|--------------------------------------|\\n| 模型名称                                          | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token）                                                | （每千Token）                                                | 免费额度（注）                       |\\n| qwen-audio-asr当前等同qwen-audio-asr-2024-12-04   | 稳定版 | 8,192        | 6,144       | 2,048       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\\n| qwen-audio-asr-latest始终等同最新快照版           | 最新版 | 8,192        | 6,144       | 2,048       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\\n| qwen-audio-asr-2024-12-04\\xa0又称qwen-audio-asr-1204 | 快照版 | 8,192        | 6,144       | 2,048       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\\n\\n### 通义千问数学模型\\n\\n通义千问数学模型是专门用于数学解题的语言模型。使用方法 | API参考 | 在线体验\\n\\n| 模型名称                                           | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                        |\\n|----------------------------------------------------|--------|--------------|-------------|-------------|---------------|---------------|---------------------------------------|\\n| 模型名称                                           | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                        |\\n| qwen-math-plus当前等同qwen-math-plus-2024-09-19    | 稳定版 | 4,096        | 3,072       | 3,072       | 0.004元       | 0.012元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-math-plus-latest始终等同最新快照版            | 最新版 | 4,096        | 3,072       | 3,072       | 0.004元       | 0.012元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-math-plus-2024-09-19又称qwen-math-plus-0919   | 快照版 | 4,096        | 3,072       | 3,072       | 0.004元       | 0.012元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-math-plus-2024-08-16又称qwen-math-plus-0816   | 快照版 | 4,096        | 3,072       | 3,072       | 0.004元       | 0.012元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-math-turbo当前等同qwen-math-turbo-2024-09-19  | 稳定版 | 4,096        | 3,072       | 3,072       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-math-turbo-latest始终等同最新快照版           | 最新版 | 4,096        | 3,072       | 3,072       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-math-turbo-2024-09-19又称qwen-math-turbo-0919 | 快照版 | 4,096        | 3,072       | 3,072       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\\n\\n### 通义千问Coder\\n\\n通义千问代码模型。使用方法 | API参考 | 在线体验\\n\\n| 模型名称                                                  | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                        |\\n|-----------------------------------------------------------|--------|--------------|-------------|-------------|---------------|---------------|---------------------------------------|\\n| 模型名称                                                  | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                        |\\n| qwen-coder-plus当前等同qwen-coder-plus-2024-11-06         | 稳定版 | 131,072      | 129,024     | 8,192       | 0.0035元      | 0.007元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-coder-plus-latest等同qwen-coder-plus最新的快照版本   | 最新版 | 131,072      | 129,024     | 8,192       | 0.0035元      | 0.007元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-coder-plus-2024-11-06 又称qwen-coder-plus-1106       | 快照版 | 131,072      | 129,024     | 8,192       | 0.0035元      | 0.007元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-coder-turbo当前等同qwen-coder-turbo-2024-09-19       | 稳定版 | 131,072      | 129,024     | 8,192       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-coder-turbo-latest等同qwen-coder-turbo最新的快照版本 | 最新版 | 131,072      | 129,024     | 8,192       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-coder-turbo-2024-09-19又称qwen-coder-turbo-0919      | 快照版 | 131,072      | 129,024     | 8,192       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\\n\\n### 通义千问翻译模型\\n\\n基于通义千问模型优化的机器翻译大语言模型，擅长中英互译、中文与小语种互译、英文与小语种互译，小语种包括日、韩、法、西、德、葡（巴西）、泰、印尼、越、阿等26种。在多语言互译的基础上，提供术语干预、领域提示、记忆库等能力，提升模型在复杂应用场景下的翻译效果。使用方法\\n\\n| 模型名称      | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                       |\\n|---------------|--------------|-------------|-------------|---------------|---------------|--------------------------------------|\\n| 模型名称      | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                       |\\n| qwen-mt-plus  | 2,048        | 1,024       | 1,024       | 0.015元       | 0.045元       | 各50万Token有效期：百炼开通后180天内 |\\n| qwen-mt-turbo | 2,048        | 1,024       | 1,024       | 0.001元       | 0.003元       | 各50万Token有效期：百炼开通后180天内 |\\n\\n## 文本生成-通义千问-开源版\\n\\n- 模型名称中，xxb表示参数规模，例如qwen2-72b-instruct表示参数规模为72B，即720亿。\\n- 百炼支持调用通义千问的开源版，您无需本地部署模型。对于开源版，建议使用Qwen2.5或Qwen2模型。\\n\\n### QwQ-开源版\\n\\n基于 Qwen2.5-32B 模型训练的 QwQ 推理模型，通过强化学习大幅度提升了模型推理能力。模型数学代码等核心指标（AIME 24/25、LiveCodeBench）以及部分通用指标（IFEval、LiveBench等）达到DeepSeek-R1 满血版水平，各指标均显著超过同样基于 Qwen2.5-32B 的 DeepSeek-R1-Distill-Qwen-32B。使用方法\\n\\n| 模型名称   | 上下文长度   | 最大输入    | 最大思维链长度   | 最大回复长度   | 输入成本      | 输出成本      | 免费额度（注）                       |\\n|------------|--------------|-------------|------------------|----------------|---------------|---------------|--------------------------------------|\\n| 模型名称   | （Token数）  | （Token数） | （Token数）      | （Token数）    | （每千Token） | （每千Token） | 免费额度（注）                       |\\n| qwq-32b    | 131,072      | 98,304      | 32,768           | 8,192          | 0.002元       | 0.006元       | 100万 Token有效期：百炼开通后180天内 |\\n\\n### QwQ-Preview\\n\\nqwq-32b-preview 模型是由 Qwen 团队于2024年开发的实验性研究模型，专注于增强 AI 推理能力，尤其是数学和编程领域。qwq-32b-preview 模型的局限性请参见QwQ官方博客。使用方法 | API参考｜在线体验\\n\\n| 模型名称        | 上下文长度   | 最大输入    | 最大输出    | 输入成本                  | 输出成本                  | 免费额度（注）                      |\\n|-----------------|--------------|-------------|-------------|---------------------------|---------------------------|-------------------------------------|\\n| 模型名称        | （Token数）  | （Token数） | （Token数） | （每千Token）             | （每千Token）             | 免费额度（注）                      |\\n| qwq-32b-preview | 32,768       | 30,720      | 16,384      | 0.002元Batch调用：0.001元 | 0.006元Batch调用：0.003元 | 100万Token有效期：百炼开通后180天内 |\\n\\n### Qwen2.5\\n\\nQwen2.5是Qwen大型语言模型的最新系列。针对Qwen2.5，我们发布了一系列基础语言模型和指令调优语言模型，参数规模从5亿到720亿不等。Qwen2.5在Qwen2基础上进行了以下改进：\\n\\n- 在我们最新的大规模数据集上进行预训练，包含多达18万亿个Token。\\n- 由于我们在这些领域的专业专家模型，模型的知识显著增多，编码和数学能力也大幅提高。\\n- 在遵循指令、生成长文本（超过8K个标记）、理解结构化数据（例如表格）和生成结构化输出（尤其是JSON）方面有显著改进。对系统提示的多样性更具弹性，增强了聊天机器人的角色扮演实现和条件设置。\\n- 支持超过29种语言，包括中文、英语、法语、西班牙语、葡萄牙语、德语、意大利语、俄语、日语、韩语、越南语、泰语、阿拉伯语等。\\n\\n使用方法 | API参考 | 在线体验\\n\\n| 模型名称                | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                        |\\n|-------------------------|--------------|-------------|-------------|---------------|---------------|---------------------------------------|\\n| 模型名称                | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                        |\\n| qwen2.5-14b-instruct-1m | 1,000,000    | 1,000,000   | 8,192       | 0.001元       | 0.003元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2.5-7b-instruct-1m  | 1,000,000    | 1,000,000   | 8,192       | 0.0005元      | 0.001元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2.5-72b-instruct    | 131,072      | 129,024     | 8,192       | 0.004元       | 0.012元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2.5-32b-instruct    | 131,072      | 129,024     | 8,192       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2.5-14b-instruct    | 131,072      | 129,024     | 8,192       | 0.001元       | 0.003元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2.5-7b-instruct     | 131,072      | 129,024     | 8,192       | 0.0005元      | 0.001元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2.5-3b-instruct     | 32,768       | 30,720      | 8,192       | 0.0003元      | 0.0009元      | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2.5-1.5b-instruct   | 32,768       | 30,720      | 8,192       | 限时免费      | 限时免费      | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2.5-0.5b-instruct   | 32,768       | 30,720      | 8,192       | 限时免费      | 限时免费      | 各100万Token有效期：百炼开通后180天内 |\\n\\n### Qwen2\\n\\n阿里云的通义千问2-开源版。使用方法 | API参考 | 在线体验\\n\\n| 模型名称                | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                        |\\n|-------------------------|--------------|-------------|-------------|---------------|---------------|---------------------------------------|\\n| 模型名称                | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                        |\\n| qwen2-72b-instruct      | 131,072      | 128,000     | 6,144       | 0.004元       | 0.012元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2-57b-a14b-instruct | 65,536       | 63,488      | 6,144       | 0.0035元      | 0.007元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2-7b-instruct       | 131,072      | 128,000     | 6,144       | 0.001元       | 0.002元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2-1.5b-instruct     | 32,768       | 30,720      | 6,144       | 限时免费      | 限时免费      | 限时免费                              |\\n| qwen2-0.5b-instruct     | 32,768       | 30,720      | 6,144       | 限时免费      | 限时免费      | 限时免费                              |\\n\\n### Qwen1.5\\n\\n阿里云的通义千问1.5-开源版。使用方法 | API参考 | 在线体验\\n\\n| 模型名称          | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                        |\\n|-------------------|--------------|-------------|-------------|---------------|---------------|---------------------------------------|\\n| 模型名称          | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                        |\\n| qwen1.5-110b-chat | 32,000       | 30,000      | 8,000       | 0.007元       | 0.014元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen1.5-72b-chat  | 32,000       | 30,000      | 2,000       | 0.005元       | 0.01元        | 各100万Token有效期：百炼开通后180天内 |\\n| qwen1.5-32b-chat  | 32,000       | 30,000      | 2,000       | 0.0035元      | 0.007元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen1.5-14b-chat  | 8,000        | 6,000       | 2,000       | 0.002元       | 0.004元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen1.5-7b-chat   | 8,000        | 6,000       | 2,000       | 0.001元       | 0.002元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen1.5-1.8b-chat | 32,000       | 30,000      | 2,000       | 限时免费      | 限时免费      | 限时免费                              |\\n| qwen1.5-0.5b-chat | 32,000       | 30,000      | 2,000       | 限时免费      | 限时免费      | 限时免费                              |\\n\\n### Qwen\\n\\n阿里云的通义千问-开源版。使用方法 | API参考 | 在线体验\\n\\n| 模型名称                   | 上下文长度   | 最大输入    | 最大输出    | 输入成本           | 输出成本           | 免费额度（注）                        |\\n|----------------------------|--------------|-------------|-------------|--------------------|--------------------|---------------------------------------|\\n| 模型名称                   | （Token数）  | （Token数） | （Token数） | （每千Token）      | （每千Token）      | 免费额度（注）                        |\\n| qwen-72b-chat              | 32,000       | 30,000      | 2,000       | 0.02元             | 0.02元             | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-14b-chat              | 8,000        | 6,000       | 2,000       | 0.008元            | 0.008元            | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-7b-chat               | 7,500        | 6,000       | 1,500       | 0.006元            | 0.006元            | 各100万Token有效期：百炼开通后180天内 |\\n| qwen-1.8b-chat             | 8,000        | 6,000       | 2,000       | 限时免费           | 限时免费           | 限时免费                              |\\n| qwen-1.8b-longcontext-chat | 32,000       | 30,000      | 2,000       | 限时免费（需申请） | 限时免费（需申请） | 限时免费（需申请）                    |\\n\\n### QVQ\\n\\nqvq-72b-preview模型是由 Qwen 团队开发的实验性研究模型，专注于提升视觉推理能力，尤其在数学推理领域。qvq-72b-preview模型的局限性请参见QVQ官方博客。使用方法 | API参考\\n\\n| 模型名称        | 上下文长度   | 最大输入            | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                     |\\n|-----------------|--------------|---------------------|-------------|---------------|---------------|------------------------------------|\\n| 模型名称        | （Token数）  | （Token数）         | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                     |\\n| qvq-72b-preview | 32,768       | 16,384单图最大16384 | 16,384      | 0.012元       | 0.036元       | 10万Token有效期：百炼开通后180天内 |\\n\\n### Qwen-Omni\\n\\n基于Qwen2.5训练的全新多模态理解生成大模型，支持文本、图像、语音、视频输入理解，具备文本和语音同时流式生成的能力，多模态内容理解速度显著提升。使用方法｜API 参考\\n\\n| 模型名称        | 上下文长度   | 最大输入    | 最大输出    | 免费额度（注）                                    |\\n|-----------------|--------------|-------------|-------------|---------------------------------------------------|\\n| 模型名称        | （Token数）  | （Token数） | （Token数） | 免费额度（注）                                    |\\n| qwen2.5-omni-7b | 32,768       | 30,720      | 2,048       | 100万Token（不区分模态）有效期：百炼开通后180天内 |\\n\\n开源版模型的免费额度用完后，输入与输出的计费规则如下：\\n\\n### Qwen-VL\\n\\n阿里云的通义千问VL开源版。使用方法 | API参考\\n\\n其中，Qwen2.5-VL在Qwen2-VL的基础上做了如下改进：\\n\\n- 感知更丰富的世界：Qwen2.5-VL不仅擅长识别常见物体，如花、鸟、鱼和昆虫等，还能分析图像中的文本、图表、图标、图形和布局等。\\n- 长视频理解能力：支持对长视频文件（最长10分钟）进行理解，具备通过精准定位相关视频片段来捕捉事件的新能力\\n- 视觉定位：Qwen2.5-VL可通过生成bounding box（矩形框的左上角和右下角坐标）或者point（矩形框的中心点坐标）来准确定位图像中的物体，并能够为坐标和属性提供稳定的JSON输出。\\n- 结构化输出：可支持对发票、表单、表格等数据进行结构化输出，惠及金融、商业等领域的应用。\\n\\n| 模型名称                | 上下文长度   | 最大输入             | 最大输出    | 输入成本                                                     | 输出成本                                                     | 免费额度（注）                        |\\n|-------------------------|--------------|----------------------|-------------|--------------------------------------------------------------|--------------------------------------------------------------|---------------------------------------|\\n| 模型名称                | （Token数）  | （Token数）          | （Token数） | （每千Token）                                                | （每千Token）                                                | 免费额度（注）                        |\\n| qwen2.5-vl-72b-instruct | 131,072      | 129,024单图最大16384 | 8,192       | 0.016元                                                      | 0.048元                                                      | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2.5-vl-32b-instruct | 131,072      | 129,024单图最大16384 | 8,192       | 0.008元                                                      | 0.024元                                                      | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2.5-vl-7b-instruct  | 131,072      | 129,024单图最大16384 | 8,192       | 0.002元                                                      | 0.005元                                                      | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2.5-vl-3b-instruct  | 131,072      | 129,024单图最大16384 | 8,192       | 0.0012元                                                     | 0.0036元                                                     | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2-vl-72b-instruct   | 32,768       | 30,720单图最大16384  | 2,048       | 0.016元                                                      | 0.048元                                                      | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2-vl-7b-instruct    | 32,000       | 30,000单图最大16384  | 2,000       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内  |\\n| qwen2-vl-2b-instruct    | 32,000       | 30,000单图最大16384  | 2,000       | 限时免费                                                     | 限时免费                                                     | 各10万Token有效期：百炼开通后180天内  |\\n| qwen-vl-v1              | 8,000        | 6,000单图最大1280    | 1,500       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内  |\\n| qwen-vl-chat-v1         | 8,000        | 6,000单图最大1280    | 1,500       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内  |\\n\\n### Qwen-Audio\\n\\n阿里云的通义千问Audio开源版。使用方法\\n\\n| 模型名称                                                                          | 上下文长度   | 最大输入    | 最大输出    | 输入成本                                                     | 输出成本                                                     | 免费额度（注）                       |\\n|-----------------------------------------------------------------------------------|--------------|-------------|-------------|--------------------------------------------------------------|--------------------------------------------------------------|--------------------------------------|\\n| 模型名称                                                                          | （Token数）  | （Token数） | （Token数） | （每千Token）                                                | （每千Token）                                                | 免费额度（注）                       |\\n| qwen2-audio-instruct相比qwen-audio-chat提升了音频理解能力，且新增了语音聊天能力。 | 8,000        | 6,000       | 1,500       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\\n| qwen-audio-chat                                                                   | 8,000        | 6,000       | 1,500       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\\n\\n### Qwen-Math\\n\\n基于Qwen模型构建的专门用于数学解题的语言模型。Qwen2.5-Math相比Qwen2-Math有了实质性的改进。Qwen2.5-Math支持中文和英文，并整合了多种推理方法，包括CoT（Chain of Thought）、PoT（Program of Thought）和 TIR（Tool-Integrated Reasoning）。使用方法 | API参考| 在线体验\\n\\n| 模型名称                   | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                        |\\n|----------------------------|--------------|-------------|-------------|---------------|---------------|---------------------------------------|\\n| 模型名称                   | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                        |\\n| qwen2.5-math-72b-instruct  | 4,096        | 3,072       | 3,072       | 0.004元       | 0.012元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2.5-math-7b-instruct   | 4,096        | 3,072       | 3,072       | 0.001元       | 0.002元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2.5-math-1.5b-instruct | 4,096        | 3,072       | 3,072       | 限时免费体验  | 限时免费体验  | 限时免费体验                          |\\n| qwen2-math-72b-instruct    | 4,096        | 3,072       | 3,072       | 0.004元       | 0.012元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2-math-7b-instruct     | 4,096        | 3,072       | 3,072       | 0.001元       | 0.002元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2-math-1.5b-instruct   | 4,096        | 3,072       | 3,072       | 限时免费体验  | 限时免费体验  | 限时免费体验                          |\\n\\n### Qwen-Coder\\n\\n通义千问代码模型开源版。Qwen2.5-Coder相比CodeQwen1.5有了实质性的改进。Qwen2.5-Coder在包含5.5万亿Token的编程相关数据上进行了训练，使即使较小的编程专用模型也能在编程评估基准测试中表现出媲美大型语言模型的竞争力。API参考 | 在线体验\\n\\n| 模型名称                    | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                        |\\n|-----------------------------|--------------|-------------|-------------|---------------|---------------|---------------------------------------|\\n| 模型名称                    | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                        |\\n| qwen2.5-coder-32b-instruct  | 131,072      | 129,024     | 8,192       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2.5-coder-14b-instruct  | 131,072      | 129,024     | 8,192       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2.5-coder-7b-instruct   | 131,072      | 129,024     | 8,192       | 0.001元       | 0.002元       | 各100万Token有效期：百炼开通后180天内 |\\n| qwen2.5-coder-3b-instruct   | 32,768       | 30,720      | 8,192       | 限时免费体验  | 限时免费体验  | 限时免费体验                          |\\n| qwen2.5-coder-1.5b-instruct | 32,768       | 30,720      | 8,192       | 限时免费体验  | 限时免费体验  | 限时免费体验                          |\\n| qwen2.5-coder-0.5b-instruct | 32,768       | 30,720      | 8,192       | 限时免费体验  | 限时免费体验  | 限时免费体验                          |\\n\\n## 文本生成-第三方模型\\n\\n### DeepSeek\\n\\nDeepSeek-R1 在后训练阶段大规模使用了强化学习技术，在仅有极少标注数据的情况下，极大提升了模型推理能力，尤其在数学、代码、自然语言推理等任务上；DeepSeek-V3 为 MoE 模型，671B 参数，激活 37B，在 14.8T token 上进行了预训练，在长文本、代码、数学、百科、中文能力上表现优秀。API参考\\n\\n| 模型名称                                            | 上下文长度   | 最大输入    | 最大思维链长度   | 最大回复长度   | 输入成本                  | 输出成本                  | 免费额度查看剩余额度                  |\\n|-----------------------------------------------------|--------------|-------------|------------------|----------------|---------------------------|---------------------------|---------------------------------------|\\n| 模型名称                                            | （Token数）  | （Token数） | （Token数）      | （Token数）    | （每千Token）             | （每千Token）             | 免费额度查看剩余额度                  |\\n| deepseek-r1 671B 满血版模型                         | 65,792       | 57,344      | 32,768           | 8,192          | 0.004元Batch调用：0.002元 | 0.016元Batch调用：0.008元 | 各100万Token有效期：百炼开通后180天内 |\\n| deepseek-v3参数量为 671B                            | 65,792       | 57,344      | 不涉及           | 8,192          | 0.002元Batch调用：0.001元 | 0.008元Batch调用：0.004元 | 各100万Token有效期：百炼开通后180天内 |\\n| deepseek-r1-distill-qwen-1.5b基于 Qwen2.5-Math-1.5B | 32,768       | 32,768      | 16,384           | 16,384         | 限时免费体验              | 限时免费体验              | 限时免费体验                          |\\n| deepseek-r1-distill-qwen-7b基于 Qwen2.5-Math-7B     | 32,768       | 32,768      | 16,384           | 16,384         | 0.0005元                  | 0.001元                   | 各100万Token有效期：百炼开通后180天内 |\\n| deepseek-r1-distill-qwen-14b基于 Qwen2.5-14B        | 32,768       | 32,768      | 16,384           | 16,384         | 0.001元                   | 0.003元                   | 各100万Token有效期：百炼开通后180天内 |\\n| deepseek-r1-distill-qwen-32b基于 Qwen2.5-32B        | 32,768       | 32,768      | 16,384           | 16,384         | 0.002元                   | 0.006元                   | 各100万Token有效期：百炼开通后180天内 |\\n| deepseek-r1-distill-llama-8b基于 Llama-3.1-8B       | 32,768       | 32,768      | 16,384           | 16,384         | 限时免费体验              | 限时免费体验              | 限时免费体验                          |\\n| deepseek-r1-distill-llama-70b基于 Llama-3.3-70B     | 32,768       | 32,768      | 16,384           | 16,384         | 限时免费体验              | 限时免费体验              | 限时免费体验                          |\\n\\n### Llama-仅文本输入\\n\\nMeta推出的大语言模型，下列模型只支持输入文本。API参考 | 在线体验（需申请）\\n\\n| 模型名称               | 上下文长度   | 最大输入    | 输入输出成本                                                 | 免费额度（注）                                    |\\n|------------------------|--------------|-------------|--------------------------------------------------------------|---------------------------------------------------|\\n| 模型名称               | （Token数）  | （Token数） | 输入输出成本                                                 | 免费额度（注）                                    |\\n| llama3.3-70b-instruct  | 32,000       | 30,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\\n| llama3.2-3b-instruct   | 32,000       | 30,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\\n| llama3.2-1b-instruct   | 32,000       | 30,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\\n| llama3.1-405b-instruct | 32,000       | 30,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\\n| llama3.1-70b-instruct  | 32,000       | 30,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\\n| llama3.1-8b-instruct   | 32,000       | 30,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\\n| llama3-70b-instruct    | 8,000        | 8,000       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\\n| llama3-8b-instruct     | 8,000        | 8,000       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\\n| llama2-13b-chat-v2     | 4,000        | 4,000       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\\n| llama2-7b-chat-v2      | 4,000        | 4,000       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\\n\\n### Llama-文本和图像输入\\n\\nMeta推出的大语言模型，下列模型支持输入文本和图像。API参考 | 在线体验（需申请）\\n\\n| 模型名称                     | 上下文长度   | 输入输出成本                                                 | 免费额度（注）                                    |\\n|------------------------------|--------------|--------------------------------------------------------------|---------------------------------------------------|\\n| 模型名称                     | （Token数）  | 输入输出成本                                                 | 免费额度（注）                                    |\\n| llama3.2-90b-vision-instruct | 8192         | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\\n| llama3.2-11b-vision          | 8192         | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\\n\\n### 百川\\n\\n百川智能推出的大语言模型。API参考 | 在线体验（需申请）\\n\\n| 模型名称        | 上下文长度   | 最大输入    | 输入成本                                                     | 输出成本                                                     | 免费额度（注）                                |\\n|-----------------|--------------|-------------|--------------------------------------------------------------|--------------------------------------------------------------|-----------------------------------------------|\\n| 模型名称        | （Token数）  | （Token数） | （每千Token）                                                | （每千Token）                                                | 免费额度（注）                                |\\n| baichuan2-turbo | 32,000       | 32,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 100万Token（需申请）有效期：申请通过后180天内 |\\n\\n### 百川-开源版\\n\\n来自百川智能，该系列模型在平台中支持微调训练。API参考 | 在线体验（需申请）\\n\\n| 模型名称              | 上下文长度   | 最大输入    | 输入成本                                                     | 输出成本                                                     | 免费额度（注）                                  |\\n|-----------------------|--------------|-------------|--------------------------------------------------------------|--------------------------------------------------------------|-------------------------------------------------|\\n| 模型名称              | （Token数）  | （Token数） | （每千Token）                                                | （每千Token）                                                | 免费额度（注）                                  |\\n| baichuan2-13b-chat-v1 | 4096         | 4096        | 0.008元                                                      | 0.008元                                                      | 各100万Token（需申请）有效期：百炼开通后180天内 |\\n| baichuan2-7b-chat-v1  | 4096         | 4096        | 0.006元                                                      | 0.006元                                                      | 各100万Token（需申请）有效期：百炼开通后180天内 |\\n| baichuan-7b-v1        | 4096         | 4096        | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内 |\\n\\n### ChatGLM\\n\\n智谱AI推出的大语言模型。API参考 | 在线体验\\n\\n| 模型名称      | 上下文长度   | 最大输入    | 输入成本                                                     | 输出成本                                                     | 免费额度（注）                        |\\n|---------------|--------------|-------------|--------------------------------------------------------------|--------------------------------------------------------------|---------------------------------------|\\n| 模型名称      | （Token数）  | （Token数） | （每千Token）                                                | （每千Token）                                                | 免费额度（注）                        |\\n| chatglm3-6b   | 7500         | 7500        | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token有效期：百炼开通后180天内 |\\n| chatglm-6b-v2 | 6500         | 6500        | 0.006元                                                      | 0.006元                                                      | 各100万Token有效期：百炼开通后180天内 |\\n\\n### 零一万物\\n\\n零一万物推出的大语言模型。API参考 | 在线体验（需申请）\\n\\n| 模型名称                   | 上下文长度   | 最大输入    | 输入输出成本                                                 | 免费额度（注）                                  |\\n|----------------------------|--------------|-------------|--------------------------------------------------------------|-------------------------------------------------|\\n| 模型名称                   | （Token数）  | （Token数） | 输入输出成本                                                 | 免费额度（注）                                  |\\n| yi-large                   | 32,000       | 32,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：申请通过后180天内 |\\n| yi-medium                  | 32,000       | 32,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：申请通过后180天内 |\\n| yi-large-rag有实时联网能力 | 16,000       | 16,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：申请通过后180天内 |\\n| yi-large-turbo             | 16,000       | 16,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：申请通过后180天内 |\\n\\n### MiniMax\\n\\nMiniMax推出的大语言模型。API参考 | 在线体验（需申请）\\n\\n| 模型名称      | 说明             | 上下文长度   | 最大输入    | 输入输出成本                                                 | 免费额度（注）                                  |\\n|---------------|------------------|--------------|-------------|--------------------------------------------------------------|-------------------------------------------------|\\n| 模型名称      | 说明             | （Token数）  | （Token数） | 输入输出成本                                                 | 免费额度（注）                                  |\\n| abab6.5g-chat | 适合英文场景     | 8,000        | 8,000       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：申请通过后180天内 |\\n| abab6.5t-chat | 适合中文场景     | 8,000        | 8,000       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：申请通过后180天内 |\\n| abab6.5s-chat | 适合超长文本场景 | 245,000      | 245,000     | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：申请通过后180天内 |\\n\\n### 姜子牙\\n\\nIDEA研究院推出的大语言模型。API参考\\n\\n| 模型名称          | 输入输出成本       |\\n|-------------------|--------------------|\\n| ziya-llama-13b-v1 | 限时免费（需申请） |\\n\\n### BELLE\\n\\nBELLE推出的大语言模型。API参考\\n\\n| 模型名称              | 输入输出成本       |\\n|-----------------------|--------------------|\\n| belle-llama-13b-2m-v1 | 限时免费（需申请） |\\n\\n### 元语\\n\\n元语智能推出的大语言模型。API参考\\n\\n| 模型名称          | 输入输出成本       |\\n|-------------------|--------------------|\\n| chatyuan-large-v2 | 限时免费（需申请） |\\n\\n### BiLLa\\n\\nBiLLa是开源的推理能力增强的中英双语LLaMA模型，较大提升LLaMA的中文理解能力, 并尽可能减少对原始LLaMA英文能力的损伤。API参考\\n\\n| 模型名称        | 输入输出成本       |\\n|-----------------|--------------------|\\n| billa-7b-sft-v1 | 限时免费（需申请） |\\n\\n## 图像生成-通义万相与图像编辑\\n\\n### 文生图\\n\\n文生图V2版\\n\\n文生图V2系列模型是全面升级的文生图模型，您可以选择V2系列模型进行文生图创作。API参考 ｜ 在线体验\\n\\n| 模型名称          | 说明                                                            | 单价      | 免费额度（注）                   |\\n|-------------------|-----------------------------------------------------------------|-----------|----------------------------------|\\n| wanx2.1-t2i-plus  | 生成图像细节更丰富，速度较慢。对应通义万相官网2.1专业模型。     | 0.20元/张 | 各500张有效期：百炼开通后180天内 |\\n| wanx2.1-t2i-turbo | 生成速度快、效果全面、性价比高。对应通义万相官网2.1极速模型。   | 0.14元/张 | 各500张有效期：百炼开通后180天内 |\\n| wanx2.0-t2i-turbo | 擅长质感人像，速度中等、成本较低。对应通义万相官网2.0极速模型。 | 0.04元/张 | 各500张有效期：百炼开通后180天内 |\\n\\n| wanx2.1-t2i-plus                                                                                                                                                                                                                      | wanx2.1-t2i-turbo                                                                                                                                                                                                                     | wanx2.0-t2i-turbo                                                                                                                                                                                                                     |\\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| 场景1：文字生成能力提示词：生成一张新年祝福贺卡，背景有白雪，放鞭炮的小孩，蛇形成文案2025，并写上HAPPY NEW YEAR。效果对比：wanx2.1模型（plus和turbo）的文字生成能力更强，适合创意设计场景。                                           | 场景1：文字生成能力提示词：生成一张新年祝福贺卡，背景有白雪，放鞭炮的小孩，蛇形成文案2025，并写上HAPPY NEW YEAR。效果对比：wanx2.1模型（plus和turbo）的文字生成能力更强，适合创意设计场景。                                           | 场景1：文字生成能力提示词：生成一张新年祝福贺卡，背景有白雪，放鞭炮的小孩，蛇形成文案2025，并写上HAPPY NEW YEAR。效果对比：wanx2.1模型（plus和turbo）的文字生成能力更强，适合创意设计场景。                                           |\\n|                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                       |\\n| 场景2：人像生成能力提示词：中国女孩，圆脸，看着镜头，优雅的民族服装，商业摄影，室外，电影级光照，半身特写，精致的淡妆，锐利的边缘。效果对比：wanx2.0模型在质感人像生成方面表现出色，其成本仅为wanx2.1 turbo模型的三分之一，性价比高。 | 场景2：人像生成能力提示词：中国女孩，圆脸，看着镜头，优雅的民族服装，商业摄影，室外，电影级光照，半身特写，精致的淡妆，锐利的边缘。效果对比：wanx2.0模型在质感人像生成方面表现出色，其成本仅为wanx2.1 turbo模型的三分之一，性价比高。 | 场景2：人像生成能力提示词：中国女孩，圆脸，看着镜头，优雅的民族服装，商业摄影，室外，电影级光照，半身特写，精致的淡妆，锐利的边缘。效果对比：wanx2.0模型在质感人像生成方面表现出色，其成本仅为wanx2.1 turbo模型的三分之一，性价比高。 |\\n|                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                       |\\n\\n文生图V1版\\n\\n推荐您使用全面升级的文生图V2版模型。\\n\\n可以基于输入的文本生成图片。此外，还支持输入参考图片，并参考图片内容或者图片风格进行图片生成。API参考 | 在线体验\\n\\n| 模型名称   | 示例输入             | 示例输出   | 单价      | 免费额度（注）                 |\\n|------------|----------------------|------------|-----------|--------------------------------|\\n| wanx-v1    | 提示词：一只小狗在笑 |            | 0.16元/张 | 500张有效期：百炼开通后180天内 |\\n\\n### 通用图像编辑\\n\\n通义万相-通用图像编辑模型通过简单的指令即可实现多样化的图像编辑，适用于扩图、去水印、风格迁移、图像修复、图像美化等场景。API参考\\n\\n| 模型名称          | 计费单价   | 免费额度                                 |\\n|-------------------|------------|------------------------------------------|\\n| wanx2.1-imageedit | 0.14元/张  | 免费额度：500张有效期：百炼开通后180天内 |\\n\\n目前通用图像编辑支持以下功能：\\n\\n| 模型功能   | 输入图像                               | 输入提示词                                                 | 输出图像   |\\n|------------|----------------------------------------|------------------------------------------------------------|------------|\\n| 全局风格化 |                                        | 转换成法国绘本风格                                         |            |\\n| 局部风格化 |                                        | 把房子变成木板风格。                                       |            |\\n| 局部重绘   | 输入图像涂抹区域图像（白色为涂抹区域） | 一只陶瓷兔子抱着一朵陶瓷花。                               | 输出图像   |\\n| 去文字水印 |                                        | 去除图像中的文字。                                         |            |\\n| 扩图       |                                        | 一位绿色仙子。                                             |            |\\n| 图像超分   | 模糊图像                               | 图像超分。                                                 | 清晰图像   |\\n| 图像上色   |                                        | 蓝色背景，黄色的叶子。                                     |            |\\n| 线稿生图   |                                        | 北欧极简风格的客厅。                                       |            |\\n| 垫图       |                                        | 卡通形象小心翼翼地探出头，窥视着房间内一颗璀璨的蓝色宝石。 |            |\\n\\n### 涂鸦作画\\n\\n基于输入的手绘图加文字描述，即可生成精美的涂鸦绘画作品。API参考\\n\\n| 模型名称                  | 示例输入             | 示例输出   | 单价      | 免费额度（注）                 |\\n|---------------------------|----------------------|------------|-----------|--------------------------------|\\n| wanx-sketch-to-image-lite | 提示词：一棵参天大树 |            | 0.06元/张 | 500张有效期：百炼开通后180天内 |\\n\\n### 图像局部重绘\\n\\n根据用户输入的原始图片和局部涂抹图、prompt提示词文字内容，生成符合语义描述的多样化风格的局部重绘图像。API参考\\n\\n| 模型名称        | 示例输入                               | 示例输出   | 单价                                                         | 免费额度（注）                 |\\n|-----------------|----------------------------------------|------------|--------------------------------------------------------------|--------------------------------|\\n| wanx-x-painting | 布局涂抹图：提示词：一只狗戴着红色眼镜 |            | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：百炼开通后180天内 |\\n\\n### 人像风格重绘\\n\\n人像风格重绘可以将输入的人物图像进行多种风格化的重绘生成，使新生成的图像在兼顾原始人物相貌的同时，带来不同风格的绘画效果。API参考\\n\\n| 模型名称              | 示例输入       | 示例输出   | 单价      | 免费额度（注）                 |\\n|-----------------------|----------------|------------|-----------|--------------------------------|\\n| wanx-style-repaint-v1 | 风格：清雅国风 |            | 0.12元/张 | 500张有效期：百炼开通后180天内 |\\n\\n### 图像背景生成\\n\\n图像背景生成可以基于输入的前景图像素材拓展生成背景信息，实现自然的光影融合效果，与细腻的写实画面生成。支持文本描述、图像引导等多种方式，同时支持对生成的图像智能添加文字内容。API参考\\n\\n| 模型名称                      | 示例输入                                                         | 示例输出   | 单价      | 免费额度（注）                 |\\n|-------------------------------|------------------------------------------------------------------|------------|-----------|--------------------------------|\\n| wanx-background-generation-v2 | 提示词：在桌面上，旁边有插着花朵的花瓶，背后是纯色高级的背景墙。 |            | 0.08元/张 | 500张有效期：百炼开通后180天内 |\\n\\n### 图像画面扩展\\n\\n图像画面大模型，对输入图像进行画面自由扩展，支持旋转画面，支持按照扩展系数和扩展像素数两种方式进行扩图。用户可以通过指定宽度、高度画面扩展比例或者左、右、上、下的扩展的像素值来控制画面扩展，可用于创意娱乐、辅助作图、画面设计、影视后期制作等场景。API参考\\n\\n| 模型名称           | 示例输入   | 示例输出   | 单价                                                         | 免费额度（注）                 |\\n|--------------------|------------|------------|--------------------------------------------------------------|--------------------------------|\\n| image-out-painting |            |            | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：百炼开通后180天内 |\\n\\n### 人物实例分割\\n\\n输入人物图像，模型识别出图像中的不同人物对象并画出每个对象边界的像素级掩码。API参考\\n\\n| 模型名称                    | 示例输入   | 示例输出                                       | 单价                                                         | 免费额度（注）                 |\\n|-----------------------------|------------|------------------------------------------------|--------------------------------------------------------------|--------------------------------|\\n| image-instance-segmentation |            | 输出结果1：像素级掩码图像输出结果2：可视化图像 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：百炼开通后180天内 |\\n\\n### 图像擦除补全\\n\\n输入图像并指定待擦除区域掩码图像以及保留区域掩码图像，模型在保留原图背景的同时擦除指定图像区域。API参考\\n\\n针对人物图像的擦除、补全，推荐通过人物实例分割得到图像中不同人物对象的图像掩码，选择完整的人物图像掩码擦除一个或多个人物。\\n\\n| 模型名称               | 示例输入               | 示例输出   | 单价                                                         | 免费额度（注）                 |\\n|------------------------|------------------------|------------|--------------------------------------------------------------|--------------------------------|\\n| image-erase-completion | 原图待擦除区域保留区域 |            | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：百炼开通后180天内 |\\n\\n### 动漫人物生成\\n\\nCosplay动漫人物生成通过输入人像图片和卡通形象图片，可快速生成人物卡通写真。API参考\\n\\n| 模型名称              | 示例输入         | 示例输出   | 单价                                                         | 免费额度（注）                 |\\n|-----------------------|------------------|------------|--------------------------------------------------------------|--------------------------------|\\n| wanx-style-cosplay-v1 | 人脸图像模板图像 |            | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 300张有效期：百炼开通后180天内 |\\n\\n### 虚拟模特\\n\\n可以对上传的真人实拍商品展示图进行智能生成，将其中的模特和背景替换为心仪的内容，在保持人物姿态不变的情况下，使用虚拟模特对商品进行更加精美、多样的展示。支持各种与模特产生互动的商品，如手持小商品、服装、鞋靴、配饰等。API参考\\n\\n| 模型名称          | 版本   | 模型简介                                                                                     | 单价                                                         | 免费额度（注）                 |\\n|-------------------|--------|----------------------------------------------------------------------------------------------|--------------------------------------------------------------|--------------------------------|\\n| wanx-virtualmodel | V1     | 支持真人实拍图上传图片短边：512像素或1024像素                                                | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：百炼开通后180天内 |\\n| virtualmodel-v2   | V2     | 支持真人、人台实拍图上传图片短边为：1024像素或2048像素支持改变图片的长宽比文本引导效果更准确 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：百炼开通后180天内 |\\n\\n| 输入图    | 参数配置                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 输出图   |\\n|-----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|\\n| v1 真人图 | \"prompt\":\"一位年轻男性站着摆拍，在空荡的卧室里，窗户旁边，阳光照射进来，highly detailed，8K，极简主义风格\"\"face_prompt\":\"英俊的男性，脸好，脸美，质量上乘，杰作，（逼真度：1.4）\"\"predefined_face_id\":\"boy3\"                                                                                                                                                                                                                                                                                                                            | v1输出   |\\n| v2人台图  | \"prompt\":\"A woman stands beside a luxurious swimming pool, her attire and posture suggesting leisure and relaxation. The pool\\'s calm, crystal-clear waters reflect the surrounding opulent setting, with elegant lounge chairs inviting moments of repose under the sun. Perhaps it\\'s a high-end resort or an upscale private villa, where the tiled pool deck and meticulously landscaped greenery speak of exclusivity and refinement.\"\"face_prompt\":\"good face, beautiful face, best quality.\"\"aspect_ratio\":\"4:3\"\"realPerson\":false | v2输出   |\\n\\n### 鞋靴模特\\n\\n鞋靴模特支持输入多视角鞋靴系列图片，同时对输入模特模板图的鞋子区域进行鞋靴AI试穿，实现模特鞋靴布局重绘生成，最终生成图片的效果，布局自然、细节丰富、画面细腻、试穿结果逼真。可用于模特商品图设计、新鞋AI试穿、模特穿戴布局重绘等场景。API参考\\n\\n| 模型名称     | 示例输入   | 示例输出   | 单价                                                         | 免费额度（注）                 |\\n|--------------|------------|------------|--------------------------------------------------------------|--------------------------------|\\n| shoemodel-v1 |            |            | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：百炼开通后180天内 |\\n\\n### 创意海报生成\\n\\n根据您的要求自动生成海报的背景和文字排版，支持多种海报风格。无需设计基础，轻松制作出彩作品，让创意触手可及。API参考\\n\\n| 模型名称                  | 示例输入                                                                                                                                                                | 示例输出   | 单价                                                         | 免费额度（注）                 |\\n|---------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|--------------------------------------------------------------|--------------------------------|\\n| wanx-poster-generation-v1 | \"title\":\"元宵节\",\"sub_title\":\"正月十五\",\"body_text\":\"团圆时节，汤圆香甜，祝你幸福美满！\",\"prompt_text_zh\":\"灯笼，小猫，梅花\",\"wh_ratios\":\"竖版\",\"lora_name\":\"童话油画\", |            | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：百炼开通后180天内 |\\n\\n### 图配文\\n\\n您只要输入背景图和文字，就能将文字排版到图片上，形成一张完整的图文海报。API参考\\n\\n| 模型名称   | 示例输入   | 示例输入                                                                                                          | 示例输出   | 单价     |\\n|------------|------------|-------------------------------------------------------------------------------------------------------------------|------------|----------|\\n| wanx-ast   |            | \"title\":\"Lorem Ipsum\",\"subtitle\":\"Duis aute irure dolor in reprehenderit\",\"text\":\"VIEW NOW\",\"underlay\": 1,\"logo\": |            | 限时免费 |\\n\\n### 人物写真生成-FaceChain\\n\\n- 人物图像检测：对用户上传的人物图像进行检测，判断其中所包含的人脸是否符合Facechain微调所需的标准，检测维度包括人脸数量、大小、角度、光照、清晰度等多维度，支持图像组输入，并返回每张图像对应的检测结果。API参考\\n- 人物形象训练：对上传的图像进行模型训练，从而获得该图像中对应人物的resource，基于该resource可以实现人物的写真生成。API参考\\n- 人物写真生成：基于人物形象训练已经得到的形象，可以继续通过人物生成写真模型完成该形象的写真生成，支持多种预设风格，包括证件照、商务写真等。API参考\\n\\n| 模型名称             | 说明         | 示例输入       | 示例输出   | 单价      | 免费额度（注）                 |\\n|----------------------|--------------|----------------|------------|-----------|--------------------------------|\\n| facechain-facedetect | 人物图像检测 | 风格：商务写真 |            | 限时免费  | 限时免费                       |\\n| facechain-finetune   | 人物形象训练 | 风格：商务写真 |            | 2.5元/次  | 50次有效期：申请通过后180天内  |\\n| facechain-generation | 人物写真生成 | 风格：商务写真 |            | 0.18元/张 | 500张有效期：申请通过后180天内 |\\n\\n### 创意文字生成-WordArt锦书\\n\\n- 文字纹理生成：可以对输入的文字内容或文字图片进行创意设计，根据提示词内容对文字添加材质和纹理，实现立体凸显或场景融合的效果，生成效果精美、风格多样的艺术字，结合背景可以直接作为文字海报使用。API参考\\n- 文字变形：可以对输入的文字边缘轮廓进行创意变形，根据提示词内容进行边缘变化，实现一种字体的更多种创意用法，返回带有文字内容的黑底白色mask图。API参考\\n- 百家姓生成：可以输入姓氏文字进行创意设计，支持根据提示词和风格引导图进行自定义设计，同时提供多种精美的预设风格模板，生成图片可以应用于个性社交场景，如作为个人头像、屏幕壁纸、字体表情包等。API参考该模型将于2025年6月正式下线。请您在下线之前及时切换至其他模型，以免影响您的业务。\\n\\n| 模型名称         | 说明         | 示例输入                                           | 示例输出   | 单价      | 免费额度（注）                   |\\n|------------------|--------------|----------------------------------------------------|------------|-----------|----------------------------------|\\n| wordart-texture  | 文字纹理生成 | 提示词：精美玉石风格类型：立体材质                 |            | 0.08元/张 | 各500张有效期：百炼开通后180天内 |\\n| wordart-semantic | 文字变形     | 文字：桂林山水提示词：山峦叠嶂、漓江蜿蜒、岩石奇秀 |            | 0.24元/张 | 各500张有效期：百炼开通后180天内 |\\n| wordart-surnames | 百家姓生成   | 百家姓：沈风格：奇幻楼阁                           |            | 暂无      | 各500张有效期：百炼开通后180天内 |\\n\\n### AI试衣\\n\\n- AI试衣一款虚拟试衣图片生成模型，基于人像照片及服装图生成穿着后的试衣图片。API参考\\n- AI试衣-图片分割支持对模特图、服饰图进行分割，可用于AI试衣图片的前后处理。API参考\\n- AI试衣-图片精修是对AI试衣生成的效果图进行二次生成，输出还原度更高的精修试衣效果图。API参考\\n\\n| 模型名称           | 说明            | 示例输入   | 示例输出   | 免费额度（注）                   |\\n|--------------------|-----------------|------------|------------|----------------------------------|\\n| aitryon            | AI试衣          |            |            | 各400张有效期：百炼开通后180天内 |\\n| aitryon-parsing-v1 | AI试衣-图片分割 |            |            | 各400张有效期：百炼开通后180天内 |\\n| aitryon-refiner    | AI试衣-图片精修 |            |            | 100张有效期：百炼开通后180天内   |\\n\\nAI试衣计费单价\\n\\n| 模型服务        | 模型名称           | 计量单价   | 折扣   | 阶梯层级                     |\\n|-----------------|--------------------|------------|--------|------------------------------|\\n| AI试衣          | aitryon            | 0.20元/张  | 无     | 无                           |\\n| AI试衣-图片分割 | aitryon-parsing-v1 | 0.004元/张 | 无     | 无                           |\\n| AI试衣-图片精修 | aitryon-refiner    | 0.30元/张  | 无     | 生成数量 ≤ 25张              |\\n| AI试衣-图片精修 | aitryon-refiner    | 0.275元/张 | 9.2折  | 25张 ＜ 生成数量 ≤ 125张     |\\n| AI试衣-图片精修 | aitryon-refiner    | 0.25元/张  | 8.4折  | 125张 ＜ 生成数量 ≤ 250张    |\\n| AI试衣-图片精修 | aitryon-refiner    | 0.225元/张 | 7.5折  | 250张 ＜ 生成数量 ≤ 1250张   |\\n| AI试衣-图片精修 | aitryon-refiner    | 0.2元/张   | 6.7折  | 1250张 ＜ 生成数量 ≤ 2500张  |\\n| AI试衣-图片精修 | aitryon-refiner    | 0.175元/张 | 5.8折  | 2500张 ＜ 生成数量 ≤ 2.5万张 |\\n| AI试衣-图片精修 | aitryon-refiner    | 0.15元/张  | 5折    | 生成数量 ＞ 2.5万张          |\\n\\n## 图像生成-第三方模型\\n\\n### Stable Diffusion\\n\\nAPI参考\\n\\n| 模型名称                         | 说明                                                                                                                                                                                                                                              | 单价                                                         | 免费额度（注）                 |\\n|----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------|--------------------------------|\\n| stable-diffusion-3.5-large       | 具有8亿参数的多模态扩散变压器（MMDiT）文本到图像生成模型，具备卓越的图像质量和提示词匹配度，支持生成100万像素的高分辨率图像，且能够在普通消费级硬件上高效运行。相比于v1.5和xl，在图像质量、文本内容生成、复杂提示理解和资源效率方面均有显著提升。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：申请通过后180天内 |\\n| stable-diffusion-3.5-large-turbo | 在stable-diffusion-3.5-large的基础上采用对抗性扩散蒸馏（ADD）技术的模型，具备更快的速度。                                                                                                                                                         | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：申请通过后180天内 |\\n| stable-diffusion-xl              | 相比v1.5做了重大改进，被认为是当前开源文生图模型的SOTA水准，具体改进包括：unet backbone是之前的3倍；增加了refinement模块用于改善生成图片的质量；更高效的训练技巧等。                                                                              | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：申请通过后180天内 |\\n| stable-diffusion-v1.5            | 通过clip模型将文本的embedding和图片embedding映射到相同空间，从而通过输入文本并结合unet的稳定扩散预测噪声的能力，生成图片。是一款基础的文生图模型，得到了业界广泛使用。                                                                            | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：申请通过后180天内 |\\n\\n### FLUX\\n\\nBlack Forest Labs的开源文生图模型，尤其擅长生成包含文字、多主体、手部细节的图片。\\n\\nAPI详情 | 在线体验\\n\\n| 模型名称     | 说明                                                                                   | 单价                                                         | 免费额度（注）                  |\\n|--------------|----------------------------------------------------------------------------------------|--------------------------------------------------------------|---------------------------------|\\n| flux-merged  | 结合了flux-dev的深度和flux-schnell的快速执行。                                         | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 1000张有效期：百炼开通后180天内 |\\n| flux-dev     | 开发者版，面向非商业应用，具有与专业版相近的图像质量和指令遵循能力，同时运行效率更高。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 1000张有效期：百炼开通后180天内 |\\n| flux-schnell | 快速版，轻量级模型。                                                                   | 计费方案即将推出。                                           | 1000张有效期：百炼开通后180天内 |\\n\\n## 语音合成（文本转语音）\\n\\n### CosyVoice\\n\\nCosyVoice是通义实验室依托大规模预训练语言模型，深度融合文本理解和语音生成的新一代生成式语音合成大模型，支持文本至语音的实时流式合成。API参考 | 在线体验\\n\\n| 模型名称     | 单价                                                                                            | 免费额度                     |\\n|--------------|-------------------------------------------------------------------------------------------------|------------------------------|\\n| cosyvoice-v1 | 2元/万字符根据待合成字符数计费（其中1个汉字算2个字符，英文、标点符号、空格均按照1个字符计费）。 | 每主账号每模型每月2000字符。 |\\n\\n音色列表：\\n\\n| 模型名称     | voice参数     | 音色     | 音频试听   | 适用场景                                                     | 语言         |   默认采样率（Hz） | 默认音频格式   |\\n|--------------|---------------|----------|------------|--------------------------------------------------------------|--------------|--------------------|----------------|\\n| cosyvoice-v1 | longxiaochun  | 龙小淳   |            | 语音助手、导航播报、聊天数字人                               | 中文+英文    |              22050 | mp3            |\\n| cosyvoice-v1 | longxiaoxia   | 龙小夏   |            | 语音助手、聊天数字人                                         | 中文         |              22050 | mp3            |\\n| cosyvoice-v1 | longxiaocheng | 龙小诚   |            | 语音助手、导航播报、聊天数字人                               | 中文+英文    |              22050 | mp3            |\\n| cosyvoice-v1 | longxiaobai   | 龙小白   |            | 聊天数字人、有声书、语音助手                                 | 中文         |              22050 | mp3            |\\n| cosyvoice-v1 | longlaotie    | 龙老铁   |            | 新闻播报、有声书、语音助手、直播带货、导航播报               | 中文东北口音 |              22050 | mp3            |\\n| cosyvoice-v1 | longshu       | 龙书     |            | 有声书、语音助手、导航播报、新闻播报、智能客服               | 中文         |              22050 | mp3            |\\n| cosyvoice-v1 | longshuo      | 龙硕     |            | 语音助手、导航播报、新闻播报、客服催收                       | 中文         |              22050 | mp3            |\\n| cosyvoice-v1 | longjing      | 龙婧     |            | 语音助手、导航播报、新闻播报、客服催收                       | 中文         |              22050 | mp3            |\\n| cosyvoice-v1 | longmiao      | 龙妙     |            | 客服催收、导航播报、有声书、语音助手                         | 中文         |              22050 | mp3            |\\n| cosyvoice-v1 | longyue       | 龙悦     |            | 语音助手、诗词朗诵、有声书朗读、导航播报、新闻播报、客服催收 | 中文         |              22050 | mp3            |\\n| cosyvoice-v1 | longyuan      | 龙媛     |            | 有声书、语音助手、聊天数字人                                 | 中文         |              22050 | mp3            |\\n| cosyvoice-v1 | longfei       | 龙飞     |            | 会议播报、新闻播报、有声书                                   | 中文         |              22050 | mp3            |\\n| cosyvoice-v1 | longjielidou  | 龙杰力豆 |            | 新闻播报、有声书、聊天助手                                   | 中文+英文    |              22050 | mp3            |\\n| cosyvoice-v1 | longtong      | 龙彤     |            | 有声书、导航播报、聊天数字人                                 | 中文         |              22050 | mp3            |\\n| cosyvoice-v1 | longxiang     | 龙祥     |            | 新闻播报、有声书、导航播报                                   | 中文         |              22050 | mp3            |\\n| cosyvoice-v1 | loongstella   | Stella   |            | 语音助手、直播带货、导航播报、客服催收、有声书               | 中文+英文    |              22050 | mp3            |\\n| cosyvoice-v1 | loongbella    | Bella    |            | 语音助手、客服催收、新闻播报、导航播报                       | 中文         |              22050 | mp3            |\\n\\n### Sambert\\n\\nSambert语音合成API基于达摩院改良的自回归韵律模型，支持文本至语音的实时流式合成。API参考\\n\\n| 模型名称   | 单价                                                                                                                | 免费额度                    |\\n|------------|---------------------------------------------------------------------------------------------------------------------|-----------------------------|\\n| 见下表     | 1元/万字符根据待合成字符数计费（其中1个汉字算2个字符，英文、标点符号、空格均按照1个字符计费）。SSML标签内容不计费。 | 每主账号每模型每月3万字符。 |\\n\\n模型（音色）列表：\\n\\n| 模型名称               | 音色           | 音频试听   | 时间戳支持   | 适用场景                             | 特色         | 语言      | 默认采样率（Hz）   |\\n|------------------------|----------------|------------|--------------|--------------------------------------|--------------|-----------|--------------------|\\n| sambert-zhinan-v1      | 知楠           |            | 是           | 通用场景                             | 广告男声     | 中文+英文 | 48k                |\\n| sambert-zhiqi-v1       | 知琪           |            | 是           | 通用场景                             | 温柔女声     | 中文+英文 | 48k                |\\n| sambert-zhichu-v1      | 知厨           |            | 是           | 新闻播报                             | 舌尖男声     | 中文+英文 | 48k                |\\n| sambert-zhide-v1       | 知德           |            | 是           | 新闻播报                             | 新闻男声     | 中文+英文 | 48k                |\\n| sambert-zhijia-v1      | 知佳           |            | 是           | 新闻播报                             | 标准女声     | 中文+英文 | 48k                |\\n| sambert-zhiru-v1       | 知茹           |            | 是           | 新闻播报                             | 新闻女声     | 中文+英文 | 48k                |\\n| sambert-zhiqian-v1     | 知倩           |            | 是           | 配音解说、新闻播报                   | 资讯女声     | 中文+英文 | 48k                |\\n| sambert-zhixiang-v1    | 知祥           |            | 是           | 配音解说                             | 磁性男声     | 中文+英文 | 48k                |\\n| sambert-zhiwei-v1      | 知薇           |            | 是           | 阅读产品简介                         | 萝莉女声     | 中文+英文 | 48k                |\\n| sambert-zhihao-v1      | 知浩           |            | 是           | 通用场景                             | 咨询男声     | 中文+英文 | 16k                |\\n| sambert-zhijing-v1     | 知婧           |            | 是           | 通用场景                             | 严厉女声     | 中文+英文 | 16k                |\\n| sambert-zhiming-v1     | 知茗           |            | 是           | 通用场景                             | 诙谐男声     | 中文+英文 | 16k                |\\n| sambert-zhimo-v1       | 知墨           |            | 是           | 通用场景                             | 情感男声     | 中文+英文 | 16k                |\\n| sambert-zhina-v1       | 知娜           |            | 是           | 通用场景                             | 浙普女声     | 中文+英文 | 16k                |\\n| sambert-zhishu-v1      | 知树           |            | 是           | 通用场景                             | 资讯男声     | 中文+英文 | 16k                |\\n| sambert-zhistella-v1   | 知莎           |            | 是           | 通用场景                             | 知性女声     | 中文+英文 | 16k                |\\n| sambert-zhiting-v1     | 知婷           |            | 是           | 通用场景                             | 电台女声     | 中文+英文 | 16k                |\\n| sambert-zhixiao-v1     | 知笑           |            | 是           | 通用场景                             | 资讯女声     | 中文+英文 | 16k                |\\n| sambert-zhiya-v1       | 知雅           |            | 是           | 通用场景                             | 严厉女声     | 中文+英文 | 16k                |\\n| sambert-zhiye-v1       | 知晔           |            | 是           | 通用场景                             | 青年男声     | 中文+英文 | 16k                |\\n| sambert-zhiying-v1     | 知颖           |            | 是           | 通用场景                             | 软萌童声     | 中文+英文 | 16k                |\\n| sambert-zhiyuan-v1     | 知媛           |            | 是           | 通用场景                             | 知心姐姐     | 中文+英文 | 16k                |\\n| sambert-zhiyue-v1      | 知悦           |            | 是           | 客服                                 | 温柔女声     | 中文+英文 | 16k                |\\n| sambert-zhigui-v1      | 知柜           |            | 是           | 阅读产品简介                         | 直播女声     | 中文+英文 | 16k                |\\n| sambert-zhishuo-v1     | 知硕           |            | 是           | 数字人                               | 自然男声     | 中文+英文 | 16k                |\\n| sambert-zhimiao-emo-v1 | 知妙（多情感） |            | 是           | 阅读产品简介、数字人、直播           | 多种情感女声 | 中文+英文 | 16k                |\\n| sambert-zhimao-v1      | 知猫           |            | 是           | 阅读产品简介、配音解说、数字人、直播 | 直播女声     | 中文+英文 | 16k                |\\n| sambert-zhilun-v1      | 知伦           |            | 是           | 配音解说                             | 悬疑解说     | 中文+英文 | 16k                |\\n| sambert-zhifei-v1      | 知飞           |            | 是           | 配音解说                             | 激昂解说     | 中文+英文 | 16k                |\\n| sambert-zhida-v1       | 知达           |            | 是           | 新闻播报                             | 标准男声     | 中文+英文 | 16k                |\\n| sambert-camila-v1      | Camila         |            | 否           | 通用场景                             | 西班牙语女声 | 西班牙语  | 16k                |\\n| sambert-perla-v1       | Perla          |            | 否           | 通用场景                             | 意大利语女声 | 意大利语  | 16k                |\\n| sambert-indah-v1       | Indah          |            | 否           | 通用场景                             | 印尼语女声   | 印尼语    | 16k                |\\n| sambert-clara-v1       | Clara          |            | 否           | 通用场景                             | 法语女声     | 法语      | 16k                |\\n| sambert-hanna-v1       | Hanna          |            | 否           | 通用场景                             | 德语女声     | 德语      | 16k                |\\n| sambert-beth-v1        | Beth           |            | 是           | 通用场景                             | 咨询女声     | 美式英文  | 16k                |\\n| sambert-betty-v1       | Betty          |            | 是           | 通用场景                             | 客服女声     | 美式英文  | 16k                |\\n| sambert-cally-v1       | Cally          |            | 是           | 通用场景                             | 自然女声     | 美式英文  | 16k                |\\n| sambert-cindy-v1       | Cindy          |            | 是           | 通用场景                             | 对话女声     | 美式英文  | 16k                |\\n| sambert-eva-v1         | Eva            |            | 是           | 通用场景                             | 陪伴女声     | 美式英文  | 16k                |\\n| sambert-donna-v1       | Donna          |            | 是           | 通用场景                             | 教育女声     | 美式英文  | 16k                |\\n| sambert-brian-v1       | Brian          |            | 是           | 通用场景                             | 客服男声     | 美式英文  | 16k                |\\n| sambert-waan-v1        | Waan           |            | 否           | 通用场景                             | 泰语女声     | 泰语      | 16k                |\\n\\n## 语音识别（语音转文本）与翻译（语音转成指定语种的文本）\\n\\n### Gummy\\n\\nGummy大模型支持实时语音识别与翻译，能够精准识别中、英、日、韩等10种语言。此外，它还支持中、英、日、韩之间的互译，以及其他6种语言单向翻译成中文或英文。API参考\\n\\n| 模型名称          | 支持的语言                                                                                                                              | 支持的采样率   | 适用场景                                                   | 支持的音频格式                       | 单价         | 免费额度                                                                                                           |\\n|-------------------|-----------------------------------------------------------------------------------------------------------------------------------------|----------------|------------------------------------------------------------|--------------------------------------|--------------|--------------------------------------------------------------------------------------------------------------------|\\n| gummy-realtime-v1 | 中文、英文、日语、韩语、粤语、德语、法语、俄语、意大利语、西班牙语翻译语言对：中 → 英/日/韩英 → 中/日/韩日/韩/粤/德/法/俄/意/西 → 中/英 | 16kHz及以上    | 会议演讲、视频直播等长时间不间断识别的场景                 | pcm、wav、mp3、opus、speex、aac、amr | 0.00015元/秒 | 36,000秒（10小时）2025年1月17日0点前开通百炼：有效期至2025年7月15日2025年1月17日0点后开通百炼：自开通日起180天有效 |\\n| gummy-chat-v1     | 中文、英文、日语、韩语、粤语、德语、法语、俄语、意大利语、西班牙语翻译语言对：中 → 英/日/韩英 → 中/日/韩日/韩/粤/德/法/俄/意/西 → 中/英 | 16kHz          | 对话聊天、指令控制、语音输入法、语音搜索等短时语音交互场景 | pcm、wav、mp3、opus、speex、aac、amr | 0.00015元/秒 | 36,000秒（10小时）2025年1月17日0点前开通百炼：有效期至2025年7月15日2025年1月17日0点后开通百炼：自开通日起180天有效 |\\n\\n### Paraformer\\n\\n录音文件识别\\n\\nAPI参考 | 在线体验\\n\\n| 模型名称          | 支持的语言                                                                                                                                                                                                   | 支持的采样率   | 适用场景   | 支持的音频格式                                                                          | 单价         | 免费额度                                        |\\n|-------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|------------|-----------------------------------------------------------------------------------------|--------------|-------------------------------------------------|\\n| paraformer-v2     | 中文普通话、中文方言（粤语、吴语、闽南语、东北话、甘肃话、贵州话、河南话、湖北话、湖南话、宁夏话、山西话、陕西话、山东话、四川话、天津话、江西话、云南话、上海话）、英语、日语、韩语、德语、法语、俄语       | 任意           | 视频直播   | aac、amr、avi、flac、flv、m4a、mkv、mov、mp3、mp4、mpeg、ogg、opus、wav、webm、wma、wmv | 0.00008元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\\n| paraformer-8k-v2  | 中文普通话                                                                                                                                                                                                   | 8kHz           | 电话语音   | aac、amr、avi、flac、flv、m4a、mkv、mov、mp3、mp4、mpeg、ogg、opus、wav、webm、wma、wmv | 0.00008元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\\n| paraformer-v1     | 中文普通话、英语                                                                                                                                                                                             | 任意           | 音频或视频 | aac、amr、avi、flac、flv、m4a、mkv、mov、mp3、mp4、mpeg、ogg、opus、wav、webm、wma、wmv | 0.00008元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\\n| paraformer-8k-v1  | 中文普通话                                                                                                                                                                                                   | 8kHz           | 电话语音   | aac、amr、avi、flac、flv、m4a、mkv、mov、mp3、mp4、mpeg、ogg、opus、wav、webm、wma、wmv | 0.00008元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\\n| paraformer-mtl-v1 | 中文普通话、中文方言（粤语、吴语、闽南语、东北话、甘肃话、贵州话、河南话、湖北话、湖南话、宁夏话、山西话、陕西话、山东话、四川话、天津话）、英语、日语、韩语、西班牙语、印尼语、法语、德语、意大利语、马来语 | 16kHz及以上    | 音频或视频 | aac、amr、avi、flac、flv、m4a、mkv、mov、mp3、mp4、mpeg、ogg、opus、wav、webm、wma、wmv | 0.00008元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\\n\\n实时语音识别\\n\\nAPI参考 | 在线体验\\n\\n| 模型名称                  | 支持的语言                                                                                                                                                                                               | 支持的采样率   | 适用场景         | 支持的音频格式                       | 单价         | 免费额度                                        |\\n|---------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|------------------|--------------------------------------|--------------|-------------------------------------------------|\\n| paraformer-realtime-v2    | 中文普通话、中文方言（粤语、吴语、闽南语、东北话、甘肃话、贵州话、河南话、湖北话、湖南话、宁夏话、山西话、陕西话、山东话、四川话、天津话、江西话、云南话、上海话）、英语、日语、韩语支持多个语种自由切换 | 任意           | 视频直播、会议等 | pcm、wav、mp3、opus、speex、aac、amr | 0.00024元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\\n| paraformer-realtime-v1    | 中文                                                                                                                                                                                                     | 16kHz          | 视频直播、会议等 | pcm、wav、mp3、opus、speex、aac、amr | 0.00024元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\\n| paraformer-realtime-8k-v2 | 中文                                                                                                                                                                                                     | 8kHz           | 电话客服等       | pcm、wav、mp3、opus、speex、aac、amr | 0.00024元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\\n| paraformer-realtime-8k-v1 | 中文                                                                                                                                                                                                     | 8kHz           | 电话客服等       | pcm、wav、mp3、opus、speex、aac、amr | 0.00024元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\\n\\n### SenseVoice\\n\\n录音文件识别\\n\\n专注于高精度多语言语音识别，还能识别情绪（高兴、悲伤、生气等）和特定事件（背景音乐、歌唱、掌声和笑声等）。API参考\\n\\n| 模型名称      | 支持的语言                                             | 支持的格式                                                                                          | 单价         | 免费额度                                        |\\n|---------------|--------------------------------------------------------|-----------------------------------------------------------------------------------------------------|--------------|-------------------------------------------------|\\n| sensevoice-v1 | 超过50种语言（中、英、日、韩、粤等）附录：支持语言列表 | 音频或视频：aac、amr、avi、flac、flv、m4a、mkv、mov、mp3、mp4、mpeg、ogg、opus、wav、webm、wma、wmv | 0.0007 元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\\n\\n## 视频生成-通义万相与视频编辑\\n\\n### 文生视频\\n\\n通义万相-文生视频模型通过一句话即可生成视频，视频呈现丰富的艺术风格及影视级画质。API参考 ｜在线体验\\n\\n| 模型名称          | 说明                           | 单价      | 免费额度                         |\\n|-------------------|--------------------------------|-----------|----------------------------------|\\n| wanx2.1-t2v-turbo | 生成速度更快，表现均衡。       | 0.24元/秒 | 各200秒有效期：百炼开通后180天内 |\\n| wanx2.1-t2v-plus  | 生成细节更丰富，画面更具质感。 | 0.70元/秒 | 各200秒有效期：百炼开通后180天内 |\\n\\n| 输入示例                         | 输出视频   |\\n|----------------------------------|------------|\\n| 输入提示词：一只小猫在月光下奔跑 |            |\\n\\n### 图生视频\\n\\n通义万相-图生视频模型将输入图片作为视频首帧，再根据提示词生成视频。视频呈现丰富的艺术风格及影视级画质。API参考 ｜在线体验\\n\\n| 模型名称          | 说明                                                   | 单价      | 免费额度                         |\\n|-------------------|--------------------------------------------------------|-----------|----------------------------------|\\n| wanx2.1-i2v-turbo | 生成速度更快，耗时仅为plus模型的三分之一，性价比更高。 | 0.24元/秒 | 各200秒有效期：百炼开通后180天内 |\\n| wanx2.1-i2v-plus  | 生成细节更丰富，画面更具质感。                         | 0.70元/秒 | 各200秒有效期：百炼开通后180天内 |\\n\\n| 输入示例                                 | 输出视频                                                                          |\\n|------------------------------------------|-----------------------------------------------------------------------------------|\\n| 输入提示词：一只猫在草地上奔跑输入图片： | 输出视频：将图片作为视频的第一帧，再根据提示词生成视频。模型：wanx2.1-i2v-turbo。 |\\n\\n### 舞动人像AnimateAnyone\\n\\n基于人物图片和人物动作模板，生成人物动作视频。直接使用时需依次调用下述三个模型。AnimateAnyone图像检测 API详情 | AnimateAnyone 动作模板生成API详情｜ AnimateAnyone视频生成API详情\\n\\n| 模型名称                     | 说明                                       | 单价       | 免费额度                          |\\n|------------------------------|--------------------------------------------|------------|-----------------------------------|\\n| animate-anyone-detect-gen2   | 检测输入的图片是否符合要求                 | 0.004元/张 | 200张有效期：百炼开通后180天内    |\\n| animate-anyone-template-gen2 | 从人物运动视频中提取人物动作并生成动作模板 | 0.08元/秒  | 各1800秒有效期：百炼开通后180天内 |\\n| animate-anyone-gen2          | 基于人物图片和动作模板生成人物动作视频     | 0.08元/秒  | 各1800秒有效期：百炼开通后180天内 |\\n\\n下面两个模型支持独立部署。模型部署后，模型调用参考这两个API详情。AnimateAnyone图像检测 API详情 | AnimateAnyone视频生成API详情\\n\\n| 模型名称              | 说明                                   | 单价                                                                                  | 免费额度   |\\n|-----------------------|----------------------------------------|---------------------------------------------------------------------------------------|------------|\\n| animate-anyone-detect | 检测输入图片是否符合要求               | 当前仅支持部署后调用，仅收取部署费用。部署单价：10000元/算力单元/月20元/算力单元/小时 | 无         |\\n| animate-anyone        | 基于人物图片和动作模板生成人物动作视频 | 当前仅支持部署后调用，仅收取部署费用。部署单价：10000元/算力单元/月20元/算力单元/小时 | 无         |\\n\\n舞动人像模型效果示例\\n\\n| 输入：人物图片   | 输入：动作视频   | 输出（按图片背景生成）   | 输出（按视频背景生成）   |\\n|------------------|------------------|--------------------------|--------------------------|\\n|                  |                  |                          |                          |\\n\\n- 以上示例，由集成了“舞动人像AnimateAnyone”的通义APP生成。\\n- 舞动人像AnimateAnyone模型的生成内容为视频画面，不包含音频。\\n\\n### 悦动人像EMO\\n\\n基于人物肖像图片和人声音频文件，生成人物肖像动态视频。使用时需依次调用下述模型。EMO 图像检测API详情 | EMO 视频生成API详情\\n\\n| 模型名称      | 说明                                               | 单价                                                               | 免费额度                        |\\n|---------------|----------------------------------------------------|--------------------------------------------------------------------|---------------------------------|\\n| emo-detect-v1 | 检测输入的图片是否符合要求，不需要部署，可直接调用 | 0.004元/张                                                         | 200张有效期：百炼开通后180天内  |\\n| emo-v1        | 生成人物肖像动态视频，不需要部署，可直接调用       | 生成1:1画幅视频：0.08元/秒生成3:4画幅视频：0.16元/秒               | 1800秒有效期：百炼开通后180天内 |\\n| emo-detect    | 检测输入的图片是否符合要求，仅支持部署后调用       | 当前仅支持部署后调用，仅收取部署费用。部署单价：20元/算力单元/小时 | 无                              |\\n| emo           | 生成人物肖像动态视频，仅支持部署后调用             | 当前仅支持部署后调用，仅收取部署费用。部署单价：20元/算力单元/小时 | 无                              |\\n\\n| 输入物：人物肖像图片+人声音频文件   | 输出物：人物肖像动态视频                                    |\\n|-------------------------------------|-------------------------------------------------------------|\\n| 人物肖像：人声音频：参见右侧视频    | 人物视频：使用动作风格强度：活泼（\"style_level\": \"active\"） |\\n\\n### 灵动人像LivePortrait\\n\\n基于人物肖像图片和人声音频文件，快速、轻量地生成人物肖像动态视频。与悦动人像EMO模型相比，生成速度快、价格低，但是生成效果不如悦动人像EMO模型。使用时需依次调用下述两个模型。LivePortrait 图像检测API详情 | LivePortrait 视频生成API详情\\n\\n| 模型名称            | 说明                       | 单价       | 免费额度                        |\\n|---------------------|----------------------------|------------|---------------------------------|\\n| liveportrait-detect | 检测输入的图片是否符合要求 | 0.004元/张 | 200张有效期：百炼开通后180天内  |\\n| liveportrait        | 生成人物肖像动态视频       | 0.02元/秒  | 1800秒有效期：百炼开通后180天内 |\\n\\n| 输入物：人物肖像图片+人声音频文件   | 输出物：人物肖像动态视频   |\\n|-------------------------------------|----------------------------|\\n| 人物肖像：人声音频：参见右侧视频    | 人物视频：                 |\\n\\n### 表情包Emoji\\n\\n基于人脸图片和预设的人脸动态模板，生成人脸动态视频。该能力可用于表情包制作、视频素材生成等场景。使用时需依次调用下述模型。Emoji 图像检测API详情 ｜ Emoji 视频生成API详情\\n\\n| 模型名称        | 说明                                               | 单价       | 免费额度                       |\\n|-----------------|----------------------------------------------------|------------|--------------------------------|\\n| emoji-detect-v1 | 检测输入图片是否符合要求                           | 0.004元/张 | 200张有效期：百炼开通后180天内 |\\n| emoji-v1        | 基于人物肖像图片和指定的表情包模板生成人物同款表情 | 0.08元/秒  | 500秒有效期：百炼开通后180天内 |\\n\\n| 输入：人物肖像图片   | 输出：人物肖像动态视频                                       |\\n|----------------------|--------------------------------------------------------------|\\n|                      | “开心”表情的模板序列：（\"input.driven_id\": \"mengwa_kaixin\"） |\\n\\n### 声动人像VideoRetalk\\n\\n基于人物视频和人声音频，生成人物讲话口型与输入音频相匹配的视频。使用时需调用下述模型。VideoRetalk视频生成API详情\\n\\n| 模型名称    | 说明                                     | 单价      | 免费额度                        |\\n|-------------|------------------------------------------|-----------|---------------------------------|\\n| videoretalk | 生成人物讲话口型与输入音频相匹配的新视频 | 0.08元/秒 | 1800秒有效期：百炼开通后180天内 |\\n\\n### 视频风格重绘\\n\\n支持根据用户输入的文字内容，生成符合语义描述的不同风格的视频，或者根据用户输入的视频，进行视频风格重绘。API参考\\n\\n| 模型名称              | 说明         | 计费     | 免费额度   |\\n|-----------------------|--------------|----------|------------|\\n| video-style-transform | 视觉风格重绘 | 限时免费 | 限时免费   |\\n\\n模型效果示例\\n\\n| 原始视频（输入）   | 日式漫画（输出）   |\\n|--------------------|--------------------|\\n|                    |                    |\\n\\n## 文本向量\\n\\n文本向量模型用于将文本转换成一组可以代表文字的数字，适用于搜索、聚类、推荐、分类任务。模型根据输入Token数计费。同步接口API详情 | 批处理接口API详情\\n\\n## 公共云\\n\\n| 模型名称                | 向量维度                              | 最大行数   | 单行最大处理Token数   | 支持语种                                                                              | 单价（每千输入Token）        | 免费额度（注）                         |\\n|-------------------------|---------------------------------------|------------|-----------------------|---------------------------------------------------------------------------------------|------------------------------|----------------------------------------|\\n| text-embedding-v3       | 1,024（默认）、768、512、256、128或64 | 10         | 8,192                 | 中文、英语、西班牙语、法语、葡萄牙语、印尼语、日语、韩语、德语、俄罗斯语等50+主流语种 | 0.0005元Batch调用：0.00025元 | 各50万Token有效期：百炼开通后180天内   |\\n| text-embedding-v2       | 1,536                                 | 25         | 2,048                 | 中文、英语、西班牙语、法语、葡萄牙语、印尼语、日语、韩语、德语、俄罗斯语              | 0.0007元Batch调用：0.00035元 | 各50万Token有效期：百炼开通后180天内   |\\n| text-embedding-v1       | 1,536                                 | 25         | 2,048                 | 中文、英语、西班牙语、法语、葡萄牙语、印尼语                                          | 0.0007元Batch调用：0.00035元 | 各50万Token有效期：百炼开通后180天内   |\\n| text-embedding-async-v2 | 1,536                                 | 100,000    | 2,048                 | 中文、英语、西班牙语、法语、葡萄牙语、印尼语、日语、韩语、德语、俄罗斯语              | 0.0007元                     | 各2000万Token有效期：百炼开通后180天内 |\\n| text-embedding-async-v1 | 1,536                                 | 100,000    | 2,048                 | 中文、英语、西班牙语、法语、葡萄牙语、印尼语                                          | 0.0007元                     | 各2000万Token有效期：百炼开通后180天内 |\\n\\n## 金融云\\n\\n| 模型名称          | 向量维度               |   最大行数 | 单行最大处理Token数   | 支持语种                                                                              | 单价（每千输入Token）   | 免费额度（注）                     |\\n|-------------------|------------------------|------------|-----------------------|---------------------------------------------------------------------------------------|-------------------------|------------------------------------|\\n| text-embedding-v3 | 1024（默认）、768或512 |         10 | 8,192                 | 中文、英语、西班牙语、法语、葡萄牙语、印尼语、日语、韩语、德语、俄罗斯语等50+主流语种 | 0.00133元               | 50万Token有效期：百炼开通后180天内 |\\n\\n模型升级概述\\n\\n1. text-embedding-v2\\n2. text-embedding-v2\\n    - 语种扩充：新增对日语、韩语、德语、俄罗斯语的文本向量化能力。\\n    - 效果提升：优化了预训练模型和SFT策略，提升了整体效果，公开数据评测结果显示有显著改进。\\n3. text-embedding-v3\\n4. text-embedding-v3\\n    - 语种扩充：支持意大利语、波兰语、越南语、泰语等语种，语种数量增加至50余种。\\n    - 输入长度扩展：最大输入长度从2048 Token扩展至8192 Token。\\n    - 连续向量维度自定义：允许用户选择1024、768、512、256、128或64维度，默认维度为1024。\\n    - 不再区分Query/Document类型：简化输入，text\\\\_type参数不再需要指定文本类型。\\n    - Sparse向量支持：支持在接口中指定输出稠密向量和离散向量。\\n    - 效果提升：进一步优化预训练模型和SFT策略，提升整体效果，公开数据评测结果显示效果更佳。\\n\\nv1、v2、v3模型的效果数据\\n\\n| 模型                          |   MTEB |   MTEB（Retrieval task） |   CMTEB |   CMTEB (Retrieval task) |\\n|-------------------------------|--------|--------------------------|---------|--------------------------|\\n| text-embedding-v1             |  58.3  |                    45.47 |   59.84 |                    56.59 |\\n| text-embedding-v2             |  60.13 |                    49.49 |   62.17 |                    62.78 |\\n| text-embedding-v3（64维度）   |  57.4  |                    46.52 |   59.19 |                    62.03 |\\n| text-embedding-v3（128维度）  |  60.19 |                    52.51 |   63.81 |                    68.22 |\\n| text-embedding-v3（256维度）  |  61.13 |                    54.41 |   65.92 |                    71.07 |\\n| text-embedding-v3（512维度）  |  62.11 |                    54.3  |   66.81 |                    71.88 |\\n| text-embedding-v3（768维度）  |  62.43 |                    54.74 |   67.9  |                    72.29 |\\n| text-embedding-v3（1024维度） |  63.39 |                    55.41 |   68.92 |                    73.23 |\\n\\nMTEB（大规模文本嵌入评测基准）和CMTEB（中文大规模文本嵌入评测基准）采用0-100分制评估模型性能，数值越高代表效果越优。总分通过综合分类、聚类、检索等任务反映模型通用性，Retrieval Task分数用于衡量检索任务（如文档搜索）的精度，分数越高检索效果越强。\\n\\n## 多模态向量\\n\\n多模态向量模型将文本、图像或视频转换成一组由浮点数组成的向量，适用于视频分类、图像分类、图文检索等。API参考\\n\\n| 模型名称                | 数据类型   | 向量维度   | 单价     | 免费额度（注）   | 限流                       |\\n|-------------------------|------------|------------|----------|------------------|----------------------------|\\n| multimodal-embedding-v1 | float(32)  | 1,024      | 免费试用 | 无加权条目数限制 | 每分钟调用限制（RPM）：120 |\\n\\n## 文本分类、抽取、排序\\n\\n### OpenNLU\\n\\n针对给定的文本（中文或英文）进行信息抽取或文本分类。模型根据输出Token数计费。API参考\\n\\n| 模型名称   | 最大输入Token数   | 单价（每千Token）   | 免费额度（注）                      |\\n|------------|-------------------|---------------------|-------------------------------------|\\n| opennlu-v1 | 1,024             | 0.00465元           | 100万Token有效期：百炼开通后180天内 |\\n\\n### 文本排序模型\\n\\n通常用于语义检索，即给定查询 (Query) 和一系列候选文本 (Documents)，会根据与查询的语义相关性从高到低对候选文本进行排序。API参考\\n\\n| 模型名称      |   最大Document数量 | 单行最大输入Token   | 最大输入Token   | 支持语言                                                        | 单价     | 免费额度                            |\\n|---------------|--------------------|---------------------|-----------------|-----------------------------------------------------------------|----------|-------------------------------------|\\n| gte-rerank-v2 |                500 | 4,000               | 30,000          | 中、英、日、韩、泰语、西、法、葡、德、印尼语、阿拉伯语等50+语种 | 限时免费 | 100万Token有效期：百炼开通后180天内 |\\n| gte-rerank    |                500 | 4,000               | 30,000          | 中、英、日、韩、泰语、西、法、葡、德、印尼语、阿拉伯语等50+语种 | 限时免费 | 100万Token有效期：百炼开通后180天内 |\\n\\n- 单行最大输入Token：每个Query或Document的最大Token数量为4,000。如果输入内容超过此长度，将会被截断。\\n- 最大候选文本数量：每次请求中Document的最大数量为500。\\n- 最大输入Token：每次请求中所有Query和Document的Token总数不得超过30,000。\\n\\n## 行业\\n\\n### 通义法睿\\n\\n适用于回答法律问题、推荐裁判类案、辅助案情分析、生成法律文书、检索法律知识、审查合同条款等。API参考 | 在线体验\\n\\n| 模型名称   | 上下文长度   | 最大输入    | 最大输出   | 输入成本      | 输出成本      |\\n|------------|--------------|-------------|------------|---------------|---------------|\\n| 模型名称   | （Token数）  | （Token数） |            | （每千Token） | （每千Token） |\\n| farui-plus | 12k          | 12k         | 2k         | 0.02元        | 0.02元        |\\n\\n### 意图理解\\n\\n通义意图理解模型，能够在百毫秒级时间内快速、准确地解析用户意图，并选择合适工具来解决用户问题。API参考｜使用方法\\n\\n| 模型名称                | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                      |\\n|-------------------------|--------------|-------------|-------------|---------------|---------------|-------------------------------------|\\n| 模型名称                | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                      |\\n| tongyi-intent-detect-v3 | 8,192        | 8,192       | 1,024       | 0.0004元      | 0.001元       | 100万Token有效期：百炼开通后180天内 |\\n\\n该文章对您有帮助吗？\\n\\n### 为什么选择阿里云\\n\\n### 产品和定价\\n\\n### 解决方案\\n\\n### 文档与社区\\n\\n### 权益中心\\n\\n### 支持与服务\\n\\n### 关注阿里云\\n\\n关注阿里云公众号或下载阿里云APP，关注云资讯，随时随地运维管控云服务\\n\\n<!-- image -->\\n\\n### 友情链接\\n\\n© 2009-2025 Aliyun.com 版权所有 增值电信业务经营许可证： 浙B2-20080101 域名注册服务机构许可： 浙D3-20210002\\n\\n浙公网安备 33010602009975号浙B2-20080101-4'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = converter.convert(\n",
    "    \"https://help.aliyun.com/zh/model-studio/user-guide/models\"\n",
    ")\n",
    "result.document.export_to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5c43e76-a4e6-40dd-acab-188aac18e1b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 模型列表\n",
      "\n",
      "百炼提供了丰富多样的模型选择，它集成了通义系列大模型和第三方大模型，涵盖文本、图像、音视频等不同模态。\n",
      "\n",
      "## 旗舰模型\n",
      "\n",
      "| 旗舰模型                  |  通义千问-Max适合复杂任务，能力最强   |  通义千问-Plus效果、速度、成本均衡   |  通义千问-Turbo适合简单任务，速度快、成本极低   |  通义千问-Long适合大规模文本分析，效果与速度均衡、成本较低   |\n",
      "|---------------------------|---------------------------------------|--------------------------------------|-------------------------------------------------|--------------------------------------------------------------|\n",
      "| 最大上下文长度（Token数） | 32,768                                | 131,072                              | 1,000,000                                       | 10,000,000                                                   |\n",
      "| 最低输入价格（每千Token） | 0.0024元                              | 0.0008元                             | 0.0003元                                        | 0.0005元                                                     |\n",
      "| 最低输出价格（每千Token） | 0.0096元                              | 0.002元                              | 0.0006元                                        | 0.002元                                                      |\n",
      "\n",
      "## 模型总览\n",
      "\n",
      "| 类别           | 模型          | 说明                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|----------------|---------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| 文本生成       | 通义千问      | 通义千问大语言模型：商业版（QwQ、通义千问-Max、通义千问-Plus、通义千问-Turbo）、开源版（QwQ、Qwen2.5、Qwen2、Qwen1.5、Qwen）、超长文档模型通义千问-Long多模态模型：视觉理解模型通义千问VL、音频理解模型通义千问Audio、全模态模型通义千问Omni数学模型：通义千问数学模型代码模型：通义千问Coder翻译模型：通义千问翻译模型                                            |\n",
      "| 文本生成       | 第三方模型    | DeepSeek、Llama、百川、ChatGLM、零一万物等。                                                                                                                                                                                                                                                                                                                       |\n",
      "| 图像生成       | 通用文生图    | 可生成图像或编辑图像，适用于生成证件照、电商主图、模特图、各种风格人像图（动漫、国风、二次元等），也可用于抠图、生成背景、更改图片元素等。                                                                                                                                                                                                                         |\n",
      "| 图像生成       | 图像编辑      | 通用图像编辑：集合多个功能于一身，如去水印、图像风格化、图像修复等。涂鸦作画人像风格重绘：图像画面扩展人物实例分割、图像擦除补全                                                                                                                                                                                                                                   |\n",
      "| 图像生成       | 素材创作      | 电商素材创作：虚拟模特、鞋靴模特、ai试衣、图像背景背景生成：商品主图背景生海报创作：创意海报生成、创意文字生成人物创作：人物写真生成                                                                                                                                                                                                                               |\n",
      "| 图像生成       | 第三方模型    | Stable Diffusion和FLUX。                                                                                                                                                                                                                                                                                                                                           |\n",
      "| 语音合成与识别 | 语音合成      | CosyVoice和Sambert可实现文本转语音，适用于智能语音客服、有声读物、车载导航、教育辅导等场景。                                                                                                                                                                                                                                                                       |\n",
      "| 语音合成与识别 | 语音识别/翻译 | Gummy、Paraformer和SenseVoice可实现语音转文本，适用于实时会议记录、实时直播字幕、电话客服等场景。此外，Gummy还支持语音翻译。                                                                                                                                                                                                                                       |\n",
      "| 视频编辑与生成 | 文生视频      | 文生视频：一句话生成视频，视频风格丰富，画质细腻。                                                                                                                                                                                                                                                                                                                 |\n",
      "| 视频编辑与生成 | 图生视频      | 图生视频：将输入图片作为视频首帧，并根据提示词生成视频。图+动作模板生成舞蹈视频：舞动人像AnimateAnyone基于人物图片和动作视频生成舞蹈视频。图+音频生成对口型视频悦动人像EMO基于人物图片和音频，适合唱演场景。灵动人像LivePortrait基于人物图片和音频，适合语音播报场景。图+表情模板生成表情包视频：表情包Emoji基于人脸图片和预设的人脸动态模板，生成人脸表情包视频。 |\n",
      "| 视频编辑与生成 | 视频编辑      | 视频口型替换：声动人像VideoRetalk基于人物视频和音频，适合短视频制作、视频翻译等场景。视频风格转换：视频风格重绘可将视频转换为日式漫画、美式漫画等风格。                                                                                                                                                                                                            |\n",
      "| 向量           | 文本向量      | 将文本转换成一组可以代表文字的数字，用于搜索、聚类、推荐、分类等。                                                                                                                                                                                                                                                                                                 |\n",
      "| 向量           | 多模态向量    | 将文本、图像、语音转换成一组数字，用于音视频分类、图像分类、图文检索等。                                                                                                                                                                                                                                                                                           |\n",
      "| 行业           | 通义法睿      | 适用于法律咨询、案例分析和法规解读等。                                                                                                                                                                                                                                                                                                                             |\n",
      "| 行业           | 意图理解      | 意图理解模型能够在毫秒级时间内解析用户意图，并选择合适工具来解决用户问题。                                                                                                                                                                                                                                                                                         |\n",
      "\n",
      "## 文本生成-通义千问\n",
      "\n",
      "以下是通义千问模型的商业版。相较于开源版，商业版具有最新的能力和改进。\n",
      "\n",
      "### QwQ\n",
      "\n",
      "基于 Qwen2.5 模型训练的 QwQ 推理模型，通过强化学习大幅度提升了模型推理能力。模型数学代码等核心指标（AIME 24/25、LiveCodeBench）以及部分通用指标（IFEval、LiveBench等）达到DeepSeek-R1 满血版水平。使用方法\n",
      "\n",
      "| 模型名称                             | 版本   | 上下文长度   | 最大输入    | 最大思维链长度   | 最大回复长度   | 输入成本                    | 输出成本                  | 免费额度（注）                         |\n",
      "|--------------------------------------|--------|--------------|-------------|------------------|----------------|-----------------------------|---------------------------|----------------------------------------|\n",
      "| 模型名称                             | 版本   | （Token数）  | （Token数） | （Token数）      | （Token数）    | （每千Token）               | （每千Token）             | 免费额度（注）                         |\n",
      "| qwq-plus当前等同 qwq-plus-2025-03-05 | 稳定版 | 131,072      | 98,304      | 32,768           | 8,192          | 0.0016元Batch调用：0.0008元 | 0.004元Batch调用：0.002元 | 各100万 Token有效期：百炼开通后180天内 |\n",
      "| qwq-plus-latest始终等同最新快照版    | 最新版 | 131,072      | 98,304      | 32,768           | 8,192          | 0.0016元                    | 0.004元                   | 各100万 Token有效期：百炼开通后180天内 |\n",
      "| qwq-plus-2025-03-05又称qwq-plus-0305 | 快照版 | 131,072      | 98,304      | 32,768           | 8,192          | 0.0016元                    | 0.004元                   | 各100万 Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### 通义千问-Max\n",
      "\n",
      "通义千问系列效果最好的模型，适合复杂、多步骤的任务。使用方法 | API参考 | 在线体验\n",
      "\n",
      "#### 公共云\n",
      "\n",
      "| 模型名称                                          | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本                    | 输出成本                    | 免费额度（注）                        |\n",
      "|---------------------------------------------------|--------|--------------|-------------|-------------|-----------------------------|-----------------------------|---------------------------------------|\n",
      "| 模型名称                                          | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token）               | （每千Token）               | 免费额度（注）                        |\n",
      "| qwen-max当前等同qwen-max-2024-09-19               | 稳定版 | 32,768       | 30,720      | 8,192       | 0.0024元Batch调用：0.0012元 | 0.0096元Batch调用：0.0048元 | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-max-latest始终等同最新快照版                 | 最新版 | 32,768       | 30,720      | 8,192       | 0.0024元Batch调用：0.0012元 | 0.0096元Batch调用：0.0048元 | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-max-2025-01-25又称qwen-max-0125、Qwen2.5-Max | 快照版 | 32,768       | 30,720      | 8,192       | 0.0024元                    | 0.0096元                    | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-max-2024-09-19又称qwen-max-0919              | 快照版 | 32,768       | 30,720      | 8,192       | 0.02元                      | 0.06元                      | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-max-2024-04-28又称qwen-max-0428              | 快照版 | 8,000        | 6,000       | 2,000       | 0.04元                      | 0.12元                      | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-max-2024-04-03又称qwen-max-0403              | 快照版 | 8,000        | 6,000       | 2,000       | 0.04元                      | 0.12元                      | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-max-2024-01-07又称qwen-max-0107              | 快照版 | 8,000        | 6,000       | 2,000       | 0.04元                      | 0.12元                      | 各100万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "#### 金融云\n",
      "\n",
      "| 模型名称   | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本                  | 输出成本                  | 免费额度                            |\n",
      "|------------|--------|--------------|-------------|-------------|---------------------------|---------------------------|-------------------------------------|\n",
      "| 模型名称   | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token）             | （每千Token）             | 免费额度                            |\n",
      "| qwen-max   | 稳定版 | 8,000        | 6,000       | 2,000       | 0.038元Batch调用：0.019元 | 0.114元Batch调用：0.057元 | 100万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### 通义千问-Plus\n",
      "\n",
      "能力均衡，推理效果、成本和速度介于通义千问-Max和通义千问-Turbo之间，适合中等复杂任务。使用方法 | API参考 | 在线体验\n",
      "\n",
      "#### 公共云\n",
      "\n",
      "| 模型名称                               | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本                    | 输出成本                  | 免费额度（注）                        |\n",
      "|----------------------------------------|--------|--------------|-------------|-------------|-----------------------------|---------------------------|---------------------------------------|\n",
      "| 模型名称                               | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token）               | （每千Token）             | 免费额度（注）                        |\n",
      "| qwen-plus当前等同qwen-plus-2025-01-25  | 稳定版 | 131,072      | 129,024     | 8,192       | 0.0008元Batch调用：0.0004元 | 0.002元Batch调用：0.001元 | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-plus-latest始终等同最新快照版     | 最新版 | 131,072      | 129,024     | 8,192       | 0.0008元Batch调用：0.0004元 | 0.002元Batch调用：0.001元 | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-plus-2025-01-25又称qwen-plus-0125 | 快照版 | 131,072      | 129,024     | 8,192       | 0.0008元                    | 0.002元                   | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-plus-2025-01-12又称qwen-plus-0112 | 快照版 | 131,072      | 129,024     | 8,192       | 0.0008元                    | 0.002元                   | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-plus-2024-12-20又称qwen-plus-1220 | 快照版 | 131,072      | 129,024     | 8,192       | 0.0008元                    | 0.002元                   | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-plus-2024-11-27又称qwen-plus-1127 | 快照版 | 131,072      | 129,024     | 8,192       | 0.0008元                    | 0.002元                   | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-plus-2024-11-25又称qwen-plus-1125 | 快照版 | 131,072      | 129,024     | 8,192       | 0.0008元                    | 0.002元                   | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-plus-2024-09-19又称qwen-plus-0919 | 快照版 | 131,072      | 129,024     | 8,192       | 0.0008元                    | 0.002元                   | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-plus-2024-08-06又称qwen-plus-0806 | 快照版 | 131,072      | 128,000     | 8,192       | 0.004元                     | 0.012元                   | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-plus-2024-07-23又称qwen-plus-0723​ | 快照版 | 32,000       | 30,000      | 8,000       | 0.004元                     | 0.012元                   | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-plus-2024-06-24又称qwen-plus-0624 | 快照版 | 32,000       | 30,000      | 8,000       | 0.004元                     | 0.012元                   | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-plus-2024-02-06又称qwen-plus-0206 | 快照版 | 32,000       | 30,000      | 8,000       | 0.004元                     | 0.012元                   | 各100万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "#### 金融云\n",
      "\n",
      "| 模型名称   | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本                      | 输出成本                    | 免费额度                            |\n",
      "|------------|--------|--------------|-------------|-------------|-------------------------------|-----------------------------|-------------------------------------|\n",
      "| 模型名称   | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token）                 | （每千Token）               | 免费额度                            |\n",
      "| qwen-plus  | 稳定版 | 131,072      | 128,000     | 8,192       | 0.00152元Batch调用：0.00076元 | 0.0038元Batch调用：0.0019元 | 100万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### 通义千问-Turbo\n",
      "\n",
      "通义千问系列速度最快、成本极低的模型，适合简单任务。使用方法 | API参考 | 在线体验\n",
      "\n",
      "| 模型名称                                 | 版本   | 上下文长度   | 最大输入    | 最大输出   | 输入成本                     | 输出成本                    | 免费额度（注）                        |\n",
      "|------------------------------------------|--------|--------------|-------------|------------|------------------------------|-----------------------------|---------------------------------------|\n",
      "| 模型名称                                 | 版本   | （Token数）  | （Token数） |            | （每千Token）                | （每千Token）               | 免费额度（注）                        |\n",
      "| qwen-turbo当前等同 qwen-turbo-2025-02-11 | 稳定版 | 1,000,000    | 1,000,000   | 8,192      | 0.0003元Batch调用：0.00015元 | 0.0006元Batch调用：0.0003元 | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-turbo-latest始终等同最新快照版      | 最新版 | 1,000,000    | 1,000,000   | 8,192      | 0.0003元Batch调用：0.00015元 | 0.0006元Batch调用：0.0003元 | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-turbo-2025-02-11又称qwen-turbo-0211 | 快照版 | 1,000,000    | 1,000,000   | 8,192      | 0.0003元                     | 0.0006元                    | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-turbo-2024-11-01又称qwen-turbo-1101 | 快照版 | 1,000,000    | 1,000,000   | 8,192      | 0.0003元                     | 0.0006元                    | 1000万Token有效期：百炼开通后180天内  |\n",
      "| qwen-turbo-2024-09-19又称qwen-turbo-0919 | 快照版 | 131,072      | 129,024     | 8,192      | 0.0003元                     | 0.0006元                    | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-turbo-2024-06-24又称qwen-turbo-0624 | 快照版 | 8,000        | 6,000       | 2,000      | 0.002元                      | 0.006元                     | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-turbo-2024-02-06又称qwen-turbo-0206 | 快照版 | 8,000        | 6,000       | 2,000      | 0.002元                      | 0.006元                     | 各100万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### 通义千问-Long\n",
      "\n",
      "通义千问系列上下文窗口最长，能力均衡且成本较低的模型，适合长文本分析、信息抽取、总结摘要和分类打标等任务。使用方法 | 在线体验\n",
      "\n",
      "## 公共云\n",
      "\n",
      "| 模型名称                               | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本                     | 输出成本                  | 免费额度（注）                      |\n",
      "|----------------------------------------|--------|--------------|-------------|-------------|------------------------------|---------------------------|-------------------------------------|\n",
      "| 模型名称                               | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token）                | （每千Token）             | 免费额度（注）                      |\n",
      "| qwen-long                              | 稳定版 | 10,000,000   | 10,000,000  | 8,192       | 0.0005元Batch调用：0.00025元 | 0.002元Batch调用：0.001元 | 100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-long-latest始终等同最新快照版     | 最新版 | 10,000,000   | 10,000,000  | 8,192       | 0.0005元Batch调用：0.00025元 | 0.002元Batch调用：0.001元 | 100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-long-2025-01-25又称qwen-long-0125 | 快照版 | 10,000,000   | 10,000,000  | 8,192       | 0.0005元                     | 0.002元                   | 100万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "## 金融云\n",
      "\n",
      "| 模型名称   | 上下文长度   | 最大输入    | 最大输出    | 输入成本                     | 输出成本                  | 免费额度（注）                      |\n",
      "|------------|--------------|-------------|-------------|------------------------------|---------------------------|-------------------------------------|\n",
      "| 模型名称   | （Token数）  | （Token数） | （Token数） | （每千Token）                | （每千Token）             | 免费额度（注）                      |\n",
      "| qwen-long  | 10,000,000   | 10,000,000  | 8,192       | 0.0005元Batch调用：0.00025元 | 0.002元Batch调用：0.001元 | 100万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### 通义千问Omni\n",
      "\n",
      "通义千问全新多模态理解生成大模型，支持文本、图像、语音与视频输入，并输出文本与音频，提供了4种自然对话音色。使用方法｜API 参考\n",
      "\n",
      "| 模型名称                                           | 版本   | 上下文长度   | 最大输入    | 最大输出    | 免费额度（注）                                      |\n",
      "|----------------------------------------------------|--------|--------------|-------------|-------------|-----------------------------------------------------|\n",
      "| 模型名称                                           | 版本   | （Token数）  | （Token数） | （Token数） | 免费额度（注）                                      |\n",
      "| qwen-omni-turbo当前等同qwen-omni-turbo-2025-03-26  | 稳定版 | 32,768       | 30,720      | 2,048       | 各100万Token（不区分模态）有效期：百炼开通后180天内 |\n",
      "| qwen-omni-turbo-latest始终等同最新快照版           | 最新版 | 32,768       | 30,720      | 2,048       | 各100万Token（不区分模态）有效期：百炼开通后180天内 |\n",
      "| qwen-omni-turbo-2025-03-26又称qwen-omni-turbo-0326 | 快照版 | 32,768       | 30,720      | 2,048       | 各100万Token（不区分模态）有效期：百炼开通后180天内 |\n",
      "| qwen-omni-turbo-2025-01-19又称qwen-omni-turbo-0119 | 快照版 | 32,768       | 30,720      | 2,048       | 各100万Token（不区分模态）有效期：百炼开通后180天内 |\n",
      "\n",
      "免费额度用完后，输入与输出的计费规则如下：\n",
      "\n",
      "### QVQ\n",
      "\n",
      "通义千问QVQ是视觉推理模型，支持视觉输入及思维链输出，在数学、编程、视觉分析、创作以及通用任务上都表现了更强的能力。使用方法\n",
      "\n",
      "| 模型名称                           | 版本   | 上下文长度   | 最大输入            | 最大思维链长度   | 最大回复长度   | 输入成本      | 输出成本      | 免费额度（注）                         |\n",
      "|------------------------------------|--------|--------------|---------------------|------------------|----------------|---------------|---------------|----------------------------------------|\n",
      "| 模型名称                           | 版本   | （Token数）  | （Token数）         | （Token数）      | （Token数）    | （每千Token） | （每千Token） | 免费额度（注）                         |\n",
      "| qvq-max当前等同 qvq-max-2025-03-25 | 稳定版 | 122,880      | 98,304单图最大16384 | 16,384           | 8,192          | 0.008元       | 0.032元       | 各100万 Token有效期：百炼开通后180天内 |\n",
      "| qvq-max-latest始终等同最新快照版   | 最新版 | 122,880      | 98,304单图最大16384 | 16,384           | 8,192          | 0.008元       | 0.032元       | 各100万 Token有效期：百炼开通后180天内 |\n",
      "| qvq-max-2025-03-25又称qvq-max-0325 | 快照版 | 122,880      | 98,304单图最大16384 | 16,384           | 8,192          | 0.008元       | 0.032元       | 各100万 Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### 通义千问VL\n",
      "\n",
      "通义千问VL是具有视觉（图像）理解能力的文本生成模型，不仅能进行OCR（图片文字识别），还能进一步总结和推理，例如从商品照片中提取属性，根据习题图进行解题等。使用方法 | API参考 | 在线体验\n",
      "\n",
      "| 模型名称                                                                                                                   | 版本   | 上下文长度   | 最大输入             | 最大输出    | 输入成本                     | 输出成本                     | 免费额度（注）                        |\n",
      "|----------------------------------------------------------------------------------------------------------------------------|--------|--------------|----------------------|-------------|------------------------------|------------------------------|---------------------------------------|\n",
      "| 模型名称                                                                                                                   | 版本   | （Token数）  | （Token数）          | （Token数） | （每千Token）                | （每千Token）                | 免费额度（注）                        |\n",
      "| qwen-vl-max相比qwen-vl-plus再次提升视觉推理和指令遵循能力，在更多复杂任务中提供最佳性能。当前等同qwen-vl-max-2024-11-19    | 稳定版 | 32,768       | 30,720单图最大16384  | 2,048       | 0.003元Batch调用：0.0015元   | 0.009元Batch调用：0.0045元   | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-vl-max-latest始终等同最新快照版                                                                                       | 最新版 | 131,072      | 129,024单图最大16384 | 8,192       | 0.003元Batch调用：0.0015元   | 0.009元Batch调用：0.0045元   | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-vl-max-2025-01-25又称qwen-vl-max-0125此版本属于Qwen2.5-VL系列模型，扩展上下文至128k，显著增强图像和视频的理解能力。   | 快照版 | 131,072      | 129,024单图最大16384 | 8,192       | 0.003元                      | 0.009元                      | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-vl-max-2024-12-30又称qwen-vl-max-1230                                                                                 | 快照版 | 32,768       | 30,720单图最大16384  | 2,048       | 0.003元                      | 0.009元                      | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-vl-max-2024-11-19又称qwen-vl-max-1119                                                                                 | 快照版 | 32,768       | 30,720单图最大16384  | 2,048       | 0.003元                      | 0.009元                      | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-vl-max-2024-10-30又称qwen-vl-max-1030                                                                                 | 快照版 | 32,768       | 30,720单图最大16384  | 2,048       | 0.02元                       | 0.02元                       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-vl-max-2024-08-09又称qwen-vl-max-0809此版本扩展上下文至32k，增强图像理解能力，能更好地识别图片中的多语种和手写体。    | 快照版 | 32,768       | 30,720单图最大16384  | 2,048       | 0.02元                       | 0.02元                       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-vl-max-2024-02-01又称qwen-vl-max-0201                                                                                 | 快照版 | 8,000        | 6,000单图最大1280    | 2,000       | 0.02元                       | 0.02元                       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-vl-plus大幅提升细节识别和文字识别能力，支持超百万像素分辨率和任意宽高比的图像。在广泛的视觉任务中提供卓越性能。       | 稳定版 | 8,000        | 6,000单图最大1280    | 2,000       | 0.0015元Batch调用：0.00075元 | 0.0045元Batch调用：0.00225元 | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-vl-plus-latest始终等同最新快照版                                                                                      | 最新版 | 131,072      | 129,024单图最大16384 | 8,192       | 0.0015元                     | 0.0045元                     | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-vl-plus-2025-01-25又称qwen-vl-plus-0125此版本属于Qwen2.5-VL系列模型，扩展上下文至128k，显著增强图像和视频的理解能力。 | 快照版 | 131,072      | 129,024单图最大16384 | 8,192       | 0.0015元                     | 0.0045元                     | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-vl-plus-2025-01-02又称qwen-vl-plus-0102                                                                               | 快照版 | 32,768       | 30,720单图最大16384  | 2,048       | 0.0015元                     | 0.0045元                     | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-vl-plus-2024-08-09又称qwen-vl-plus-0809                                                                               | 快照版 | 32,768       | 30,720单图最大16384  | 2,048       | 0.0015元                     | 0.0045元                     | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-vl-plus-2023-12-01                                                                                                    | 快照版 | 8,000        | 6,000                | 2,000       | 0.008元                      | 0.008元                      | 各100万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### 通义千问OCR\n",
      "\n",
      "通义千问OCR模型是专用于文字提取的模型。相较于通义千问VL模型，它更专注于文档、表格、试题、手写体文字等类型图像的文字提取能力。它能够识别多种语言，包括英语、法语、日语、韩语、德语、俄语和意大利语等。使用方法 | API参考｜在线体验\n",
      "\n",
      "| 模型名称                                   | 版本   | 上下文长度   | 最大输入           | 最大输出    | 输入输出单价   | 免费额度（注）                        |\n",
      "|--------------------------------------------|--------|--------------|--------------------|-------------|----------------|---------------------------------------|\n",
      "| 模型名称                                   | 版本   | （Token数）  | （Token数）        | （Token数） | （每千Token）  | 免费额度（注）                        |\n",
      "| qwen-vl-ocr当前等同qwen-vl-ocr-2024-10-28  | 稳定版 | 34096        | 30000单图最大30000 | 4096        | 0.005元        | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-vl-ocr-latest始终等同最新快照版       | 最新版 | 34096        | 30000单图最大30000 | 4096        | 0.005元        | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-vl-ocr-2024-10-28又称qwen-vl-ocr-1028 | 快照版 | 34096        | 30000单图最大30000 | 4096        | 0.005元        | 各100万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### 通义千问Audio\n",
      "\n",
      "通义千问Audio是音频理解模型，支持输入多种音频（人类语音、自然音、音乐、歌声）和文本，并输出文本。该模型不仅能对输入的音频进行转录，还具备更深层次的语义理解、情感分析、音频事件检测、语音聊天等能力。使用方法\n",
      "\n",
      "| 模型名称                                                                                                         | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本                                                     | 输出成本                                                     | 免费额度（注）                       |\n",
      "|------------------------------------------------------------------------------------------------------------------|--------|--------------|-------------|-------------|--------------------------------------------------------------|--------------------------------------------------------------|--------------------------------------|\n",
      "| 模型名称                                                                                                         | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token）                                                | （每千Token）                                                | 免费额度（注）                       |\n",
      "| qwen-audio-turbo当前等同qwen-audio-turbo-2024-08-07                                                              | 稳定版 | 8,000        | 6,000       | 1,500       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\n",
      "| qwen-audio-turbo-latest始终等同最新快照版                                                                        | 最新版 | 8,192        | 6,144       | 2,048       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\n",
      "| qwen-audio-turbo-2024-12-04又称qwen-audio-turbo-1204较上个快照版本大幅提升语音识别准确率，且新增了语音聊天能力。 | 快照版 | 8,192        | 6,144       | 2,048       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\n",
      "| qwen-audio-turbo-2024-08-07又称qwen-audio-turbo-0807                                                             | 快照版 | 8,000        | 6,000       | 1,500       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### 通义千问ASR（Beta版本）\n",
      "\n",
      "通义千问ASR是基于Qwen-Audio训练，专用于语音识别的模型。目前支持的语言有：中文和英文。使用方法\n",
      "\n",
      "通义千问Audio与通义千问ASR（Beta版本）的区别\n",
      "\n",
      "- 功能对比：\n",
      "- 功能对比：\n",
      "    - 通义千问Audio模型是对话模型，不仅能够进行语音识别，还具备更深层次的语义理解、语音聊天等能力，支持设置提示词。\n",
      "    - 通义千问ASR模型是专用于语音识别的模型，不支持设置提示词（包括System Prompt和User Prompt）。\n",
      "- 准确率对比：\n",
      "- 准确率对比：\n",
      "    - 在语音识别准确率上，通义千问ASR模型高于通义千问Audio模型。\n",
      "- 音频时长对比：\n",
      "- 音频时长对比：\n",
      "    - 通义千问Audio模型：30秒内。\n",
      "    - 通义千问ASR模型：3分钟以内。\n",
      "- 支持识别的语言对比 ：\n",
      "- 支持识别的语言对比：\n",
      "    - 通义千问Audio模型：中文、英文、粤语、法语、意大利语、西班牙语、德语和日语。\n",
      "    - 通义千问ASR模型：中文、英文。目前通义千问ASR是Beta版本，后续版本中将会陆续支持更多语言的识别。\n",
      "\n",
      "| 模型名称                                          | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本                                                     | 输出成本                                                     | 免费额度（注）                       |\n",
      "|---------------------------------------------------|--------|--------------|-------------|-------------|--------------------------------------------------------------|--------------------------------------------------------------|--------------------------------------|\n",
      "| 模型名称                                          | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token）                                                | （每千Token）                                                | 免费额度（注）                       |\n",
      "| qwen-audio-asr当前等同qwen-audio-asr-2024-12-04   | 稳定版 | 8,192        | 6,144       | 2,048       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\n",
      "| qwen-audio-asr-latest始终等同最新快照版           | 最新版 | 8,192        | 6,144       | 2,048       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\n",
      "| qwen-audio-asr-2024-12-04 又称qwen-audio-asr-1204 | 快照版 | 8,192        | 6,144       | 2,048       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### 通义千问数学模型\n",
      "\n",
      "通义千问数学模型是专门用于数学解题的语言模型。使用方法 | API参考 | 在线体验\n",
      "\n",
      "| 模型名称                                           | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                        |\n",
      "|----------------------------------------------------|--------|--------------|-------------|-------------|---------------|---------------|---------------------------------------|\n",
      "| 模型名称                                           | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                        |\n",
      "| qwen-math-plus当前等同qwen-math-plus-2024-09-19    | 稳定版 | 4,096        | 3,072       | 3,072       | 0.004元       | 0.012元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-math-plus-latest始终等同最新快照版            | 最新版 | 4,096        | 3,072       | 3,072       | 0.004元       | 0.012元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-math-plus-2024-09-19又称qwen-math-plus-0919   | 快照版 | 4,096        | 3,072       | 3,072       | 0.004元       | 0.012元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-math-plus-2024-08-16又称qwen-math-plus-0816   | 快照版 | 4,096        | 3,072       | 3,072       | 0.004元       | 0.012元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-math-turbo当前等同qwen-math-turbo-2024-09-19  | 稳定版 | 4,096        | 3,072       | 3,072       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-math-turbo-latest始终等同最新快照版           | 最新版 | 4,096        | 3,072       | 3,072       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-math-turbo-2024-09-19又称qwen-math-turbo-0919 | 快照版 | 4,096        | 3,072       | 3,072       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### 通义千问Coder\n",
      "\n",
      "通义千问代码模型。使用方法 | API参考 | 在线体验\n",
      "\n",
      "| 模型名称                                                  | 版本   | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                        |\n",
      "|-----------------------------------------------------------|--------|--------------|-------------|-------------|---------------|---------------|---------------------------------------|\n",
      "| 模型名称                                                  | 版本   | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                        |\n",
      "| qwen-coder-plus当前等同qwen-coder-plus-2024-11-06         | 稳定版 | 131,072      | 129,024     | 8,192       | 0.0035元      | 0.007元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-coder-plus-latest等同qwen-coder-plus最新的快照版本   | 最新版 | 131,072      | 129,024     | 8,192       | 0.0035元      | 0.007元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-coder-plus-2024-11-06 又称qwen-coder-plus-1106       | 快照版 | 131,072      | 129,024     | 8,192       | 0.0035元      | 0.007元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-coder-turbo当前等同qwen-coder-turbo-2024-09-19       | 稳定版 | 131,072      | 129,024     | 8,192       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-coder-turbo-latest等同qwen-coder-turbo最新的快照版本 | 最新版 | 131,072      | 129,024     | 8,192       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-coder-turbo-2024-09-19又称qwen-coder-turbo-0919      | 快照版 | 131,072      | 129,024     | 8,192       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### 通义千问翻译模型\n",
      "\n",
      "基于通义千问模型优化的机器翻译大语言模型，擅长中英互译、中文与小语种互译、英文与小语种互译，小语种包括日、韩、法、西、德、葡（巴西）、泰、印尼、越、阿等26种。在多语言互译的基础上，提供术语干预、领域提示、记忆库等能力，提升模型在复杂应用场景下的翻译效果。使用方法\n",
      "\n",
      "| 模型名称      | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                       |\n",
      "|---------------|--------------|-------------|-------------|---------------|---------------|--------------------------------------|\n",
      "| 模型名称      | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                       |\n",
      "| qwen-mt-plus  | 2,048        | 1,024       | 1,024       | 0.015元       | 0.045元       | 各50万Token有效期：百炼开通后180天内 |\n",
      "| qwen-mt-turbo | 2,048        | 1,024       | 1,024       | 0.001元       | 0.003元       | 各50万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "## 文本生成-通义千问-开源版\n",
      "\n",
      "- 模型名称中，xxb表示参数规模，例如qwen2-72b-instruct表示参数规模为72B，即720亿。\n",
      "- 百炼支持调用通义千问的开源版，您无需本地部署模型。对于开源版，建议使用Qwen2.5或Qwen2模型。\n",
      "\n",
      "### QwQ-开源版\n",
      "\n",
      "基于 Qwen2.5-32B 模型训练的 QwQ 推理模型，通过强化学习大幅度提升了模型推理能力。模型数学代码等核心指标（AIME 24/25、LiveCodeBench）以及部分通用指标（IFEval、LiveBench等）达到DeepSeek-R1 满血版水平，各指标均显著超过同样基于 Qwen2.5-32B 的 DeepSeek-R1-Distill-Qwen-32B。使用方法\n",
      "\n",
      "| 模型名称   | 上下文长度   | 最大输入    | 最大思维链长度   | 最大回复长度   | 输入成本      | 输出成本      | 免费额度（注）                       |\n",
      "|------------|--------------|-------------|------------------|----------------|---------------|---------------|--------------------------------------|\n",
      "| 模型名称   | （Token数）  | （Token数） | （Token数）      | （Token数）    | （每千Token） | （每千Token） | 免费额度（注）                       |\n",
      "| qwq-32b    | 131,072      | 98,304      | 32,768           | 8,192          | 0.002元       | 0.006元       | 100万 Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### QwQ-Preview\n",
      "\n",
      "qwq-32b-preview 模型是由 Qwen 团队于2024年开发的实验性研究模型，专注于增强 AI 推理能力，尤其是数学和编程领域。qwq-32b-preview 模型的局限性请参见QwQ官方博客。使用方法 | API参考｜在线体验\n",
      "\n",
      "| 模型名称        | 上下文长度   | 最大输入    | 最大输出    | 输入成本                  | 输出成本                  | 免费额度（注）                      |\n",
      "|-----------------|--------------|-------------|-------------|---------------------------|---------------------------|-------------------------------------|\n",
      "| 模型名称        | （Token数）  | （Token数） | （Token数） | （每千Token）             | （每千Token）             | 免费额度（注）                      |\n",
      "| qwq-32b-preview | 32,768       | 30,720      | 16,384      | 0.002元Batch调用：0.001元 | 0.006元Batch调用：0.003元 | 100万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### Qwen2.5\n",
      "\n",
      "Qwen2.5是Qwen大型语言模型的最新系列。针对Qwen2.5，我们发布了一系列基础语言模型和指令调优语言模型，参数规模从5亿到720亿不等。Qwen2.5在Qwen2基础上进行了以下改进：\n",
      "\n",
      "- 在我们最新的大规模数据集上进行预训练，包含多达18万亿个Token。\n",
      "- 由于我们在这些领域的专业专家模型，模型的知识显著增多，编码和数学能力也大幅提高。\n",
      "- 在遵循指令、生成长文本（超过8K个标记）、理解结构化数据（例如表格）和生成结构化输出（尤其是JSON）方面有显著改进。对系统提示的多样性更具弹性，增强了聊天机器人的角色扮演实现和条件设置。\n",
      "- 支持超过29种语言，包括中文、英语、法语、西班牙语、葡萄牙语、德语、意大利语、俄语、日语、韩语、越南语、泰语、阿拉伯语等。\n",
      "\n",
      "使用方法 | API参考 | 在线体验\n",
      "\n",
      "| 模型名称                | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                        |\n",
      "|-------------------------|--------------|-------------|-------------|---------------|---------------|---------------------------------------|\n",
      "| 模型名称                | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                        |\n",
      "| qwen2.5-14b-instruct-1m | 1,000,000    | 1,000,000   | 8,192       | 0.001元       | 0.003元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2.5-7b-instruct-1m  | 1,000,000    | 1,000,000   | 8,192       | 0.0005元      | 0.001元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2.5-72b-instruct    | 131,072      | 129,024     | 8,192       | 0.004元       | 0.012元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2.5-32b-instruct    | 131,072      | 129,024     | 8,192       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2.5-14b-instruct    | 131,072      | 129,024     | 8,192       | 0.001元       | 0.003元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2.5-7b-instruct     | 131,072      | 129,024     | 8,192       | 0.0005元      | 0.001元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2.5-3b-instruct     | 32,768       | 30,720      | 8,192       | 0.0003元      | 0.0009元      | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2.5-1.5b-instruct   | 32,768       | 30,720      | 8,192       | 限时免费      | 限时免费      | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2.5-0.5b-instruct   | 32,768       | 30,720      | 8,192       | 限时免费      | 限时免费      | 各100万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### Qwen2\n",
      "\n",
      "阿里云的通义千问2-开源版。使用方法 | API参考 | 在线体验\n",
      "\n",
      "| 模型名称                | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                        |\n",
      "|-------------------------|--------------|-------------|-------------|---------------|---------------|---------------------------------------|\n",
      "| 模型名称                | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                        |\n",
      "| qwen2-72b-instruct      | 131,072      | 128,000     | 6,144       | 0.004元       | 0.012元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2-57b-a14b-instruct | 65,536       | 63,488      | 6,144       | 0.0035元      | 0.007元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2-7b-instruct       | 131,072      | 128,000     | 6,144       | 0.001元       | 0.002元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2-1.5b-instruct     | 32,768       | 30,720      | 6,144       | 限时免费      | 限时免费      | 限时免费                              |\n",
      "| qwen2-0.5b-instruct     | 32,768       | 30,720      | 6,144       | 限时免费      | 限时免费      | 限时免费                              |\n",
      "\n",
      "### Qwen1.5\n",
      "\n",
      "阿里云的通义千问1.5-开源版。使用方法 | API参考 | 在线体验\n",
      "\n",
      "| 模型名称          | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                        |\n",
      "|-------------------|--------------|-------------|-------------|---------------|---------------|---------------------------------------|\n",
      "| 模型名称          | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                        |\n",
      "| qwen1.5-110b-chat | 32,000       | 30,000      | 8,000       | 0.007元       | 0.014元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen1.5-72b-chat  | 32,000       | 30,000      | 2,000       | 0.005元       | 0.01元        | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen1.5-32b-chat  | 32,000       | 30,000      | 2,000       | 0.0035元      | 0.007元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen1.5-14b-chat  | 8,000        | 6,000       | 2,000       | 0.002元       | 0.004元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen1.5-7b-chat   | 8,000        | 6,000       | 2,000       | 0.001元       | 0.002元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen1.5-1.8b-chat | 32,000       | 30,000      | 2,000       | 限时免费      | 限时免费      | 限时免费                              |\n",
      "| qwen1.5-0.5b-chat | 32,000       | 30,000      | 2,000       | 限时免费      | 限时免费      | 限时免费                              |\n",
      "\n",
      "### Qwen\n",
      "\n",
      "阿里云的通义千问-开源版。使用方法 | API参考 | 在线体验\n",
      "\n",
      "| 模型名称                   | 上下文长度   | 最大输入    | 最大输出    | 输入成本           | 输出成本           | 免费额度（注）                        |\n",
      "|----------------------------|--------------|-------------|-------------|--------------------|--------------------|---------------------------------------|\n",
      "| 模型名称                   | （Token数）  | （Token数） | （Token数） | （每千Token）      | （每千Token）      | 免费额度（注）                        |\n",
      "| qwen-72b-chat              | 32,000       | 30,000      | 2,000       | 0.02元             | 0.02元             | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-14b-chat              | 8,000        | 6,000       | 2,000       | 0.008元            | 0.008元            | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-7b-chat               | 7,500        | 6,000       | 1,500       | 0.006元            | 0.006元            | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen-1.8b-chat             | 8,000        | 6,000       | 2,000       | 限时免费           | 限时免费           | 限时免费                              |\n",
      "| qwen-1.8b-longcontext-chat | 32,000       | 30,000      | 2,000       | 限时免费（需申请） | 限时免费（需申请） | 限时免费（需申请）                    |\n",
      "\n",
      "### QVQ\n",
      "\n",
      "qvq-72b-preview模型是由 Qwen 团队开发的实验性研究模型，专注于提升视觉推理能力，尤其在数学推理领域。qvq-72b-preview模型的局限性请参见QVQ官方博客。使用方法 | API参考\n",
      "\n",
      "| 模型名称        | 上下文长度   | 最大输入            | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                     |\n",
      "|-----------------|--------------|---------------------|-------------|---------------|---------------|------------------------------------|\n",
      "| 模型名称        | （Token数）  | （Token数）         | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                     |\n",
      "| qvq-72b-preview | 32,768       | 16,384单图最大16384 | 16,384      | 0.012元       | 0.036元       | 10万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### Qwen-Omni\n",
      "\n",
      "基于Qwen2.5训练的全新多模态理解生成大模型，支持文本、图像、语音、视频输入理解，具备文本和语音同时流式生成的能力，多模态内容理解速度显著提升。使用方法｜API 参考\n",
      "\n",
      "| 模型名称        | 上下文长度   | 最大输入    | 最大输出    | 免费额度（注）                                    |\n",
      "|-----------------|--------------|-------------|-------------|---------------------------------------------------|\n",
      "| 模型名称        | （Token数）  | （Token数） | （Token数） | 免费额度（注）                                    |\n",
      "| qwen2.5-omni-7b | 32,768       | 30,720      | 2,048       | 100万Token（不区分模态）有效期：百炼开通后180天内 |\n",
      "\n",
      "开源版模型的免费额度用完后，输入与输出的计费规则如下：\n",
      "\n",
      "### Qwen-VL\n",
      "\n",
      "阿里云的通义千问VL开源版。使用方法 | API参考\n",
      "\n",
      "其中，Qwen2.5-VL在Qwen2-VL的基础上做了如下改进：\n",
      "\n",
      "- 感知更丰富的世界：Qwen2.5-VL不仅擅长识别常见物体，如花、鸟、鱼和昆虫等，还能分析图像中的文本、图表、图标、图形和布局等。\n",
      "- 长视频理解能力：支持对长视频文件（最长10分钟）进行理解，具备通过精准定位相关视频片段来捕捉事件的新能力\n",
      "- 视觉定位：Qwen2.5-VL可通过生成bounding box（矩形框的左上角和右下角坐标）或者point（矩形框的中心点坐标）来准确定位图像中的物体，并能够为坐标和属性提供稳定的JSON输出。\n",
      "- 结构化输出：可支持对发票、表单、表格等数据进行结构化输出，惠及金融、商业等领域的应用。\n",
      "\n",
      "| 模型名称                | 上下文长度   | 最大输入             | 最大输出    | 输入成本                                                     | 输出成本                                                     | 免费额度（注）                        |\n",
      "|-------------------------|--------------|----------------------|-------------|--------------------------------------------------------------|--------------------------------------------------------------|---------------------------------------|\n",
      "| 模型名称                | （Token数）  | （Token数）          | （Token数） | （每千Token）                                                | （每千Token）                                                | 免费额度（注）                        |\n",
      "| qwen2.5-vl-72b-instruct | 131,072      | 129,024单图最大16384 | 8,192       | 0.016元                                                      | 0.048元                                                      | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2.5-vl-32b-instruct | 131,072      | 129,024单图最大16384 | 8,192       | 0.008元                                                      | 0.024元                                                      | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2.5-vl-7b-instruct  | 131,072      | 129,024单图最大16384 | 8,192       | 0.002元                                                      | 0.005元                                                      | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2.5-vl-3b-instruct  | 131,072      | 129,024单图最大16384 | 8,192       | 0.0012元                                                     | 0.0036元                                                     | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2-vl-72b-instruct   | 32,768       | 30,720单图最大16384  | 2,048       | 0.016元                                                      | 0.048元                                                      | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2-vl-7b-instruct    | 32,000       | 30,000单图最大16384  | 2,000       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内  |\n",
      "| qwen2-vl-2b-instruct    | 32,000       | 30,000单图最大16384  | 2,000       | 限时免费                                                     | 限时免费                                                     | 各10万Token有效期：百炼开通后180天内  |\n",
      "| qwen-vl-v1              | 8,000        | 6,000单图最大1280    | 1,500       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内  |\n",
      "| qwen-vl-chat-v1         | 8,000        | 6,000单图最大1280    | 1,500       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内  |\n",
      "\n",
      "### Qwen-Audio\n",
      "\n",
      "阿里云的通义千问Audio开源版。使用方法\n",
      "\n",
      "| 模型名称                                                                          | 上下文长度   | 最大输入    | 最大输出    | 输入成本                                                     | 输出成本                                                     | 免费额度（注）                       |\n",
      "|-----------------------------------------------------------------------------------|--------------|-------------|-------------|--------------------------------------------------------------|--------------------------------------------------------------|--------------------------------------|\n",
      "| 模型名称                                                                          | （Token数）  | （Token数） | （Token数） | （每千Token）                                                | （每千Token）                                                | 免费额度（注）                       |\n",
      "| qwen2-audio-instruct相比qwen-audio-chat提升了音频理解能力，且新增了语音聊天能力。 | 8,000        | 6,000       | 1,500       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\n",
      "| qwen-audio-chat                                                                   | 8,000        | 6,000       | 1,500       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各10万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### Qwen-Math\n",
      "\n",
      "基于Qwen模型构建的专门用于数学解题的语言模型。Qwen2.5-Math相比Qwen2-Math有了实质性的改进。Qwen2.5-Math支持中文和英文，并整合了多种推理方法，包括CoT（Chain of Thought）、PoT（Program of Thought）和 TIR（Tool-Integrated Reasoning）。使用方法 | API参考| 在线体验\n",
      "\n",
      "| 模型名称                   | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                        |\n",
      "|----------------------------|--------------|-------------|-------------|---------------|---------------|---------------------------------------|\n",
      "| 模型名称                   | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                        |\n",
      "| qwen2.5-math-72b-instruct  | 4,096        | 3,072       | 3,072       | 0.004元       | 0.012元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2.5-math-7b-instruct   | 4,096        | 3,072       | 3,072       | 0.001元       | 0.002元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2.5-math-1.5b-instruct | 4,096        | 3,072       | 3,072       | 限时免费体验  | 限时免费体验  | 限时免费体验                          |\n",
      "| qwen2-math-72b-instruct    | 4,096        | 3,072       | 3,072       | 0.004元       | 0.012元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2-math-7b-instruct     | 4,096        | 3,072       | 3,072       | 0.001元       | 0.002元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2-math-1.5b-instruct   | 4,096        | 3,072       | 3,072       | 限时免费体验  | 限时免费体验  | 限时免费体验                          |\n",
      "\n",
      "### Qwen-Coder\n",
      "\n",
      "通义千问代码模型开源版。Qwen2.5-Coder相比CodeQwen1.5有了实质性的改进。Qwen2.5-Coder在包含5.5万亿Token的编程相关数据上进行了训练，使即使较小的编程专用模型也能在编程评估基准测试中表现出媲美大型语言模型的竞争力。API参考 | 在线体验\n",
      "\n",
      "| 模型名称                    | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                        |\n",
      "|-----------------------------|--------------|-------------|-------------|---------------|---------------|---------------------------------------|\n",
      "| 模型名称                    | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                        |\n",
      "| qwen2.5-coder-32b-instruct  | 131,072      | 129,024     | 8,192       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2.5-coder-14b-instruct  | 131,072      | 129,024     | 8,192       | 0.002元       | 0.006元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2.5-coder-7b-instruct   | 131,072      | 129,024     | 8,192       | 0.001元       | 0.002元       | 各100万Token有效期：百炼开通后180天内 |\n",
      "| qwen2.5-coder-3b-instruct   | 32,768       | 30,720      | 8,192       | 限时免费体验  | 限时免费体验  | 限时免费体验                          |\n",
      "| qwen2.5-coder-1.5b-instruct | 32,768       | 30,720      | 8,192       | 限时免费体验  | 限时免费体验  | 限时免费体验                          |\n",
      "| qwen2.5-coder-0.5b-instruct | 32,768       | 30,720      | 8,192       | 限时免费体验  | 限时免费体验  | 限时免费体验                          |\n",
      "\n",
      "## 文本生成-第三方模型\n",
      "\n",
      "### DeepSeek\n",
      "\n",
      "DeepSeek-R1 在后训练阶段大规模使用了强化学习技术，在仅有极少标注数据的情况下，极大提升了模型推理能力，尤其在数学、代码、自然语言推理等任务上；DeepSeek-V3 为 MoE 模型，671B 参数，激活 37B，在 14.8T token 上进行了预训练，在长文本、代码、数学、百科、中文能力上表现优秀。API参考\n",
      "\n",
      "| 模型名称                                            | 上下文长度   | 最大输入    | 最大思维链长度   | 最大回复长度   | 输入成本                  | 输出成本                  | 免费额度查看剩余额度                  |\n",
      "|-----------------------------------------------------|--------------|-------------|------------------|----------------|---------------------------|---------------------------|---------------------------------------|\n",
      "| 模型名称                                            | （Token数）  | （Token数） | （Token数）      | （Token数）    | （每千Token）             | （每千Token）             | 免费额度查看剩余额度                  |\n",
      "| deepseek-r1 671B 满血版模型                         | 65,792       | 57,344      | 32,768           | 8,192          | 0.004元Batch调用：0.002元 | 0.016元Batch调用：0.008元 | 各100万Token有效期：百炼开通后180天内 |\n",
      "| deepseek-v3参数量为 671B                            | 65,792       | 57,344      | 不涉及           | 8,192          | 0.002元Batch调用：0.001元 | 0.008元Batch调用：0.004元 | 各100万Token有效期：百炼开通后180天内 |\n",
      "| deepseek-r1-distill-qwen-1.5b基于 Qwen2.5-Math-1.5B | 32,768       | 32,768      | 16,384           | 16,384         | 限时免费体验              | 限时免费体验              | 限时免费体验                          |\n",
      "| deepseek-r1-distill-qwen-7b基于 Qwen2.5-Math-7B     | 32,768       | 32,768      | 16,384           | 16,384         | 0.0005元                  | 0.001元                   | 各100万Token有效期：百炼开通后180天内 |\n",
      "| deepseek-r1-distill-qwen-14b基于 Qwen2.5-14B        | 32,768       | 32,768      | 16,384           | 16,384         | 0.001元                   | 0.003元                   | 各100万Token有效期：百炼开通后180天内 |\n",
      "| deepseek-r1-distill-qwen-32b基于 Qwen2.5-32B        | 32,768       | 32,768      | 16,384           | 16,384         | 0.002元                   | 0.006元                   | 各100万Token有效期：百炼开通后180天内 |\n",
      "| deepseek-r1-distill-llama-8b基于 Llama-3.1-8B       | 32,768       | 32,768      | 16,384           | 16,384         | 限时免费体验              | 限时免费体验              | 限时免费体验                          |\n",
      "| deepseek-r1-distill-llama-70b基于 Llama-3.3-70B     | 32,768       | 32,768      | 16,384           | 16,384         | 限时免费体验              | 限时免费体验              | 限时免费体验                          |\n",
      "\n",
      "### Llama-仅文本输入\n",
      "\n",
      "Meta推出的大语言模型，下列模型只支持输入文本。API参考 | 在线体验（需申请）\n",
      "\n",
      "| 模型名称               | 上下文长度   | 最大输入    | 输入输出成本                                                 | 免费额度（注）                                    |\n",
      "|------------------------|--------------|-------------|--------------------------------------------------------------|---------------------------------------------------|\n",
      "| 模型名称               | （Token数）  | （Token数） | 输入输出成本                                                 | 免费额度（注）                                    |\n",
      "| llama3.3-70b-instruct  | 32,000       | 30,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\n",
      "| llama3.2-3b-instruct   | 32,000       | 30,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\n",
      "| llama3.2-1b-instruct   | 32,000       | 30,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\n",
      "| llama3.1-405b-instruct | 32,000       | 30,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\n",
      "| llama3.1-70b-instruct  | 32,000       | 30,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\n",
      "| llama3.1-8b-instruct   | 32,000       | 30,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\n",
      "| llama3-70b-instruct    | 8,000        | 8,000       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\n",
      "| llama3-8b-instruct     | 8,000        | 8,000       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\n",
      "| llama2-13b-chat-v2     | 4,000        | 4,000       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\n",
      "| llama2-7b-chat-v2      | 4,000        | 4,000       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\n",
      "\n",
      "### Llama-文本和图像输入\n",
      "\n",
      "Meta推出的大语言模型，下列模型支持输入文本和图像。API参考 | 在线体验（需申请）\n",
      "\n",
      "| 模型名称                     | 上下文长度   | 输入输出成本                                                 | 免费额度（注）                                    |\n",
      "|------------------------------|--------------|--------------------------------------------------------------|---------------------------------------------------|\n",
      "| 模型名称                     | （Token数）  | 输入输出成本                                                 | 免费额度（注）                                    |\n",
      "| llama3.2-90b-vision-instruct | 8192         | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\n",
      "| llama3.2-11b-vision          | 8192         | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内。 |\n",
      "\n",
      "### 百川\n",
      "\n",
      "百川智能推出的大语言模型。API参考 | 在线体验（需申请）\n",
      "\n",
      "| 模型名称        | 上下文长度   | 最大输入    | 输入成本                                                     | 输出成本                                                     | 免费额度（注）                                |\n",
      "|-----------------|--------------|-------------|--------------------------------------------------------------|--------------------------------------------------------------|-----------------------------------------------|\n",
      "| 模型名称        | （Token数）  | （Token数） | （每千Token）                                                | （每千Token）                                                | 免费额度（注）                                |\n",
      "| baichuan2-turbo | 32,000       | 32,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 100万Token（需申请）有效期：申请通过后180天内 |\n",
      "\n",
      "### 百川-开源版\n",
      "\n",
      "来自百川智能，该系列模型在平台中支持微调训练。API参考 | 在线体验（需申请）\n",
      "\n",
      "| 模型名称              | 上下文长度   | 最大输入    | 输入成本                                                     | 输出成本                                                     | 免费额度（注）                                  |\n",
      "|-----------------------|--------------|-------------|--------------------------------------------------------------|--------------------------------------------------------------|-------------------------------------------------|\n",
      "| 模型名称              | （Token数）  | （Token数） | （每千Token）                                                | （每千Token）                                                | 免费额度（注）                                  |\n",
      "| baichuan2-13b-chat-v1 | 4096         | 4096        | 0.008元                                                      | 0.008元                                                      | 各100万Token（需申请）有效期：百炼开通后180天内 |\n",
      "| baichuan2-7b-chat-v1  | 4096         | 4096        | 0.006元                                                      | 0.006元                                                      | 各100万Token（需申请）有效期：百炼开通后180天内 |\n",
      "| baichuan-7b-v1        | 4096         | 4096        | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：百炼开通后180天内 |\n",
      "\n",
      "### ChatGLM\n",
      "\n",
      "智谱AI推出的大语言模型。API参考 | 在线体验\n",
      "\n",
      "| 模型名称      | 上下文长度   | 最大输入    | 输入成本                                                     | 输出成本                                                     | 免费额度（注）                        |\n",
      "|---------------|--------------|-------------|--------------------------------------------------------------|--------------------------------------------------------------|---------------------------------------|\n",
      "| 模型名称      | （Token数）  | （Token数） | （每千Token）                                                | （每千Token）                                                | 免费额度（注）                        |\n",
      "| chatglm3-6b   | 7500         | 7500        | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token有效期：百炼开通后180天内 |\n",
      "| chatglm-6b-v2 | 6500         | 6500        | 0.006元                                                      | 0.006元                                                      | 各100万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### 零一万物\n",
      "\n",
      "零一万物推出的大语言模型。API参考 | 在线体验（需申请）\n",
      "\n",
      "| 模型名称                   | 上下文长度   | 最大输入    | 输入输出成本                                                 | 免费额度（注）                                  |\n",
      "|----------------------------|--------------|-------------|--------------------------------------------------------------|-------------------------------------------------|\n",
      "| 模型名称                   | （Token数）  | （Token数） | 输入输出成本                                                 | 免费额度（注）                                  |\n",
      "| yi-large                   | 32,000       | 32,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：申请通过后180天内 |\n",
      "| yi-medium                  | 32,000       | 32,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：申请通过后180天内 |\n",
      "| yi-large-rag有实时联网能力 | 16,000       | 16,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：申请通过后180天内 |\n",
      "| yi-large-turbo             | 16,000       | 16,000      | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：申请通过后180天内 |\n",
      "\n",
      "### MiniMax\n",
      "\n",
      "MiniMax推出的大语言模型。API参考 | 在线体验（需申请）\n",
      "\n",
      "| 模型名称      | 说明             | 上下文长度   | 最大输入    | 输入输出成本                                                 | 免费额度（注）                                  |\n",
      "|---------------|------------------|--------------|-------------|--------------------------------------------------------------|-------------------------------------------------|\n",
      "| 模型名称      | 说明             | （Token数）  | （Token数） | 输入输出成本                                                 | 免费额度（注）                                  |\n",
      "| abab6.5g-chat | 适合英文场景     | 8,000        | 8,000       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：申请通过后180天内 |\n",
      "| abab6.5t-chat | 适合中文场景     | 8,000        | 8,000       | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：申请通过后180天内 |\n",
      "| abab6.5s-chat | 适合超长文本场景 | 245,000      | 245,000     | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 各100万Token（需申请）有效期：申请通过后180天内 |\n",
      "\n",
      "### 姜子牙\n",
      "\n",
      "IDEA研究院推出的大语言模型。API参考\n",
      "\n",
      "| 模型名称          | 输入输出成本       |\n",
      "|-------------------|--------------------|\n",
      "| ziya-llama-13b-v1 | 限时免费（需申请） |\n",
      "\n",
      "### BELLE\n",
      "\n",
      "BELLE推出的大语言模型。API参考\n",
      "\n",
      "| 模型名称              | 输入输出成本       |\n",
      "|-----------------------|--------------------|\n",
      "| belle-llama-13b-2m-v1 | 限时免费（需申请） |\n",
      "\n",
      "### 元语\n",
      "\n",
      "元语智能推出的大语言模型。API参考\n",
      "\n",
      "| 模型名称          | 输入输出成本       |\n",
      "|-------------------|--------------------|\n",
      "| chatyuan-large-v2 | 限时免费（需申请） |\n",
      "\n",
      "### BiLLa\n",
      "\n",
      "BiLLa是开源的推理能力增强的中英双语LLaMA模型，较大提升LLaMA的中文理解能力, 并尽可能减少对原始LLaMA英文能力的损伤。API参考\n",
      "\n",
      "| 模型名称        | 输入输出成本       |\n",
      "|-----------------|--------------------|\n",
      "| billa-7b-sft-v1 | 限时免费（需申请） |\n",
      "\n",
      "## 图像生成-通义万相与图像编辑\n",
      "\n",
      "### 文生图\n",
      "\n",
      "文生图V2版\n",
      "\n",
      "文生图V2系列模型是全面升级的文生图模型，您可以选择V2系列模型进行文生图创作。API参考 ｜ 在线体验\n",
      "\n",
      "| 模型名称          | 说明                                                            | 单价      | 免费额度（注）                   |\n",
      "|-------------------|-----------------------------------------------------------------|-----------|----------------------------------|\n",
      "| wanx2.1-t2i-plus  | 生成图像细节更丰富，速度较慢。对应通义万相官网2.1专业模型。     | 0.20元/张 | 各500张有效期：百炼开通后180天内 |\n",
      "| wanx2.1-t2i-turbo | 生成速度快、效果全面、性价比高。对应通义万相官网2.1极速模型。   | 0.14元/张 | 各500张有效期：百炼开通后180天内 |\n",
      "| wanx2.0-t2i-turbo | 擅长质感人像，速度中等、成本较低。对应通义万相官网2.0极速模型。 | 0.04元/张 | 各500张有效期：百炼开通后180天内 |\n",
      "\n",
      "| wanx2.1-t2i-plus                                                                                                                                                                                                                      | wanx2.1-t2i-turbo                                                                                                                                                                                                                     | wanx2.0-t2i-turbo                                                                                                                                                                                                                     |\n",
      "|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| 场景1：文字生成能力提示词：生成一张新年祝福贺卡，背景有白雪，放鞭炮的小孩，蛇形成文案2025，并写上HAPPY NEW YEAR。效果对比：wanx2.1模型（plus和turbo）的文字生成能力更强，适合创意设计场景。                                           | 场景1：文字生成能力提示词：生成一张新年祝福贺卡，背景有白雪，放鞭炮的小孩，蛇形成文案2025，并写上HAPPY NEW YEAR。效果对比：wanx2.1模型（plus和turbo）的文字生成能力更强，适合创意设计场景。                                           | 场景1：文字生成能力提示词：生成一张新年祝福贺卡，背景有白雪，放鞭炮的小孩，蛇形成文案2025，并写上HAPPY NEW YEAR。效果对比：wanx2.1模型（plus和turbo）的文字生成能力更强，适合创意设计场景。                                           |\n",
      "|                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                       |\n",
      "| 场景2：人像生成能力提示词：中国女孩，圆脸，看着镜头，优雅的民族服装，商业摄影，室外，电影级光照，半身特写，精致的淡妆，锐利的边缘。效果对比：wanx2.0模型在质感人像生成方面表现出色，其成本仅为wanx2.1 turbo模型的三分之一，性价比高。 | 场景2：人像生成能力提示词：中国女孩，圆脸，看着镜头，优雅的民族服装，商业摄影，室外，电影级光照，半身特写，精致的淡妆，锐利的边缘。效果对比：wanx2.0模型在质感人像生成方面表现出色，其成本仅为wanx2.1 turbo模型的三分之一，性价比高。 | 场景2：人像生成能力提示词：中国女孩，圆脸，看着镜头，优雅的民族服装，商业摄影，室外，电影级光照，半身特写，精致的淡妆，锐利的边缘。效果对比：wanx2.0模型在质感人像生成方面表现出色，其成本仅为wanx2.1 turbo模型的三分之一，性价比高。 |\n",
      "|                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                       |\n",
      "\n",
      "文生图V1版\n",
      "\n",
      "推荐您使用全面升级的文生图V2版模型。\n",
      "\n",
      "可以基于输入的文本生成图片。此外，还支持输入参考图片，并参考图片内容或者图片风格进行图片生成。API参考 | 在线体验\n",
      "\n",
      "| 模型名称   | 示例输入             | 示例输出   | 单价      | 免费额度（注）                 |\n",
      "|------------|----------------------|------------|-----------|--------------------------------|\n",
      "| wanx-v1    | 提示词：一只小狗在笑 |            | 0.16元/张 | 500张有效期：百炼开通后180天内 |\n",
      "\n",
      "### 通用图像编辑\n",
      "\n",
      "通义万相-通用图像编辑模型通过简单的指令即可实现多样化的图像编辑，适用于扩图、去水印、风格迁移、图像修复、图像美化等场景。API参考\n",
      "\n",
      "| 模型名称          | 计费单价   | 免费额度                                 |\n",
      "|-------------------|------------|------------------------------------------|\n",
      "| wanx2.1-imageedit | 0.14元/张  | 免费额度：500张有效期：百炼开通后180天内 |\n",
      "\n",
      "目前通用图像编辑支持以下功能：\n",
      "\n",
      "| 模型功能   | 输入图像                               | 输入提示词                                                 | 输出图像   |\n",
      "|------------|----------------------------------------|------------------------------------------------------------|------------|\n",
      "| 全局风格化 |                                        | 转换成法国绘本风格                                         |            |\n",
      "| 局部风格化 |                                        | 把房子变成木板风格。                                       |            |\n",
      "| 局部重绘   | 输入图像涂抹区域图像（白色为涂抹区域） | 一只陶瓷兔子抱着一朵陶瓷花。                               | 输出图像   |\n",
      "| 去文字水印 |                                        | 去除图像中的文字。                                         |            |\n",
      "| 扩图       |                                        | 一位绿色仙子。                                             |            |\n",
      "| 图像超分   | 模糊图像                               | 图像超分。                                                 | 清晰图像   |\n",
      "| 图像上色   |                                        | 蓝色背景，黄色的叶子。                                     |            |\n",
      "| 线稿生图   |                                        | 北欧极简风格的客厅。                                       |            |\n",
      "| 垫图       |                                        | 卡通形象小心翼翼地探出头，窥视着房间内一颗璀璨的蓝色宝石。 |            |\n",
      "\n",
      "### 涂鸦作画\n",
      "\n",
      "基于输入的手绘图加文字描述，即可生成精美的涂鸦绘画作品。API参考\n",
      "\n",
      "| 模型名称                  | 示例输入             | 示例输出   | 单价      | 免费额度（注）                 |\n",
      "|---------------------------|----------------------|------------|-----------|--------------------------------|\n",
      "| wanx-sketch-to-image-lite | 提示词：一棵参天大树 |            | 0.06元/张 | 500张有效期：百炼开通后180天内 |\n",
      "\n",
      "### 图像局部重绘\n",
      "\n",
      "根据用户输入的原始图片和局部涂抹图、prompt提示词文字内容，生成符合语义描述的多样化风格的局部重绘图像。API参考\n",
      "\n",
      "| 模型名称        | 示例输入                               | 示例输出   | 单价                                                         | 免费额度（注）                 |\n",
      "|-----------------|----------------------------------------|------------|--------------------------------------------------------------|--------------------------------|\n",
      "| wanx-x-painting | 布局涂抹图：提示词：一只狗戴着红色眼镜 |            | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：百炼开通后180天内 |\n",
      "\n",
      "### 人像风格重绘\n",
      "\n",
      "人像风格重绘可以将输入的人物图像进行多种风格化的重绘生成，使新生成的图像在兼顾原始人物相貌的同时，带来不同风格的绘画效果。API参考\n",
      "\n",
      "| 模型名称              | 示例输入       | 示例输出   | 单价      | 免费额度（注）                 |\n",
      "|-----------------------|----------------|------------|-----------|--------------------------------|\n",
      "| wanx-style-repaint-v1 | 风格：清雅国风 |            | 0.12元/张 | 500张有效期：百炼开通后180天内 |\n",
      "\n",
      "### 图像背景生成\n",
      "\n",
      "图像背景生成可以基于输入的前景图像素材拓展生成背景信息，实现自然的光影融合效果，与细腻的写实画面生成。支持文本描述、图像引导等多种方式，同时支持对生成的图像智能添加文字内容。API参考\n",
      "\n",
      "| 模型名称                      | 示例输入                                                         | 示例输出   | 单价      | 免费额度（注）                 |\n",
      "|-------------------------------|------------------------------------------------------------------|------------|-----------|--------------------------------|\n",
      "| wanx-background-generation-v2 | 提示词：在桌面上，旁边有插着花朵的花瓶，背后是纯色高级的背景墙。 |            | 0.08元/张 | 500张有效期：百炼开通后180天内 |\n",
      "\n",
      "### 图像画面扩展\n",
      "\n",
      "图像画面大模型，对输入图像进行画面自由扩展，支持旋转画面，支持按照扩展系数和扩展像素数两种方式进行扩图。用户可以通过指定宽度、高度画面扩展比例或者左、右、上、下的扩展的像素值来控制画面扩展，可用于创意娱乐、辅助作图、画面设计、影视后期制作等场景。API参考\n",
      "\n",
      "| 模型名称           | 示例输入   | 示例输出   | 单价                                                         | 免费额度（注）                 |\n",
      "|--------------------|------------|------------|--------------------------------------------------------------|--------------------------------|\n",
      "| image-out-painting |            |            | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：百炼开通后180天内 |\n",
      "\n",
      "### 人物实例分割\n",
      "\n",
      "输入人物图像，模型识别出图像中的不同人物对象并画出每个对象边界的像素级掩码。API参考\n",
      "\n",
      "| 模型名称                    | 示例输入   | 示例输出                                       | 单价                                                         | 免费额度（注）                 |\n",
      "|-----------------------------|------------|------------------------------------------------|--------------------------------------------------------------|--------------------------------|\n",
      "| image-instance-segmentation |            | 输出结果1：像素级掩码图像输出结果2：可视化图像 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：百炼开通后180天内 |\n",
      "\n",
      "### 图像擦除补全\n",
      "\n",
      "输入图像并指定待擦除区域掩码图像以及保留区域掩码图像，模型在保留原图背景的同时擦除指定图像区域。API参考\n",
      "\n",
      "针对人物图像的擦除、补全，推荐通过人物实例分割得到图像中不同人物对象的图像掩码，选择完整的人物图像掩码擦除一个或多个人物。\n",
      "\n",
      "| 模型名称               | 示例输入               | 示例输出   | 单价                                                         | 免费额度（注）                 |\n",
      "|------------------------|------------------------|------------|--------------------------------------------------------------|--------------------------------|\n",
      "| image-erase-completion | 原图待擦除区域保留区域 |            | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：百炼开通后180天内 |\n",
      "\n",
      "### 动漫人物生成\n",
      "\n",
      "Cosplay动漫人物生成通过输入人像图片和卡通形象图片，可快速生成人物卡通写真。API参考\n",
      "\n",
      "| 模型名称              | 示例输入         | 示例输出   | 单价                                                         | 免费额度（注）                 |\n",
      "|-----------------------|------------------|------------|--------------------------------------------------------------|--------------------------------|\n",
      "| wanx-style-cosplay-v1 | 人脸图像模板图像 |            | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 300张有效期：百炼开通后180天内 |\n",
      "\n",
      "### 虚拟模特\n",
      "\n",
      "可以对上传的真人实拍商品展示图进行智能生成，将其中的模特和背景替换为心仪的内容，在保持人物姿态不变的情况下，使用虚拟模特对商品进行更加精美、多样的展示。支持各种与模特产生互动的商品，如手持小商品、服装、鞋靴、配饰等。API参考\n",
      "\n",
      "| 模型名称          | 版本   | 模型简介                                                                                     | 单价                                                         | 免费额度（注）                 |\n",
      "|-------------------|--------|----------------------------------------------------------------------------------------------|--------------------------------------------------------------|--------------------------------|\n",
      "| wanx-virtualmodel | V1     | 支持真人实拍图上传图片短边：512像素或1024像素                                                | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：百炼开通后180天内 |\n",
      "| virtualmodel-v2   | V2     | 支持真人、人台实拍图上传图片短边为：1024像素或2048像素支持改变图片的长宽比文本引导效果更准确 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：百炼开通后180天内 |\n",
      "\n",
      "| 输入图    | 参数配置                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 输出图   |\n",
      "|-----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------|\n",
      "| v1 真人图 | \"prompt\":\"一位年轻男性站着摆拍，在空荡的卧室里，窗户旁边，阳光照射进来，highly detailed，8K，极简主义风格\"\"face_prompt\":\"英俊的男性，脸好，脸美，质量上乘，杰作，（逼真度：1.4）\"\"predefined_face_id\":\"boy3\"                                                                                                                                                                                                                                                                                                                            | v1输出   |\n",
      "| v2人台图  | \"prompt\":\"A woman stands beside a luxurious swimming pool, her attire and posture suggesting leisure and relaxation. The pool's calm, crystal-clear waters reflect the surrounding opulent setting, with elegant lounge chairs inviting moments of repose under the sun. Perhaps it's a high-end resort or an upscale private villa, where the tiled pool deck and meticulously landscaped greenery speak of exclusivity and refinement.\"\"face_prompt\":\"good face, beautiful face, best quality.\"\"aspect_ratio\":\"4:3\"\"realPerson\":false | v2输出   |\n",
      "\n",
      "### 鞋靴模特\n",
      "\n",
      "鞋靴模特支持输入多视角鞋靴系列图片，同时对输入模特模板图的鞋子区域进行鞋靴AI试穿，实现模特鞋靴布局重绘生成，最终生成图片的效果，布局自然、细节丰富、画面细腻、试穿结果逼真。可用于模特商品图设计、新鞋AI试穿、模特穿戴布局重绘等场景。API参考\n",
      "\n",
      "| 模型名称     | 示例输入   | 示例输出   | 单价                                                         | 免费额度（注）                 |\n",
      "|--------------|------------|------------|--------------------------------------------------------------|--------------------------------|\n",
      "| shoemodel-v1 |            |            | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：百炼开通后180天内 |\n",
      "\n",
      "### 创意海报生成\n",
      "\n",
      "根据您的要求自动生成海报的背景和文字排版，支持多种海报风格。无需设计基础，轻松制作出彩作品，让创意触手可及。API参考\n",
      "\n",
      "| 模型名称                  | 示例输入                                                                                                                                                                | 示例输出   | 单价                                                         | 免费额度（注）                 |\n",
      "|---------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|--------------------------------------------------------------|--------------------------------|\n",
      "| wanx-poster-generation-v1 | \"title\":\"元宵节\",\"sub_title\":\"正月十五\",\"body_text\":\"团圆时节，汤圆香甜，祝你幸福美满！\",\"prompt_text_zh\":\"灯笼，小猫，梅花\",\"wh_ratios\":\"竖版\",\"lora_name\":\"童话油画\", |            | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：百炼开通后180天内 |\n",
      "\n",
      "### 图配文\n",
      "\n",
      "您只要输入背景图和文字，就能将文字排版到图片上，形成一张完整的图文海报。API参考\n",
      "\n",
      "| 模型名称   | 示例输入   | 示例输入                                                                                                          | 示例输出   | 单价     |\n",
      "|------------|------------|-------------------------------------------------------------------------------------------------------------------|------------|----------|\n",
      "| wanx-ast   |            | \"title\":\"Lorem Ipsum\",\"subtitle\":\"Duis aute irure dolor in reprehenderit\",\"text\":\"VIEW NOW\",\"underlay\": 1,\"logo\": |            | 限时免费 |\n",
      "\n",
      "### 人物写真生成-FaceChain\n",
      "\n",
      "- 人物图像检测：对用户上传的人物图像进行检测，判断其中所包含的人脸是否符合Facechain微调所需的标准，检测维度包括人脸数量、大小、角度、光照、清晰度等多维度，支持图像组输入，并返回每张图像对应的检测结果。API参考\n",
      "- 人物形象训练：对上传的图像进行模型训练，从而获得该图像中对应人物的resource，基于该resource可以实现人物的写真生成。API参考\n",
      "- 人物写真生成：基于人物形象训练已经得到的形象，可以继续通过人物生成写真模型完成该形象的写真生成，支持多种预设风格，包括证件照、商务写真等。API参考\n",
      "\n",
      "| 模型名称             | 说明         | 示例输入       | 示例输出   | 单价      | 免费额度（注）                 |\n",
      "|----------------------|--------------|----------------|------------|-----------|--------------------------------|\n",
      "| facechain-facedetect | 人物图像检测 | 风格：商务写真 |            | 限时免费  | 限时免费                       |\n",
      "| facechain-finetune   | 人物形象训练 | 风格：商务写真 |            | 2.5元/次  | 50次有效期：申请通过后180天内  |\n",
      "| facechain-generation | 人物写真生成 | 风格：商务写真 |            | 0.18元/张 | 500张有效期：申请通过后180天内 |\n",
      "\n",
      "### 创意文字生成-WordArt锦书\n",
      "\n",
      "- 文字纹理生成：可以对输入的文字内容或文字图片进行创意设计，根据提示词内容对文字添加材质和纹理，实现立体凸显或场景融合的效果，生成效果精美、风格多样的艺术字，结合背景可以直接作为文字海报使用。API参考\n",
      "- 文字变形：可以对输入的文字边缘轮廓进行创意变形，根据提示词内容进行边缘变化，实现一种字体的更多种创意用法，返回带有文字内容的黑底白色mask图。API参考\n",
      "- 百家姓生成：可以输入姓氏文字进行创意设计，支持根据提示词和风格引导图进行自定义设计，同时提供多种精美的预设风格模板，生成图片可以应用于个性社交场景，如作为个人头像、屏幕壁纸、字体表情包等。API参考该模型将于2025年6月正式下线。请您在下线之前及时切换至其他模型，以免影响您的业务。\n",
      "\n",
      "| 模型名称         | 说明         | 示例输入                                           | 示例输出   | 单价      | 免费额度（注）                   |\n",
      "|------------------|--------------|----------------------------------------------------|------------|-----------|----------------------------------|\n",
      "| wordart-texture  | 文字纹理生成 | 提示词：精美玉石风格类型：立体材质                 |            | 0.08元/张 | 各500张有效期：百炼开通后180天内 |\n",
      "| wordart-semantic | 文字变形     | 文字：桂林山水提示词：山峦叠嶂、漓江蜿蜒、岩石奇秀 |            | 0.24元/张 | 各500张有效期：百炼开通后180天内 |\n",
      "| wordart-surnames | 百家姓生成   | 百家姓：沈风格：奇幻楼阁                           |            | 暂无      | 各500张有效期：百炼开通后180天内 |\n",
      "\n",
      "### AI试衣\n",
      "\n",
      "- AI试衣一款虚拟试衣图片生成模型，基于人像照片及服装图生成穿着后的试衣图片。API参考\n",
      "- AI试衣-图片分割支持对模特图、服饰图进行分割，可用于AI试衣图片的前后处理。API参考\n",
      "- AI试衣-图片精修是对AI试衣生成的效果图进行二次生成，输出还原度更高的精修试衣效果图。API参考\n",
      "\n",
      "| 模型名称           | 说明            | 示例输入   | 示例输出   | 免费额度（注）                   |\n",
      "|--------------------|-----------------|------------|------------|----------------------------------|\n",
      "| aitryon            | AI试衣          |            |            | 各400张有效期：百炼开通后180天内 |\n",
      "| aitryon-parsing-v1 | AI试衣-图片分割 |            |            | 各400张有效期：百炼开通后180天内 |\n",
      "| aitryon-refiner    | AI试衣-图片精修 |            |            | 100张有效期：百炼开通后180天内   |\n",
      "\n",
      "AI试衣计费单价\n",
      "\n",
      "| 模型服务        | 模型名称           | 计量单价   | 折扣   | 阶梯层级                     |\n",
      "|-----------------|--------------------|------------|--------|------------------------------|\n",
      "| AI试衣          | aitryon            | 0.20元/张  | 无     | 无                           |\n",
      "| AI试衣-图片分割 | aitryon-parsing-v1 | 0.004元/张 | 无     | 无                           |\n",
      "| AI试衣-图片精修 | aitryon-refiner    | 0.30元/张  | 无     | 生成数量 ≤ 25张              |\n",
      "| AI试衣-图片精修 | aitryon-refiner    | 0.275元/张 | 9.2折  | 25张 ＜ 生成数量 ≤ 125张     |\n",
      "| AI试衣-图片精修 | aitryon-refiner    | 0.25元/张  | 8.4折  | 125张 ＜ 生成数量 ≤ 250张    |\n",
      "| AI试衣-图片精修 | aitryon-refiner    | 0.225元/张 | 7.5折  | 250张 ＜ 生成数量 ≤ 1250张   |\n",
      "| AI试衣-图片精修 | aitryon-refiner    | 0.2元/张   | 6.7折  | 1250张 ＜ 生成数量 ≤ 2500张  |\n",
      "| AI试衣-图片精修 | aitryon-refiner    | 0.175元/张 | 5.8折  | 2500张 ＜ 生成数量 ≤ 2.5万张 |\n",
      "| AI试衣-图片精修 | aitryon-refiner    | 0.15元/张  | 5折    | 生成数量 ＞ 2.5万张          |\n",
      "\n",
      "## 图像生成-第三方模型\n",
      "\n",
      "### Stable Diffusion\n",
      "\n",
      "API参考\n",
      "\n",
      "| 模型名称                         | 说明                                                                                                                                                                                                                                              | 单价                                                         | 免费额度（注）                 |\n",
      "|----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------|--------------------------------|\n",
      "| stable-diffusion-3.5-large       | 具有8亿参数的多模态扩散变压器（MMDiT）文本到图像生成模型，具备卓越的图像质量和提示词匹配度，支持生成100万像素的高分辨率图像，且能够在普通消费级硬件上高效运行。相比于v1.5和xl，在图像质量、文本内容生成、复杂提示理解和资源效率方面均有显著提升。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：申请通过后180天内 |\n",
      "| stable-diffusion-3.5-large-turbo | 在stable-diffusion-3.5-large的基础上采用对抗性扩散蒸馏（ADD）技术的模型，具备更快的速度。                                                                                                                                                         | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：申请通过后180天内 |\n",
      "| stable-diffusion-xl              | 相比v1.5做了重大改进，被认为是当前开源文生图模型的SOTA水准，具体改进包括：unet backbone是之前的3倍；增加了refinement模块用于改善生成图片的质量；更高效的训练技巧等。                                                                              | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：申请通过后180天内 |\n",
      "| stable-diffusion-v1.5            | 通过clip模型将文本的embedding和图片embedding映射到相同空间，从而通过输入文本并结合unet的稳定扩散预测噪声的能力，生成图片。是一款基础的文生图模型，得到了业界广泛使用。                                                                            | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 500张有效期：申请通过后180天内 |\n",
      "\n",
      "### FLUX\n",
      "\n",
      "Black Forest Labs的开源文生图模型，尤其擅长生成包含文字、多主体、手部细节的图片。\n",
      "\n",
      "API详情 | 在线体验\n",
      "\n",
      "| 模型名称     | 说明                                                                                   | 单价                                                         | 免费额度（注）                  |\n",
      "|--------------|----------------------------------------------------------------------------------------|--------------------------------------------------------------|---------------------------------|\n",
      "| flux-merged  | 结合了flux-dev的深度和flux-schnell的快速执行。                                         | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 1000张有效期：百炼开通后180天内 |\n",
      "| flux-dev     | 开发者版，面向非商业应用，具有与专业版相近的图像质量和指令遵循能力，同时运行效率更高。 | 目前仅供免费体验。免费额度用完后不可调用，敬请关注后续动态。 | 1000张有效期：百炼开通后180天内 |\n",
      "| flux-schnell | 快速版，轻量级模型。                                                                   | 计费方案即将推出。                                           | 1000张有效期：百炼开通后180天内 |\n",
      "\n",
      "## 语音合成（文本转语音）\n",
      "\n",
      "### CosyVoice\n",
      "\n",
      "CosyVoice是通义实验室依托大规模预训练语言模型，深度融合文本理解和语音生成的新一代生成式语音合成大模型，支持文本至语音的实时流式合成。API参考 | 在线体验\n",
      "\n",
      "| 模型名称     | 单价                                                                                            | 免费额度                     |\n",
      "|--------------|-------------------------------------------------------------------------------------------------|------------------------------|\n",
      "| cosyvoice-v1 | 2元/万字符根据待合成字符数计费（其中1个汉字算2个字符，英文、标点符号、空格均按照1个字符计费）。 | 每主账号每模型每月2000字符。 |\n",
      "\n",
      "音色列表：\n",
      "\n",
      "| 模型名称     | voice参数     | 音色     | 音频试听   | 适用场景                                                     | 语言         |   默认采样率（Hz） | 默认音频格式   |\n",
      "|--------------|---------------|----------|------------|--------------------------------------------------------------|--------------|--------------------|----------------|\n",
      "| cosyvoice-v1 | longxiaochun  | 龙小淳   |            | 语音助手、导航播报、聊天数字人                               | 中文+英文    |              22050 | mp3            |\n",
      "| cosyvoice-v1 | longxiaoxia   | 龙小夏   |            | 语音助手、聊天数字人                                         | 中文         |              22050 | mp3            |\n",
      "| cosyvoice-v1 | longxiaocheng | 龙小诚   |            | 语音助手、导航播报、聊天数字人                               | 中文+英文    |              22050 | mp3            |\n",
      "| cosyvoice-v1 | longxiaobai   | 龙小白   |            | 聊天数字人、有声书、语音助手                                 | 中文         |              22050 | mp3            |\n",
      "| cosyvoice-v1 | longlaotie    | 龙老铁   |            | 新闻播报、有声书、语音助手、直播带货、导航播报               | 中文东北口音 |              22050 | mp3            |\n",
      "| cosyvoice-v1 | longshu       | 龙书     |            | 有声书、语音助手、导航播报、新闻播报、智能客服               | 中文         |              22050 | mp3            |\n",
      "| cosyvoice-v1 | longshuo      | 龙硕     |            | 语音助手、导航播报、新闻播报、客服催收                       | 中文         |              22050 | mp3            |\n",
      "| cosyvoice-v1 | longjing      | 龙婧     |            | 语音助手、导航播报、新闻播报、客服催收                       | 中文         |              22050 | mp3            |\n",
      "| cosyvoice-v1 | longmiao      | 龙妙     |            | 客服催收、导航播报、有声书、语音助手                         | 中文         |              22050 | mp3            |\n",
      "| cosyvoice-v1 | longyue       | 龙悦     |            | 语音助手、诗词朗诵、有声书朗读、导航播报、新闻播报、客服催收 | 中文         |              22050 | mp3            |\n",
      "| cosyvoice-v1 | longyuan      | 龙媛     |            | 有声书、语音助手、聊天数字人                                 | 中文         |              22050 | mp3            |\n",
      "| cosyvoice-v1 | longfei       | 龙飞     |            | 会议播报、新闻播报、有声书                                   | 中文         |              22050 | mp3            |\n",
      "| cosyvoice-v1 | longjielidou  | 龙杰力豆 |            | 新闻播报、有声书、聊天助手                                   | 中文+英文    |              22050 | mp3            |\n",
      "| cosyvoice-v1 | longtong      | 龙彤     |            | 有声书、导航播报、聊天数字人                                 | 中文         |              22050 | mp3            |\n",
      "| cosyvoice-v1 | longxiang     | 龙祥     |            | 新闻播报、有声书、导航播报                                   | 中文         |              22050 | mp3            |\n",
      "| cosyvoice-v1 | loongstella   | Stella   |            | 语音助手、直播带货、导航播报、客服催收、有声书               | 中文+英文    |              22050 | mp3            |\n",
      "| cosyvoice-v1 | loongbella    | Bella    |            | 语音助手、客服催收、新闻播报、导航播报                       | 中文         |              22050 | mp3            |\n",
      "\n",
      "### Sambert\n",
      "\n",
      "Sambert语音合成API基于达摩院改良的自回归韵律模型，支持文本至语音的实时流式合成。API参考\n",
      "\n",
      "| 模型名称   | 单价                                                                                                                | 免费额度                    |\n",
      "|------------|---------------------------------------------------------------------------------------------------------------------|-----------------------------|\n",
      "| 见下表     | 1元/万字符根据待合成字符数计费（其中1个汉字算2个字符，英文、标点符号、空格均按照1个字符计费）。SSML标签内容不计费。 | 每主账号每模型每月3万字符。 |\n",
      "\n",
      "模型（音色）列表：\n",
      "\n",
      "| 模型名称               | 音色           | 音频试听   | 时间戳支持   | 适用场景                             | 特色         | 语言      | 默认采样率（Hz）   |\n",
      "|------------------------|----------------|------------|--------------|--------------------------------------|--------------|-----------|--------------------|\n",
      "| sambert-zhinan-v1      | 知楠           |            | 是           | 通用场景                             | 广告男声     | 中文+英文 | 48k                |\n",
      "| sambert-zhiqi-v1       | 知琪           |            | 是           | 通用场景                             | 温柔女声     | 中文+英文 | 48k                |\n",
      "| sambert-zhichu-v1      | 知厨           |            | 是           | 新闻播报                             | 舌尖男声     | 中文+英文 | 48k                |\n",
      "| sambert-zhide-v1       | 知德           |            | 是           | 新闻播报                             | 新闻男声     | 中文+英文 | 48k                |\n",
      "| sambert-zhijia-v1      | 知佳           |            | 是           | 新闻播报                             | 标准女声     | 中文+英文 | 48k                |\n",
      "| sambert-zhiru-v1       | 知茹           |            | 是           | 新闻播报                             | 新闻女声     | 中文+英文 | 48k                |\n",
      "| sambert-zhiqian-v1     | 知倩           |            | 是           | 配音解说、新闻播报                   | 资讯女声     | 中文+英文 | 48k                |\n",
      "| sambert-zhixiang-v1    | 知祥           |            | 是           | 配音解说                             | 磁性男声     | 中文+英文 | 48k                |\n",
      "| sambert-zhiwei-v1      | 知薇           |            | 是           | 阅读产品简介                         | 萝莉女声     | 中文+英文 | 48k                |\n",
      "| sambert-zhihao-v1      | 知浩           |            | 是           | 通用场景                             | 咨询男声     | 中文+英文 | 16k                |\n",
      "| sambert-zhijing-v1     | 知婧           |            | 是           | 通用场景                             | 严厉女声     | 中文+英文 | 16k                |\n",
      "| sambert-zhiming-v1     | 知茗           |            | 是           | 通用场景                             | 诙谐男声     | 中文+英文 | 16k                |\n",
      "| sambert-zhimo-v1       | 知墨           |            | 是           | 通用场景                             | 情感男声     | 中文+英文 | 16k                |\n",
      "| sambert-zhina-v1       | 知娜           |            | 是           | 通用场景                             | 浙普女声     | 中文+英文 | 16k                |\n",
      "| sambert-zhishu-v1      | 知树           |            | 是           | 通用场景                             | 资讯男声     | 中文+英文 | 16k                |\n",
      "| sambert-zhistella-v1   | 知莎           |            | 是           | 通用场景                             | 知性女声     | 中文+英文 | 16k                |\n",
      "| sambert-zhiting-v1     | 知婷           |            | 是           | 通用场景                             | 电台女声     | 中文+英文 | 16k                |\n",
      "| sambert-zhixiao-v1     | 知笑           |            | 是           | 通用场景                             | 资讯女声     | 中文+英文 | 16k                |\n",
      "| sambert-zhiya-v1       | 知雅           |            | 是           | 通用场景                             | 严厉女声     | 中文+英文 | 16k                |\n",
      "| sambert-zhiye-v1       | 知晔           |            | 是           | 通用场景                             | 青年男声     | 中文+英文 | 16k                |\n",
      "| sambert-zhiying-v1     | 知颖           |            | 是           | 通用场景                             | 软萌童声     | 中文+英文 | 16k                |\n",
      "| sambert-zhiyuan-v1     | 知媛           |            | 是           | 通用场景                             | 知心姐姐     | 中文+英文 | 16k                |\n",
      "| sambert-zhiyue-v1      | 知悦           |            | 是           | 客服                                 | 温柔女声     | 中文+英文 | 16k                |\n",
      "| sambert-zhigui-v1      | 知柜           |            | 是           | 阅读产品简介                         | 直播女声     | 中文+英文 | 16k                |\n",
      "| sambert-zhishuo-v1     | 知硕           |            | 是           | 数字人                               | 自然男声     | 中文+英文 | 16k                |\n",
      "| sambert-zhimiao-emo-v1 | 知妙（多情感） |            | 是           | 阅读产品简介、数字人、直播           | 多种情感女声 | 中文+英文 | 16k                |\n",
      "| sambert-zhimao-v1      | 知猫           |            | 是           | 阅读产品简介、配音解说、数字人、直播 | 直播女声     | 中文+英文 | 16k                |\n",
      "| sambert-zhilun-v1      | 知伦           |            | 是           | 配音解说                             | 悬疑解说     | 中文+英文 | 16k                |\n",
      "| sambert-zhifei-v1      | 知飞           |            | 是           | 配音解说                             | 激昂解说     | 中文+英文 | 16k                |\n",
      "| sambert-zhida-v1       | 知达           |            | 是           | 新闻播报                             | 标准男声     | 中文+英文 | 16k                |\n",
      "| sambert-camila-v1      | Camila         |            | 否           | 通用场景                             | 西班牙语女声 | 西班牙语  | 16k                |\n",
      "| sambert-perla-v1       | Perla          |            | 否           | 通用场景                             | 意大利语女声 | 意大利语  | 16k                |\n",
      "| sambert-indah-v1       | Indah          |            | 否           | 通用场景                             | 印尼语女声   | 印尼语    | 16k                |\n",
      "| sambert-clara-v1       | Clara          |            | 否           | 通用场景                             | 法语女声     | 法语      | 16k                |\n",
      "| sambert-hanna-v1       | Hanna          |            | 否           | 通用场景                             | 德语女声     | 德语      | 16k                |\n",
      "| sambert-beth-v1        | Beth           |            | 是           | 通用场景                             | 咨询女声     | 美式英文  | 16k                |\n",
      "| sambert-betty-v1       | Betty          |            | 是           | 通用场景                             | 客服女声     | 美式英文  | 16k                |\n",
      "| sambert-cally-v1       | Cally          |            | 是           | 通用场景                             | 自然女声     | 美式英文  | 16k                |\n",
      "| sambert-cindy-v1       | Cindy          |            | 是           | 通用场景                             | 对话女声     | 美式英文  | 16k                |\n",
      "| sambert-eva-v1         | Eva            |            | 是           | 通用场景                             | 陪伴女声     | 美式英文  | 16k                |\n",
      "| sambert-donna-v1       | Donna          |            | 是           | 通用场景                             | 教育女声     | 美式英文  | 16k                |\n",
      "| sambert-brian-v1       | Brian          |            | 是           | 通用场景                             | 客服男声     | 美式英文  | 16k                |\n",
      "| sambert-waan-v1        | Waan           |            | 否           | 通用场景                             | 泰语女声     | 泰语      | 16k                |\n",
      "\n",
      "## 语音识别（语音转文本）与翻译（语音转成指定语种的文本）\n",
      "\n",
      "### Gummy\n",
      "\n",
      "Gummy大模型支持实时语音识别与翻译，能够精准识别中、英、日、韩等10种语言。此外，它还支持中、英、日、韩之间的互译，以及其他6种语言单向翻译成中文或英文。API参考\n",
      "\n",
      "| 模型名称          | 支持的语言                                                                                                                              | 支持的采样率   | 适用场景                                                   | 支持的音频格式                       | 单价         | 免费额度                                                                                                           |\n",
      "|-------------------|-----------------------------------------------------------------------------------------------------------------------------------------|----------------|------------------------------------------------------------|--------------------------------------|--------------|--------------------------------------------------------------------------------------------------------------------|\n",
      "| gummy-realtime-v1 | 中文、英文、日语、韩语、粤语、德语、法语、俄语、意大利语、西班牙语翻译语言对：中 → 英/日/韩英 → 中/日/韩日/韩/粤/德/法/俄/意/西 → 中/英 | 16kHz及以上    | 会议演讲、视频直播等长时间不间断识别的场景                 | pcm、wav、mp3、opus、speex、aac、amr | 0.00015元/秒 | 36,000秒（10小时）2025年1月17日0点前开通百炼：有效期至2025年7月15日2025年1月17日0点后开通百炼：自开通日起180天有效 |\n",
      "| gummy-chat-v1     | 中文、英文、日语、韩语、粤语、德语、法语、俄语、意大利语、西班牙语翻译语言对：中 → 英/日/韩英 → 中/日/韩日/韩/粤/德/法/俄/意/西 → 中/英 | 16kHz          | 对话聊天、指令控制、语音输入法、语音搜索等短时语音交互场景 | pcm、wav、mp3、opus、speex、aac、amr | 0.00015元/秒 | 36,000秒（10小时）2025年1月17日0点前开通百炼：有效期至2025年7月15日2025年1月17日0点后开通百炼：自开通日起180天有效 |\n",
      "\n",
      "### Paraformer\n",
      "\n",
      "录音文件识别\n",
      "\n",
      "API参考 | 在线体验\n",
      "\n",
      "| 模型名称          | 支持的语言                                                                                                                                                                                                   | 支持的采样率   | 适用场景   | 支持的音频格式                                                                          | 单价         | 免费额度                                        |\n",
      "|-------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|------------|-----------------------------------------------------------------------------------------|--------------|-------------------------------------------------|\n",
      "| paraformer-v2     | 中文普通话、中文方言（粤语、吴语、闽南语、东北话、甘肃话、贵州话、河南话、湖北话、湖南话、宁夏话、山西话、陕西话、山东话、四川话、天津话、江西话、云南话、上海话）、英语、日语、韩语、德语、法语、俄语       | 任意           | 视频直播   | aac、amr、avi、flac、flv、m4a、mkv、mov、mp3、mp4、mpeg、ogg、opus、wav、webm、wma、wmv | 0.00008元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\n",
      "| paraformer-8k-v2  | 中文普通话                                                                                                                                                                                                   | 8kHz           | 电话语音   | aac、amr、avi、flac、flv、m4a、mkv、mov、mp3、mp4、mpeg、ogg、opus、wav、webm、wma、wmv | 0.00008元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\n",
      "| paraformer-v1     | 中文普通话、英语                                                                                                                                                                                             | 任意           | 音频或视频 | aac、amr、avi、flac、flv、m4a、mkv、mov、mp3、mp4、mpeg、ogg、opus、wav、webm、wma、wmv | 0.00008元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\n",
      "| paraformer-8k-v1  | 中文普通话                                                                                                                                                                                                   | 8kHz           | 电话语音   | aac、amr、avi、flac、flv、m4a、mkv、mov、mp3、mp4、mpeg、ogg、opus、wav、webm、wma、wmv | 0.00008元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\n",
      "| paraformer-mtl-v1 | 中文普通话、中文方言（粤语、吴语、闽南语、东北话、甘肃话、贵州话、河南话、湖北话、湖南话、宁夏话、山西话、陕西话、山东话、四川话、天津话）、英语、日语、韩语、西班牙语、印尼语、法语、德语、意大利语、马来语 | 16kHz及以上    | 音频或视频 | aac、amr、avi、flac、flv、m4a、mkv、mov、mp3、mp4、mpeg、ogg、opus、wav、webm、wma、wmv | 0.00008元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\n",
      "\n",
      "实时语音识别\n",
      "\n",
      "API参考 | 在线体验\n",
      "\n",
      "| 模型名称                  | 支持的语言                                                                                                                                                                                               | 支持的采样率   | 适用场景         | 支持的音频格式                       | 单价         | 免费额度                                        |\n",
      "|---------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|------------------|--------------------------------------|--------------|-------------------------------------------------|\n",
      "| paraformer-realtime-v2    | 中文普通话、中文方言（粤语、吴语、闽南语、东北话、甘肃话、贵州话、河南话、湖北话、湖南话、宁夏话、山西话、陕西话、山东话、四川话、天津话、江西话、云南话、上海话）、英语、日语、韩语支持多个语种自由切换 | 任意           | 视频直播、会议等 | pcm、wav、mp3、opus、speex、aac、amr | 0.00024元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\n",
      "| paraformer-realtime-v1    | 中文                                                                                                                                                                                                     | 16kHz          | 视频直播、会议等 | pcm、wav、mp3、opus、speex、aac、amr | 0.00024元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\n",
      "| paraformer-realtime-8k-v2 | 中文                                                                                                                                                                                                     | 8kHz           | 电话客服等       | pcm、wav、mp3、opus、speex、aac、amr | 0.00024元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\n",
      "| paraformer-realtime-8k-v1 | 中文                                                                                                                                                                                                     | 8kHz           | 电话客服等       | pcm、wav、mp3、opus、speex、aac、amr | 0.00024元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\n",
      "\n",
      "### SenseVoice\n",
      "\n",
      "录音文件识别\n",
      "\n",
      "专注于高精度多语言语音识别，还能识别情绪（高兴、悲伤、生气等）和特定事件（背景音乐、歌唱、掌声和笑声等）。API参考\n",
      "\n",
      "| 模型名称      | 支持的语言                                             | 支持的格式                                                                                          | 单价         | 免费额度                                        |\n",
      "|---------------|--------------------------------------------------------|-----------------------------------------------------------------------------------------------------|--------------|-------------------------------------------------|\n",
      "| sensevoice-v1 | 超过50种语言（中、英、日、韩、粤等）附录：支持语言列表 | 音频或视频：aac、amr、avi、flac、flv、m4a、mkv、mov、mp3、mp4、mpeg、ogg、opus、wav、webm、wma、wmv | 0.0007 元/秒 | 36,000秒（10小时）每月1日0点自动发放有效期1个月 |\n",
      "\n",
      "## 视频生成-通义万相与视频编辑\n",
      "\n",
      "### 文生视频\n",
      "\n",
      "通义万相-文生视频模型通过一句话即可生成视频，视频呈现丰富的艺术风格及影视级画质。API参考 ｜在线体验\n",
      "\n",
      "| 模型名称          | 说明                           | 单价      | 免费额度                         |\n",
      "|-------------------|--------------------------------|-----------|----------------------------------|\n",
      "| wanx2.1-t2v-turbo | 生成速度更快，表现均衡。       | 0.24元/秒 | 各200秒有效期：百炼开通后180天内 |\n",
      "| wanx2.1-t2v-plus  | 生成细节更丰富，画面更具质感。 | 0.70元/秒 | 各200秒有效期：百炼开通后180天内 |\n",
      "\n",
      "| 输入示例                         | 输出视频   |\n",
      "|----------------------------------|------------|\n",
      "| 输入提示词：一只小猫在月光下奔跑 |            |\n",
      "\n",
      "### 图生视频\n",
      "\n",
      "通义万相-图生视频模型将输入图片作为视频首帧，再根据提示词生成视频。视频呈现丰富的艺术风格及影视级画质。API参考 ｜在线体验\n",
      "\n",
      "| 模型名称          | 说明                                                   | 单价      | 免费额度                         |\n",
      "|-------------------|--------------------------------------------------------|-----------|----------------------------------|\n",
      "| wanx2.1-i2v-turbo | 生成速度更快，耗时仅为plus模型的三分之一，性价比更高。 | 0.24元/秒 | 各200秒有效期：百炼开通后180天内 |\n",
      "| wanx2.1-i2v-plus  | 生成细节更丰富，画面更具质感。                         | 0.70元/秒 | 各200秒有效期：百炼开通后180天内 |\n",
      "\n",
      "| 输入示例                                 | 输出视频                                                                          |\n",
      "|------------------------------------------|-----------------------------------------------------------------------------------|\n",
      "| 输入提示词：一只猫在草地上奔跑输入图片： | 输出视频：将图片作为视频的第一帧，再根据提示词生成视频。模型：wanx2.1-i2v-turbo。 |\n",
      "\n",
      "### 舞动人像AnimateAnyone\n",
      "\n",
      "基于人物图片和人物动作模板，生成人物动作视频。直接使用时需依次调用下述三个模型。AnimateAnyone图像检测 API详情 | AnimateAnyone 动作模板生成API详情｜ AnimateAnyone视频生成API详情\n",
      "\n",
      "| 模型名称                     | 说明                                       | 单价       | 免费额度                          |\n",
      "|------------------------------|--------------------------------------------|------------|-----------------------------------|\n",
      "| animate-anyone-detect-gen2   | 检测输入的图片是否符合要求                 | 0.004元/张 | 200张有效期：百炼开通后180天内    |\n",
      "| animate-anyone-template-gen2 | 从人物运动视频中提取人物动作并生成动作模板 | 0.08元/秒  | 各1800秒有效期：百炼开通后180天内 |\n",
      "| animate-anyone-gen2          | 基于人物图片和动作模板生成人物动作视频     | 0.08元/秒  | 各1800秒有效期：百炼开通后180天内 |\n",
      "\n",
      "下面两个模型支持独立部署。模型部署后，模型调用参考这两个API详情。AnimateAnyone图像检测 API详情 | AnimateAnyone视频生成API详情\n",
      "\n",
      "| 模型名称              | 说明                                   | 单价                                                                                  | 免费额度   |\n",
      "|-----------------------|----------------------------------------|---------------------------------------------------------------------------------------|------------|\n",
      "| animate-anyone-detect | 检测输入图片是否符合要求               | 当前仅支持部署后调用，仅收取部署费用。部署单价：10000元/算力单元/月20元/算力单元/小时 | 无         |\n",
      "| animate-anyone        | 基于人物图片和动作模板生成人物动作视频 | 当前仅支持部署后调用，仅收取部署费用。部署单价：10000元/算力单元/月20元/算力单元/小时 | 无         |\n",
      "\n",
      "舞动人像模型效果示例\n",
      "\n",
      "| 输入：人物图片   | 输入：动作视频   | 输出（按图片背景生成）   | 输出（按视频背景生成）   |\n",
      "|------------------|------------------|--------------------------|--------------------------|\n",
      "|                  |                  |                          |                          |\n",
      "\n",
      "- 以上示例，由集成了“舞动人像AnimateAnyone”的通义APP生成。\n",
      "- 舞动人像AnimateAnyone模型的生成内容为视频画面，不包含音频。\n",
      "\n",
      "### 悦动人像EMO\n",
      "\n",
      "基于人物肖像图片和人声音频文件，生成人物肖像动态视频。使用时需依次调用下述模型。EMO 图像检测API详情 | EMO 视频生成API详情\n",
      "\n",
      "| 模型名称      | 说明                                               | 单价                                                               | 免费额度                        |\n",
      "|---------------|----------------------------------------------------|--------------------------------------------------------------------|---------------------------------|\n",
      "| emo-detect-v1 | 检测输入的图片是否符合要求，不需要部署，可直接调用 | 0.004元/张                                                         | 200张有效期：百炼开通后180天内  |\n",
      "| emo-v1        | 生成人物肖像动态视频，不需要部署，可直接调用       | 生成1:1画幅视频：0.08元/秒生成3:4画幅视频：0.16元/秒               | 1800秒有效期：百炼开通后180天内 |\n",
      "| emo-detect    | 检测输入的图片是否符合要求，仅支持部署后调用       | 当前仅支持部署后调用，仅收取部署费用。部署单价：20元/算力单元/小时 | 无                              |\n",
      "| emo           | 生成人物肖像动态视频，仅支持部署后调用             | 当前仅支持部署后调用，仅收取部署费用。部署单价：20元/算力单元/小时 | 无                              |\n",
      "\n",
      "| 输入物：人物肖像图片+人声音频文件   | 输出物：人物肖像动态视频                                    |\n",
      "|-------------------------------------|-------------------------------------------------------------|\n",
      "| 人物肖像：人声音频：参见右侧视频    | 人物视频：使用动作风格强度：活泼（\"style_level\": \"active\"） |\n",
      "\n",
      "### 灵动人像LivePortrait\n",
      "\n",
      "基于人物肖像图片和人声音频文件，快速、轻量地生成人物肖像动态视频。与悦动人像EMO模型相比，生成速度快、价格低，但是生成效果不如悦动人像EMO模型。使用时需依次调用下述两个模型。LivePortrait 图像检测API详情 | LivePortrait 视频生成API详情\n",
      "\n",
      "| 模型名称            | 说明                       | 单价       | 免费额度                        |\n",
      "|---------------------|----------------------------|------------|---------------------------------|\n",
      "| liveportrait-detect | 检测输入的图片是否符合要求 | 0.004元/张 | 200张有效期：百炼开通后180天内  |\n",
      "| liveportrait        | 生成人物肖像动态视频       | 0.02元/秒  | 1800秒有效期：百炼开通后180天内 |\n",
      "\n",
      "| 输入物：人物肖像图片+人声音频文件   | 输出物：人物肖像动态视频   |\n",
      "|-------------------------------------|----------------------------|\n",
      "| 人物肖像：人声音频：参见右侧视频    | 人物视频：                 |\n",
      "\n",
      "### 表情包Emoji\n",
      "\n",
      "基于人脸图片和预设的人脸动态模板，生成人脸动态视频。该能力可用于表情包制作、视频素材生成等场景。使用时需依次调用下述模型。Emoji 图像检测API详情 ｜ Emoji 视频生成API详情\n",
      "\n",
      "| 模型名称        | 说明                                               | 单价       | 免费额度                       |\n",
      "|-----------------|----------------------------------------------------|------------|--------------------------------|\n",
      "| emoji-detect-v1 | 检测输入图片是否符合要求                           | 0.004元/张 | 200张有效期：百炼开通后180天内 |\n",
      "| emoji-v1        | 基于人物肖像图片和指定的表情包模板生成人物同款表情 | 0.08元/秒  | 500秒有效期：百炼开通后180天内 |\n",
      "\n",
      "| 输入：人物肖像图片   | 输出：人物肖像动态视频                                       |\n",
      "|----------------------|--------------------------------------------------------------|\n",
      "|                      | “开心”表情的模板序列：（\"input.driven_id\": \"mengwa_kaixin\"） |\n",
      "\n",
      "### 声动人像VideoRetalk\n",
      "\n",
      "基于人物视频和人声音频，生成人物讲话口型与输入音频相匹配的视频。使用时需调用下述模型。VideoRetalk视频生成API详情\n",
      "\n",
      "| 模型名称    | 说明                                     | 单价      | 免费额度                        |\n",
      "|-------------|------------------------------------------|-----------|---------------------------------|\n",
      "| videoretalk | 生成人物讲话口型与输入音频相匹配的新视频 | 0.08元/秒 | 1800秒有效期：百炼开通后180天内 |\n",
      "\n",
      "### 视频风格重绘\n",
      "\n",
      "支持根据用户输入的文字内容，生成符合语义描述的不同风格的视频，或者根据用户输入的视频，进行视频风格重绘。API参考\n",
      "\n",
      "| 模型名称              | 说明         | 计费     | 免费额度   |\n",
      "|-----------------------|--------------|----------|------------|\n",
      "| video-style-transform | 视觉风格重绘 | 限时免费 | 限时免费   |\n",
      "\n",
      "模型效果示例\n",
      "\n",
      "| 原始视频（输入）   | 日式漫画（输出）   |\n",
      "|--------------------|--------------------|\n",
      "|                    |                    |\n",
      "\n",
      "## 文本向量\n",
      "\n",
      "文本向量模型用于将文本转换成一组可以代表文字的数字，适用于搜索、聚类、推荐、分类任务。模型根据输入Token数计费。同步接口API详情 | 批处理接口API详情\n",
      "\n",
      "## 公共云\n",
      "\n",
      "| 模型名称                | 向量维度                              | 最大行数   | 单行最大处理Token数   | 支持语种                                                                              | 单价（每千输入Token）        | 免费额度（注）                         |\n",
      "|-------------------------|---------------------------------------|------------|-----------------------|---------------------------------------------------------------------------------------|------------------------------|----------------------------------------|\n",
      "| text-embedding-v3       | 1,024（默认）、768、512、256、128或64 | 10         | 8,192                 | 中文、英语、西班牙语、法语、葡萄牙语、印尼语、日语、韩语、德语、俄罗斯语等50+主流语种 | 0.0005元Batch调用：0.00025元 | 各50万Token有效期：百炼开通后180天内   |\n",
      "| text-embedding-v2       | 1,536                                 | 25         | 2,048                 | 中文、英语、西班牙语、法语、葡萄牙语、印尼语、日语、韩语、德语、俄罗斯语              | 0.0007元Batch调用：0.00035元 | 各50万Token有效期：百炼开通后180天内   |\n",
      "| text-embedding-v1       | 1,536                                 | 25         | 2,048                 | 中文、英语、西班牙语、法语、葡萄牙语、印尼语                                          | 0.0007元Batch调用：0.00035元 | 各50万Token有效期：百炼开通后180天内   |\n",
      "| text-embedding-async-v2 | 1,536                                 | 100,000    | 2,048                 | 中文、英语、西班牙语、法语、葡萄牙语、印尼语、日语、韩语、德语、俄罗斯语              | 0.0007元                     | 各2000万Token有效期：百炼开通后180天内 |\n",
      "| text-embedding-async-v1 | 1,536                                 | 100,000    | 2,048                 | 中文、英语、西班牙语、法语、葡萄牙语、印尼语                                          | 0.0007元                     | 各2000万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "## 金融云\n",
      "\n",
      "| 模型名称          | 向量维度               |   最大行数 | 单行最大处理Token数   | 支持语种                                                                              | 单价（每千输入Token）   | 免费额度（注）                     |\n",
      "|-------------------|------------------------|------------|-----------------------|---------------------------------------------------------------------------------------|-------------------------|------------------------------------|\n",
      "| text-embedding-v3 | 1024（默认）、768或512 |         10 | 8,192                 | 中文、英语、西班牙语、法语、葡萄牙语、印尼语、日语、韩语、德语、俄罗斯语等50+主流语种 | 0.00133元               | 50万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "模型升级概述\n",
      "\n",
      "1. text-embedding-v2\n",
      "2. text-embedding-v2\n",
      "    - 语种扩充：新增对日语、韩语、德语、俄罗斯语的文本向量化能力。\n",
      "    - 效果提升：优化了预训练模型和SFT策略，提升了整体效果，公开数据评测结果显示有显著改进。\n",
      "3. text-embedding-v3\n",
      "4. text-embedding-v3\n",
      "    - 语种扩充：支持意大利语、波兰语、越南语、泰语等语种，语种数量增加至50余种。\n",
      "    - 输入长度扩展：最大输入长度从2048 Token扩展至8192 Token。\n",
      "    - 连续向量维度自定义：允许用户选择1024、768、512、256、128或64维度，默认维度为1024。\n",
      "    - 不再区分Query/Document类型：简化输入，text\\_type参数不再需要指定文本类型。\n",
      "    - Sparse向量支持：支持在接口中指定输出稠密向量和离散向量。\n",
      "    - 效果提升：进一步优化预训练模型和SFT策略，提升整体效果，公开数据评测结果显示效果更佳。\n",
      "\n",
      "v1、v2、v3模型的效果数据\n",
      "\n",
      "| 模型                          |   MTEB |   MTEB（Retrieval task） |   CMTEB |   CMTEB (Retrieval task) |\n",
      "|-------------------------------|--------|--------------------------|---------|--------------------------|\n",
      "| text-embedding-v1             |  58.3  |                    45.47 |   59.84 |                    56.59 |\n",
      "| text-embedding-v2             |  60.13 |                    49.49 |   62.17 |                    62.78 |\n",
      "| text-embedding-v3（64维度）   |  57.4  |                    46.52 |   59.19 |                    62.03 |\n",
      "| text-embedding-v3（128维度）  |  60.19 |                    52.51 |   63.81 |                    68.22 |\n",
      "| text-embedding-v3（256维度）  |  61.13 |                    54.41 |   65.92 |                    71.07 |\n",
      "| text-embedding-v3（512维度）  |  62.11 |                    54.3  |   66.81 |                    71.88 |\n",
      "| text-embedding-v3（768维度）  |  62.43 |                    54.74 |   67.9  |                    72.29 |\n",
      "| text-embedding-v3（1024维度） |  63.39 |                    55.41 |   68.92 |                    73.23 |\n",
      "\n",
      "MTEB（大规模文本嵌入评测基准）和CMTEB（中文大规模文本嵌入评测基准）采用0-100分制评估模型性能，数值越高代表效果越优。总分通过综合分类、聚类、检索等任务反映模型通用性，Retrieval Task分数用于衡量检索任务（如文档搜索）的精度，分数越高检索效果越强。\n",
      "\n",
      "## 多模态向量\n",
      "\n",
      "多模态向量模型将文本、图像或视频转换成一组由浮点数组成的向量，适用于视频分类、图像分类、图文检索等。API参考\n",
      "\n",
      "| 模型名称                | 数据类型   | 向量维度   | 单价     | 免费额度（注）   | 限流                       |\n",
      "|-------------------------|------------|------------|----------|------------------|----------------------------|\n",
      "| multimodal-embedding-v1 | float(32)  | 1,024      | 免费试用 | 无加权条目数限制 | 每分钟调用限制（RPM）：120 |\n",
      "\n",
      "## 文本分类、抽取、排序\n",
      "\n",
      "### OpenNLU\n",
      "\n",
      "针对给定的文本（中文或英文）进行信息抽取或文本分类。模型根据输出Token数计费。API参考\n",
      "\n",
      "| 模型名称   | 最大输入Token数   | 单价（每千Token）   | 免费额度（注）                      |\n",
      "|------------|-------------------|---------------------|-------------------------------------|\n",
      "| opennlu-v1 | 1,024             | 0.00465元           | 100万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "### 文本排序模型\n",
      "\n",
      "通常用于语义检索，即给定查询 (Query) 和一系列候选文本 (Documents)，会根据与查询的语义相关性从高到低对候选文本进行排序。API参考\n",
      "\n",
      "| 模型名称      |   最大Document数量 | 单行最大输入Token   | 最大输入Token   | 支持语言                                                        | 单价     | 免费额度                            |\n",
      "|---------------|--------------------|---------------------|-----------------|-----------------------------------------------------------------|----------|-------------------------------------|\n",
      "| gte-rerank-v2 |                500 | 4,000               | 30,000          | 中、英、日、韩、泰语、西、法、葡、德、印尼语、阿拉伯语等50+语种 | 限时免费 | 100万Token有效期：百炼开通后180天内 |\n",
      "| gte-rerank    |                500 | 4,000               | 30,000          | 中、英、日、韩、泰语、西、法、葡、德、印尼语、阿拉伯语等50+语种 | 限时免费 | 100万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "- 单行最大输入Token：每个Query或Document的最大Token数量为4,000。如果输入内容超过此长度，将会被截断。\n",
      "- 最大候选文本数量：每次请求中Document的最大数量为500。\n",
      "- 最大输入Token：每次请求中所有Query和Document的Token总数不得超过30,000。\n",
      "\n",
      "## 行业\n",
      "\n",
      "### 通义法睿\n",
      "\n",
      "适用于回答法律问题、推荐裁判类案、辅助案情分析、生成法律文书、检索法律知识、审查合同条款等。API参考 | 在线体验\n",
      "\n",
      "| 模型名称   | 上下文长度   | 最大输入    | 最大输出   | 输入成本      | 输出成本      |\n",
      "|------------|--------------|-------------|------------|---------------|---------------|\n",
      "| 模型名称   | （Token数）  | （Token数） |            | （每千Token） | （每千Token） |\n",
      "| farui-plus | 12k          | 12k         | 2k         | 0.02元        | 0.02元        |\n",
      "\n",
      "### 意图理解\n",
      "\n",
      "通义意图理解模型，能够在百毫秒级时间内快速、准确地解析用户意图，并选择合适工具来解决用户问题。API参考｜使用方法\n",
      "\n",
      "| 模型名称                | 上下文长度   | 最大输入    | 最大输出    | 输入成本      | 输出成本      | 免费额度（注）                      |\n",
      "|-------------------------|--------------|-------------|-------------|---------------|---------------|-------------------------------------|\n",
      "| 模型名称                | （Token数）  | （Token数） | （Token数） | （每千Token） | （每千Token） | 免费额度（注）                      |\n",
      "| tongyi-intent-detect-v3 | 8,192        | 8,192       | 1,024       | 0.0004元      | 0.001元       | 100万Token有效期：百炼开通后180天内 |\n",
      "\n",
      "该文章对您有帮助吗？\n",
      "\n",
      "### 为什么选择阿里云\n",
      "\n",
      "### 产品和定价\n",
      "\n",
      "### 解决方案\n",
      "\n",
      "### 文档与社区\n",
      "\n",
      "### 权益中心\n",
      "\n",
      "### 支持与服务\n",
      "\n",
      "### 关注阿里云\n",
      "\n",
      "关注阿里云公众号或下载阿里云APP，关注云资讯，随时随地运维管控云服务\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "### 友情链接\n",
      "\n",
      "© 2009-2025 Aliyun.com 版权所有 增值电信业务经营许可证： 浙B2-20080101 域名注册服务机构许可： 浙D3-20210002\n",
      "\n",
      "浙公网安备 33010602009975号浙B2-20080101-4\n"
     ]
    }
   ],
   "source": [
    "print(result.document.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274ae0f9-7580-4b97-b6c2-d3457dba1d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
