{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a99e6763-b7fe-4786-8fa1-3e4cb3c7b844",
   "metadata": {},
   "source": [
    "## QA-Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645eeb3d-d7b8-4118-8055-c535328eb00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44986707-8a43-4c26-b3a5-00d8ab7fc8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textlong.knowledge import LocalFilesLoader, QAExcelsLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7963c56-ef3d-4103-acd7-ad670476f44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "支持为 <public> 从 [/Users/xuehongwei/github/textlong/notes/data/public/__QA__] 中加载类型为 xlsx,xls 的文件。\n",
      "['/Users/xuehongwei/github/textlong/notes/data/public/__QA__/消防50条问题(240520).xlsx']\n"
     ]
    }
   ],
   "source": [
    "qa = QAExcelsLoader()\n",
    "print(qa.help())\n",
    "print(qa.get_files())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2226635a-a2a7-44f5-8e4b-450b264fad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "docs = qa.load()\n",
    "\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b08705e7-0179-415f-ad79-c4b16366b591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='商场\\n场景：商场店铺内钢楼梯下方（非扶梯）\\n问题：需要设置喷头吗？', metadata={'answer': '问题答案：需要设置喷头。\\n\\n相关规范解释：根据《建筑设计防火规范》（GB 50016-2014）和《自动喷水灭火系统设计规范》（GB 50084-2001），商场店铺内的钢楼梯下方应设置自动喷水灭火系统的喷头，以确保在火灾发生时能够迅速有效地控制火势。钢楼梯下方可能存在电线、管道等易燃物品，且火灾蔓延速度快，因此需要喷头覆盖以增强防火能力。', 'source': '/Users/xuehongwei/github/textlong/notes/data/public/__QA__/消防50条问题(240520).xlsx', 'sheet': 'QA_50个问题'}),\n",
       " Document(page_content='商场\\n场景：商场自动扶梯下方\\n问题：需要设置自动喷淋灭火系统吗？', metadata={'answer': '问题答案：通常不需要。\\n\\n相关规范解释：根据《建筑设计防火规范》（GB 50016-2014）和《自动喷水灭火系统设计规范》（GB 50084-2001），自动扶梯下方并不被视为通常需要设置自动喷淋系统的区域。自动扶梯下方的空间一般较小，且不是主要的火灾荷载区域。然而，如果自动扶梯附近有其他火灾风险较高的区域或设施，如电气室、机械设备等，则应按照规范的相关要求考虑设置自动喷淋系统。', 'source': '/Users/xuehongwei/github/textlong/notes/data/public/__QA__/消防50条问题(240520).xlsx', 'sheet': 'QA_50个问题'}),\n",
       " Document(page_content='高层建筑 \\n场景：高层住宅的裙房内的网吧\\n问题：需要设置自动喷淋系统吗？', metadata={'answer': '问题答案：需要设喷淋。\\n\\n相关规范解释：《建筑设计防火规范》（GB 50016-2014）第5.4.9条规定，高层建筑内的歌舞娱乐放映游艺场所（如网吧）应设置自动喷水灭火系统。此外，根据第3.1.1条，网吧属于人员密集场所，且具有较高的火灾风险，因此需要采取更为严格的防火措施，包括设置自动喷水灭火系统。', 'source': '/Users/xuehongwei/github/textlong/notes/data/public/__QA__/消防50条问题(240520).xlsx', 'sheet': 'QA_50个问题'}),\n",
       " Document(page_content='低层建筑 \\n场景：洗浴中心的洗浴区（就是淋浴，池子，桑拿房）\\n问题：需要设置自动喷淋灭火系统和报警系统吗？', metadata={'answer': '问题（1）答案：需要设计喷淋系统和报警系统。\\n\\n相关规范解释：根据《建筑设计防火规范》（GB 50016-2014）第8.3.4条，洗浴中心属于人员密集场所，应设置自动喷水灭火系统和火灾自动报警系统。这是因为洗浴中心的人员密度较高，火灾发生时容易造成人员伤亡和财产损失，因此需要设置这些系统来提高火灾防控能力。\\n\\n问题（2）答案：在《建筑设计防火规范》（GB 50016-2014）第8.3.4条中规定。\\n\\n相关规范解释：该规范条文明确指出，人员密集场所应设置自动喷水灭火系统和火灾自动报警系统，以提高火灾防控能力。洗浴中心作为人员密集场所，应遵守这一规定。', 'source': '/Users/xuehongwei/github/textlong/notes/data/public/__QA__/消防50条问题(240520).xlsx', 'sheet': 'QA_50个问题'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textlong.qa import format_qa_docs\n",
    "retriever.invoke(\"商场扶梯下面要设置喷水吗？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc52c386-aa58-47c1-a396-0408184cbb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_zhipu import ChatZhipuAI\n",
    "from textlong.memory import MemoryManager, WithMemoryBinding\n",
    "from textlong.qa import format_qa_docs, create_qa_prompt\n",
    "from textlong.hub import load_chat_prompt\n",
    "\n",
    "# prompt = create_qa_prompt()\n",
    "prompt = load_chat_prompt(template_id=\"qa\", project_id=\"xiaofang\")\n",
    "llm = ChatZhipuAI()\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "chain = {\n",
    "    \"context\":  (lambda x: x['input']) | retriever | format_qa_docs,\n",
    "    \"question\": lambda x: x['input'],\n",
    "    \"history\":  lambda x: x['history'],\n",
    "} | prompt | llm\n",
    "\n",
    "memory = MemoryManager()\n",
    "withMemoryChain = WithMemoryBinding(chain, memory)\n",
    "config = {\"configurable\": {\"session_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82f8f8b6-1cf5-48b6-a1f8-ed16b6c7e890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "商场|扶|梯|下面|通常|不需要|设置|喷|水|系统|。|因为|扶|梯|下方|空间|较小|，|不是|主要的|火灾|荷载|区域|，|且|扶|梯|的|金属材料|不易|燃烧|。|但|若|扶|梯|附近|有|其他|火灾|风险|较高的|区域|或|设施|，|应|按|规范|要求|考虑|是否|设置|喷|水|系统|。||"
     ]
    }
   ],
   "source": [
    "# chain = create_qa_chain(llm, retriever)\n",
    "for x in withMemoryChain.stream({\"input\": \"商场扶梯下面要设置喷水吗？\"}, config):\n",
    "    print(x.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6c0617b-6af2-413f-b9b9-23a05c3c7f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果|商场|扶|梯|下面|有|储|物|间|，|那么|根据|储|物|间|内|储存|物品|的|火灾|风险|以及|空间|大小|，|可能|需要|设置|喷|水|灭火|系统|。|一般|而言|，|如果|储|物|间|内|储存|的是|易|燃|物品|，|或者|储|物|间|面积|较大|，|按照|相关|防火|规范|，|应该|设置|自动|喷|水|灭火|系统|。|这|旨在|提高|火灾|时的|灭火|效率和|安全性|。|具体|是否|需要|设置|，|应|参照|当地的|建筑设计|防火|规范|和|自动|喷|水|灭火|系统|设计|规范|。||"
     ]
    }
   ],
   "source": [
    "# chain = create_qa_chain(llm, retriever)\n",
    "for x in withMemoryChain.stream({\"input\": \"下面有储物间呢？\"}, config):\n",
    "    print(x.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f46c8f06-167a-408d-9a8d-55befcda314e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'history', 'question'], input_types={'history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'task_instruction': '\\n你是一名咨询专家，只负责根据资料回答相关提问，禁止回答与此无关的问题。\\n\\n1. 如果你获得的参考例子无法回答问题，可以查询互联网，但务必注意资料的真实性，不要做任何编造\\n2. 请使用简洁的语言回答，不要啰嗦\\n3. 不要生成\"根据提供的资料...\"等字眼\\n', 'output_format': '\\n输出样例：\\n```\\n问题答案：xxx。\\n\\n相关规范解释：xxxxxxxx\\n```\\n'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['task_instruction'], template='{{task_instruction}}', template_format='mustache')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='我有哪些资料可以参考？', template_format='mustache')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'output_format'], template='你可以参考这些资料：\\n{{context}}\\n你必须按照如下格式输出：{{output_format}}', template_format='mustache')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='好的，我会优先从上面资料寻找答案，如果找不到合适的就会从互联网查询。', template_format='mustache')), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{{question}}', template_format='mustache'))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_qa_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87650c3b-58f2-42c5-beea-f7c47189dd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'history', 'question'], input_types={'history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'output_format': '\\n输出样例：\\n```\\n问题答案：xxx。\\n\\n相关规范解释：xxxxxxxx\\n```\\n', 'task_instruction': '\\n你是一名咨询专家，只负责根据资料回答相关提问，禁止回答与此无关的问题。\\n\\n1. 如果你获得的参考例子无法回答问题，可以查询互联网，但务必注意资料的真实性，不要做任何编造\\n2. 请使用简洁的语言回答，不要啰嗦\\n3. 不要生成\"根据提供的资料...\"等字眼\\n'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['task_instruction'], template='{{task_instruction}}', template_format='mustache')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='我有哪些资料可以参考？', template_format='mustache')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'output_format'], template='你可以参考这些资料：\\n{{context}}\\n你必须按照如下格式输出：{{output_format}}', template_format='mustache')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='好的，我会优先从上面资料寻找答案，如果找不到合适的就会从互联网查询。', template_format='mustache')), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{{question}}', template_format='mustache'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_chat_prompt(\"qa\", \"消防\", in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa06a33-825c-406d-ad44-2dee9f86ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textlong.prompts import save_chat_prompt, load_chat_prompt\n",
    "save_chat_prompt(prompt, \"qa\", \"xiaofang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a248501d-8e15-4539-ab09-180524884c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"有什么例子\"\n",
    "results = db.similarity_search(query)\n",
    "print(results)\n",
    "# for doc, score in results:\n",
    "#     print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ae649-4fdc-457c-a6e2-11b8ef5ebeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4147154-bd78-452c-8e1b-448b1e145ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textlong.prompts import save_chat_prompt, load_chat_prompt, create_writing_todo_prompt\n",
    "save_chat_prompt(create_writing_todo_prompt(), \"todo\", \"xiaofang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576d7ae-cdba-454e-86b2-f81aef0e70df",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_chat_prompt(\"todo\", \"xiaofang\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc148a0-8d97-4a10-b258-0ef2533043c0",
   "metadata": {},
   "source": [
    "## sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "465999ab-3db8-463d-9929-1c0aa1d7fb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m148.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\n",
      "Collecting torch>=1.11.0 (from sentence_transformers)\n",
      "  Downloading torch-2.2.2-cp310-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from sentence_transformers) (1.26.4)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Downloading scikit_learn-1.4.2-cp310-cp310-macosx_10_9_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Downloading scipy-1.13.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m578.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.15.1 in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from sentence_transformers) (0.22.2)\n",
      "Requirement already satisfied: Pillow in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from sentence_transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Collecting networkx (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
      "Collecting huggingface-hub>=0.15.1 (from sentence_transformers)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.34.0->sentence_transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m615.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.2-cp310-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.41.0-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading scikit_learn-1.4.2-cp310-cp310-macosx_10_9_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.0-cp310-cp310-macosx_10_9_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-macosx_10_12_x86_64.whl (415 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.8/415.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-macosx_10_12_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, scipy, safetensors, networkx, joblib, torch, scikit-learn, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.22.2\n",
      "    Uninstalling huggingface-hub-0.22.2:\n",
      "      Successfully uninstalled huggingface-hub-0.22.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "Successfully installed huggingface-hub-0.23.0 joblib-1.4.2 networkx-3.3 safetensors-0.4.3 scikit-learn-1.4.2 scipy-1.13.0 sentence_transformers-2.7.0 threadpoolctl-3.5.0 tokenizers-0.19.1 torch-2.2.2 transformers-4.41.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d3093-b37b-40c3-a198-dbb99ec5cde3",
   "metadata": {},
   "source": [
    "## 完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fb6f4fc-02e1-4357-a30a-5461bdae5670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# 加载\n",
    "from textlong import format_qa_docs, QAExcelsLoader\n",
    "docs = QAExcelsLoader().load()\n",
    "\n",
    "# 入库\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# LLM\n",
    "from langchain_zhipu import ChatZhipuAI\n",
    "llm = ChatZhipuAI()\n",
    "\n",
    "# 提示语\n",
    "from textlong.prompts import create_qa_prompt, load_chat_prompt\n",
    "prompt = load_chat_prompt(\"qa\", \"xiaofang\")\n",
    "\n",
    "# chain\n",
    "chain = {\n",
    "    \"context\":  (lambda x: x['input']) | retriever | format_qa_docs,\n",
    "    \"question\": lambda x: x['input'],\n",
    "    \"history\":  lambda x: x['history'],\n",
    "} | prompt | llm\n",
    "\n",
    "# 记忆\n",
    "from textlong.memory import MemoryManager, WithMemoryBinding\n",
    "memory = MemoryManager()\n",
    "withMemoryChain = WithMemoryBinding(chain, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "401de534-7283-4d83-83f2-ed8d689556b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "需要|根据|具体情况|。\n",
      "\n",
      "规范|解释|：|宿舍|过|道|是否|需要|设置|喷|水|系统|取决于|多种|因素|，|如|宿舍|的建筑|高度|、|人员|密集|程度|、|火灾|风险|等|。|根据|《|建筑设计|防火|规范|》，|如果|宿舍|过|道|属于|人员|密集|区域|或|存在|较高的|火灾|风险|，|应|考虑|设置|喷|水|系统|。|对于|多层|或|高层|宿舍|，|为了|提高|防火|安全性|，|过|道|通常会|设置|喷|淋|系统|。|但对于|低|层|宿舍|，|如果|过|道|宽度|适中|、|通风|良好|且|火灾|风险|较低|，|可能|不需要|设置|喷|水|系统|。||"
     ]
    }
   ],
   "source": [
    "# 提问\n",
    "config = {\"configurable\": {\"session_id\": \"1\"}}\n",
    "for x in withMemoryChain.stream({\"input\": \"宿舍的过道需要吗？\"}, config):\n",
    "    print(x.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74016e-fdc0-4ebf-8ecc-d478efdbec75",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60d45a67-4619-4e9a-84a5-42d548c358ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_zhipu import ChatZhipuAI\n",
    "from textlong.qa import AskDocumentTool, create_qa_chain\n",
    "\n",
    "docs = [Document(page_content=\"textlong是一个长文生成的python模块。\")]\n",
    "\n",
    "llm = ChatZhipuAI()\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "chain = create_qa_chain(llm, db.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "934f37c1-58ca-4f30-9ea9-e72afc96e199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='textlong是一个长文生成的Python模块。通过这个模块，用户可以生成各种样式的长篇文章，适用于需要大量文本内容的不同场景，比如数据填充、测试文档等。', response_metadata={'id': '8689076627888601317', 'created': 1716713414, 'token_usage': {'completion_tokens': 40, 'prompt_tokens': 45, 'total_tokens': 85}, 'model_name': 'glm-4', 'finish_reason': 'stop'}, id='run-e58ad3d1-164c-40dc-96cc-ef1b96f957a1-0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"query\": \"textlong是什么?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c6989e8-4c50-4b04-8e9e-5c556a5a814f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='textlong是一个长文生成的Python模块。通过这个模块，用户可以生成各种类型的文本内容，适用于需要大量文本数据的场景，比如文本分析、自然语言处理等领域。具体的功能和用途可能需要参考该模块的官方文档或源代码以获得更详尽的信息。', response_metadata={'id': '8689071920604292138', 'created': 1716713409, 'token_usage': {'completion_tokens': 61, 'prompt_tokens': 45, 'total_tokens': 106}, 'model_name': 'glm-4', 'finish_reason': 'stop'}, id='run-ba845729-532e-4061-a09d-b4a1dde3b129-0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool = AskDocumentTool(chain=chain)\n",
    "tool.invoke({\"query\": \"textlong是什么?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c88520-f8fd-45a0-809c-44f594dc5732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchani-book-py3.10-ipykernel",
   "language": "python",
   "name": "langchani-book-py3.10-ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
