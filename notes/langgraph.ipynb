{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "import os\n",
    "os.chdir('..')\n",
    "from langchain_zhipu import ChatZhipuAI, ZhipuAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain_chinese import (\n",
    "    AskDocumentTool,\n",
    "    create_qa_chain,\n",
    "    create_reason_agent,\n",
    ")\n",
    "from langchain_chinese.langgraph import create_tools_calling_executor\n",
    "\n",
    "query_document = Document(page_content=\"langchain_chinese 是为中国大语言模型优化的langchain模块\")\n",
    "\n",
    "db = Chroma.from_documents([query_document], ZhipuAIEmbeddings())\n",
    "\n",
    "llm_zhipu = ChatZhipuAI(model=\"glm-4\")\n",
    "\n",
    "chain = create_qa_chain(llm_zhipu, db.as_retriever())\n",
    "qa_tool = AskDocumentTool(qa_chain=(chain | StrOutputParser()), name=\"qa_chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              +---------------------------------+          \n",
      "              | Parallel<context,question>Input |          \n",
      "              +---------------------------------+          \n",
      "                    ****               ****                \n",
      "                 ***                       ***             \n",
      "               **                             ***          \n",
      "    +-------------+                              **        \n",
      "    | Lambda(...) |                               *        \n",
      "    +-------------+                               *        \n",
      "            *                                     *        \n",
      "            *                                     *        \n",
      "            *                                     *        \n",
      "+----------------------+                          *        \n",
      "| VectorStoreRetriever |                          *        \n",
      "+----------------------+                          *        \n",
      "            *                                     *        \n",
      "            *                                     *        \n",
      "            *                                     *        \n",
      "+---------------------+                   +-------------+  \n",
      "| Lambda(format_docs) |                   | Lambda(...) |  \n",
      "+---------------------+                   +-------------+  \n",
      "                    ****               ****                \n",
      "                        ***         ***                    \n",
      "                           **     **                       \n",
      "             +----------------------------------+          \n",
      "             | Parallel<context,question>Output |          \n",
      "             +----------------------------------+          \n",
      "                               *                           \n",
      "                               *                           \n",
      "                               *                           \n",
      "                    +--------------------+                 \n",
      "                    | ChatPromptTemplate |                 \n",
      "                    +--------------------+                 \n",
      "                               *                           \n",
      "                               *                           \n",
      "                               *                           \n",
      "                        +-------------+                    \n",
      "                        | ChatZhipuAI |                    \n",
      "                        +-------------+                    \n",
      "                               *                           \n",
      "                               *                           \n",
      "                               *                           \n",
      "                    +-------------------+                  \n",
      "                    | ChatZhipuAIOutput |                  \n",
      "                    +-------------------+                  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='langchain_chinese 是专门针对中国大语言模型进行优化的langchain模块。这意味着它对langchain的功能进行了调整和改进，以更好地适应和满足中国大语言模型的特定需求。')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke([\"langchain_chinese 是啥\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "import random\n",
    "\n",
    "# @tool(args_schema=WhereIsCatSchema)\n",
    "@tool(\"WhereIsCatHidding\")\n",
    "def where_is_cat_hiding(idea: str) -> str:\n",
    "    \"\"\"从这些地方选择猫躲藏的地方\"\"\"\n",
    "    return random.choice([\"在床底下\", \"在书架中\", \"在阳台\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# llm_zhipu = ChatZhipuAI()\n",
    "tools = [qa_tool, where_is_cat_hiding]\n",
    "app = create_tools_calling_executor(llm_zhipu, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__runnable-name:  qa_chain\n",
      "log: call tool [qa_chain]\n",
      "{}\n",
      "{'chunk': [ToolMessage(content='根据您提供的资料，\"langchain_chinese\" 是专门针对中国大语言模型进行优化的 langchain 模块。这个模块的设计目的很可能是为了更好地适应中文语言环境下的需求，提供更加精准和高效的语言处理能力。至于您的问题，似乎不完整，如果“l”是您想询问的内容的话，请您提供更详细的问题，我会依据所给资料为您解答。', tool_call_id='call_8483741594421818220')]}\n",
      "{'input': [HumanMessage(content='请查询资料，告诉我langchain_chinese是什么？'), AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_8483741594421818220', 'function': {'arguments': '{\"query\":\"langchain_chinese是什么？\"}', 'name': 'qa_chain'}, 'type': 'function'}]})], 'output': [ToolMessage(content='根据您提供的资料，\"langchain_chinese\" 是专门针对中国大语言模型进行优化的 langchain 模块。这个模块的设计目的很可能是为了更好地适应中文语言环境下的需求，提供更加精准和高效的语言处理能力。至于您的问题，似乎不完整，如果“l”是您想询问的内容的话，请您提供更详细的问题，我会依据所给资料为您解答。', tool_call_id='call_8483741594421818220')]}\n",
      "经过_查询_，_\"_lang_chain__ch_inese_\"_ 是_专门_针对_中国_大_语言_模型_进行_优_化的_ lang_chain_ 模_块_。_这个_模块_的设计_目的_很_可能是_为了_更好地_适应_中文_语言_环境_下的_需求_，_提供_更加_精准_和_高效_的语言_处理_能力_。_如果您_有_关于_ lang_chain__ch_inese_ 的_其他_问题_，_请_随时_告诉我_，_我会_尽力_为您_解答_。__"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "app = create_tools_calling_executor(\n",
    "    llm_zhipu,\n",
    "    tools = tools,\n",
    "    verbose = True)\n",
    "\n",
    "inputs = [HumanMessage(content=\"请查询资料，告诉我langchain_chinese是什么？\")]\n",
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name'] in [\"agent\", \"qa_chain\"]):\n",
    "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
    "    elif(chunk['name']==\"action\"):\n",
    "        print(chunk['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    +-----------+             \n",
      "                    | __start__ |             \n",
      "                    +-----------+             \n",
      "                          *                   \n",
      "                          *                   \n",
      "                          *                   \n",
      "                      +-------+               \n",
      "                      | agent |               \n",
      "                     *+-------+*              \n",
      "                   **           ***           \n",
      "                 **                **         \n",
      "               **                    **       \n",
      "+-----------------------+              **     \n",
      "| agent_should_continue |               *     \n",
      "+-----------------------+               *     \n",
      "            *           *****           *     \n",
      "            *                ****       *     \n",
      "            *                    ***    *     \n",
      "       +---------+                +--------+  \n",
      "       | __end__ |                | action |  \n",
      "       +---------+                +--------+  \n"
     ]
    }
   ],
   "source": [
    "app.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__runnable-name:  qa_chain\n",
      "log: call runnable [qa_chain]\n",
      "lang_chain__ch_inese_ 是_专门_为中国_的大_语言_模型_优_化的_ lang_chain_ 模_块_。_这意味着_它_对_ lang_chain_ 进行_了_调整_和_改进_，_以便_更好地_适应_和_服务于_中国_的大_语言_模型_应用_场景_。__"
     ]
    }
   ],
   "source": [
    "app = create_tools_calling_executor(\n",
    "    llm_zhipu,\n",
    "    tools = tools,\n",
    "    runnables = {\"qa_chain\": chain},\n",
    "    verbose = True)\n",
    "\n",
    "inputs = [\"请查询资料，告诉我langchain_chinese是什么？\"]\n",
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name'] in [\"agent\", \"qa_chain\"]):\n",
    "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
    "    elif(chunk['name']==\"action\"):\n",
    "        print(chunk['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        +-----------+                     \n",
      "                        | __start__ |                     \n",
      "                        +-----------+                     \n",
      "                              *                           \n",
      "                              *                           \n",
      "                              *                           \n",
      "                          +-------+                       \n",
      "                          | agent |*                      \n",
      "                         *+-------+ ****                  \n",
      "                       **               ***               \n",
      "                     **                    ***            \n",
      "                   **                         ****        \n",
      "    +-----------------------+                     **      \n",
      "    | agent_should_continue |                      *      \n",
      "    +-----------------------+**                    *      \n",
      "           **        **        *******             *      \n",
      "         **            **             *****        *      \n",
      "        *                **                ****    *      \n",
      "+----------+               *                  +--------+  \n",
      "| qa_chain |             **                   | action |  \n",
      "+----------+           **                     +--------+  \n",
      "           **        **                                   \n",
      "             **    **                                     \n",
      "               *  *                                       \n",
      "           +---------+                                    \n",
      "           | __end__ |                                    \n",
      "           +---------+                                    \n"
     ]
    }
   ],
   "source": [
    "app.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__runnable-name:  WhereIsCatHidding\n",
      "log: call tool [WhereIsCatHidding]\n",
      "{}\n",
      "{'chunk': [ToolMessage(content='在阳台', tool_call_id='call_8483746542224243147')]}\n",
      "{'input': [HumanMessage(content='猫在哪里？'), AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_8483746542224243147', 'function': {'arguments': '{\"idea\":\"猫在哪里\"}', 'name': 'WhereIsCatHidding'}, 'type': 'function'}]})], 'output': [ToolMessage(content='在阳台', tool_call_id='call_8483746542224243147')]}\n",
      "猫_在_阳_台上_躲_藏着_呢_！__"
     ]
    }
   ],
   "source": [
    "inputs = [HumanMessage(content=\"猫在哪里？\")]\n",
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name'] in [\"agent\", \"qa_chain\"]):\n",
    "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
    "    elif(chunk['name']==\"action\"):\n",
    "        print(chunk['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__runnable-name:  qa_chain\n",
      "log: call runnable [qa_chain]\n"
     ]
    }
   ],
   "source": [
    "inputs =  [HumanMessage(content=\"霍金的生日是哪一天？\")]\n",
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name']==\"agent\"):\n",
    "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
    "    elif(chunk['name']==\"action\"):\n",
    "        print(chunk['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为什么_程序员_总是_携带_电脑_？_因为他们_不想_被人_称为_“_裸_奔_者_”。__"
     ]
    }
   ],
   "source": [
    "inputs =  [HumanMessage(content=\"写一个关于程序员的笑话\")]\n",
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name']==\"agent\"):\n",
    "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
    "    elif(chunk['name']==\"action\"):\n",
    "        print(chunk['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-chinese-py3.9-ipkyernel",
   "language": "python",
   "name": "langchain-chinese-py3.9-ipkyernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
