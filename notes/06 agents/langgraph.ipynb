{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from dotenv import load_dotenv, find_dotenv\n",
                "load_dotenv(find_dotenv(), override=True)\n",
                "\n",
                "import os\n",
                "os.chdir('..')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_openai import ChatOpenAI\n",
                "from langchain_zhipu import ChatZhipuAI\n",
                "from illufly.md import create_agent"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "app = create_agent(llm=ChatZhipuAI())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        " -------------------- 已有outline: 0\n",
                        "\n",
                        " -------------------- 已有detail: 0\n",
                        "### 反思\n",
                        "当前没有写作提纲，也没有已完成的扩写内容。因此，我应当首先创作写作提纲。\n",
                        "\n",
                        "### 思考\n",
                        "下一步我需要选择`create_outline`工具来创作写作提纲。该提纲需要包含足够的细节，以便之后扩写500字的都市修仙故事。\n",
                        "\n",
                        "### 推理\n",
                        "我需要构思一个包含以下元素的提纲：\n",
                        "- 主人公介绍\n",
                        "- 都市背景描述\n",
                        "- 修仙元素的引入\n",
                        "- 冲突或挑战\n",
                        "- 解决方案或结局\n",
                        "\n",
                        "### 计划\n",
                        "以下是我的执行计划：\n",
                        "\n",
                        "```json\n",
                        "{\"properties\": {\"name\": \"create_outline\", \"args\": {\"task\": \"创作一个包含主人公介绍、都市背景、修仙元素、冲突和解决方案的提纲\"}}}\n",
                        "```\n",
                        "\n",
                        "这个计划聚焦于创作一个结构化的提纲，确保之后的故事扩写有明确的指导。只计划了这一步，因为一个好的提纲是成功扩写故事的基础。\n",
                        "\n",
                        "### 输出\n",
                        "以下是我选择的执行动作：\n",
                        "\n",
                        "```json\n",
                        "{\"name\": \"create_outline\", \"args\": {\"task\": \"创作一个包含主人公介绍、都市背景、修仙元素、冲突和解决方案的提纲\"}}\n",
                        "```"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m帮我创作一篇500字的都市修仙故事\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1333\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1333\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1335\u001b[0m     config,\n\u001b[1;32m   1336\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1337\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1338\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m   1339\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1340\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1341\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1342\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1343\u001b[0m ):\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1345\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
                        "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-book-u1j6yauo-py3.10/lib/python3.10/site-packages/langgraph/pregel/__init__.py:869\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    862\u001b[0m futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    863\u001b[0m     executor\u001b[38;5;241m.\u001b[39msubmit(run_with_retry, task, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy)\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m next_tasks\n\u001b[1;32m    865\u001b[0m ]\n\u001b[1;32m    867\u001b[0m \u001b[38;5;66;03m# execute tasks, and wait for one to fail or all to finish.\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;66;03m# each task is independent from all other concurrent tasks\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m \u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_when\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIRST_EXCEPTION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m    876\u001b[0m _panic_or_proceed(done, inflight, step)\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/concurrent/futures/_base.py:306\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[1;32m    304\u001b[0m     waiter \u001b[38;5;241m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[0;32m--> 306\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_condition:\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/threading.py:600\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    598\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 600\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "app.invoke({\"task\": \"帮我创作一篇500字的都市修仙故事\"})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for x in app.stream({\"task\": \"帮我创作一篇2000字的都市修仙故事\"}):\n",
                "    print(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async for x in app.astream_events({\"task\": \"帮我创作一篇2000字的都市修仙故事\"}, version=\"v1\"):\n",
                "    kind = x[\"event\"]\n",
                "    if kind == \"on_chat_model_stream\":\n",
                "        content = x[\"data\"][\"chunk\"].content\n",
                "        if content:\n",
                "            print(content, end=\"\", flush=True)\n",
                "    else:\n",
                "        # print(kind)\n",
                "        pass"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 可视化"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture --no-stderr\n",
                "%pip install pygraphviz"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "app.get_graph().print_ascii()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_core.runnables.graph import MermaidDrawMethod\n",
                "from IPython.display import display, Image\n",
                "\n",
                "display(\n",
                "    Image(app.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API))\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## RAG"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_zhipu import ChatZhipuAI\n",
                "from langchain_openai import OpenAIEmbeddings\n",
                "from langchain_community.vectorstores import FAISS\n",
                "from langchain_core.messages import HumanMessage\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain.schema import Document\n",
                "from langchain.agents import tool\n",
                "\n",
                "from illufly.agents import create_tools_calling_executor\n",
                "from illufly.qa import AskDocumentTool, create_qa_chain\n",
                "\n",
                "query_document = Document(page_content=\"illufly 是一个基于AI的开源框架，用于生成超长文档\")\n",
                "\n",
                "db = FAISS.from_documents([query_document], OpenAIEmbeddings())\n",
                "\n",
                "llm_zhipu = ChatZhipuAI()\n",
                "\n",
                "chain = create_qa_chain(llm_zhipu, db.as_retriever())\n",
                "qa_tool = AskDocumentTool(chain=(chain | StrOutputParser()), name=\"qa_chain\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chain.invoke({\"query\": \"illufly是什么\"})"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chain.get_graph().print_ascii()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.tools import tool\n",
                "import random\n",
                "\n",
                "# @tool(args_schema=WhereIsCatSchema)\n",
                "@tool(\"WhereIsCatHidding\")\n",
                "def where_is_cat_hiding(idea: str) -> str:\n",
                "    \"\"\"从这些地方选择猫躲藏的地方\"\"\"\n",
                "    return random.choice([\"在床底下\", \"在书架中\", \"在阳台\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from langchain_openai import ChatOpenAI\n",
                "# llm_zhipu = ChatZhipuAI()\n",
                "tools = [qa_tool, where_is_cat_hiding]\n",
                "app = create_tools_calling_executor(llm_zhipu, tools)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for x in app.stream(\"讲个笑话给我?\"):\n",
                "    print(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_core.messages import HumanMessage\n",
                "app = create_tools_calling_executor(\n",
                "    llm_zhipu,\n",
                "    tools = tools,\n",
                "    verbose = True)\n",
                "\n",
                "input = \"请查询资料，告诉我illufly是什么？\"\n",
                "async for chunk in app.astream_events(input, version=\"v1\"):\n",
                "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
                "    if(chunk['event']==\"on_chain_stream\" and chunk['name'] in [\"agent\", \"qa_chain\"]):\n",
                "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
                "    elif(chunk['name']==\"action\"):\n",
                "        print(chunk['data'])\n",
                "    elif(chunk['event']==\"on_chat_model_start\"):\n",
                "\t    print(\"\\n\", chunk['event'], chunk['name'], chunk['tags'])        "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "app.get_graph().print_ascii()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "app = create_tools_calling_executor(\n",
                "    llm_zhipu,\n",
                "    tools = tools,\n",
                "    runnables = {\"qa_chain\": chain},\n",
                "    verbose = True\n",
                ")\n",
                "\n",
                "input = [\"请查询资料，告诉我illufly是什么？\"]\n",
                "async for chunk in app.astream_events(input, version=\"v1\"):\n",
                "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
                "    if(chunk['event']==\"on_chain_stream\" and chunk['name'] in [\"agent\", \"qa_chain\"]):\n",
                "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
                "    elif(chunk['name']==\"action\"):\n",
                "        print(chunk['data'])\n",
                "    elif(chunk['event']==\"on_chat_model_start\"):\n",
                "\t    print(\"\\n\", chunk['event'], chunk['name'], chunk['tags'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "app.get_graph().print_ascii()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "app = create_tools_calling_executor(\n",
                "    llm_zhipu,\n",
                "    tools = tools,\n",
                "    runnables = {\"qa_chain\": {\"node\": chain, \"to\": \"agent\"}},\n",
                "    verbose = True)\n",
                "app.get_graph().print_ascii()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs = [\"请查询资料，告诉我langchain_chinese是什么？\"]\n",
                "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
                "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
                "    if(chunk['event']==\"on_chain_stream\" and chunk['name'] in [\"agent\", \"qa_chain\"]):\n",
                "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
                "    elif(chunk['name']==\"action\"):\n",
                "        print(chunk['data'])\n",
                "    elif(chunk['event']==\"on_chat_model_start\"):\n",
                "\t    print(\"\\n\", chunk['event'], chunk['name'], chunk['tags'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs = [HumanMessage(content=\"猫在哪里？\")]\n",
                "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
                "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
                "    if(chunk['event']==\"on_chain_stream\" and chunk['name'] in [\"agent\", \"qa_chain\"]):\n",
                "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
                "    elif(chunk['name']==\"action\"):\n",
                "        print(chunk['data'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs =  [HumanMessage(content=\"霍金的生日是哪一天？\")]\n",
                "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
                "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
                "    if(chunk['event']==\"on_chain_stream\" and chunk['name']==\"agent\"):\n",
                "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
                "    elif(chunk['name']==\"action\"):\n",
                "        print(chunk['data'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs =  [HumanMessage(content=\"写一个关于程序员的笑话\")]\n",
                "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
                "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
                "    if(chunk['event']==\"on_chain_stream\" and chunk['name']==\"agent\"):\n",
                "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
                "    elif(chunk['name']==\"action\"):\n",
                "        print(chunk['data'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "langchani-book-py3.10-ipykernel",
            "language": "python",
            "name": "langchani-book-py3.10-ipykernel"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}