{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "import os\n",
    "os.chdir('../..')\n",
    "from langchain_zhipu import ChatZhipuAI, ZhipuAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UserInputLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, Iterator, List, Optional, Union\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "def _make_tools_invocation(name_to_arguments: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    tool_calls = [\n",
    "        {\"function\": {\"name\": name, \"arguments\": json.dumps(arguments)}, \"id\": idx}\n",
    "        for idx, (name, arguments) in enumerate(name_to_arguments.items())\n",
    "    ]\n",
    "\n",
    "    return {\"tool_calls\": tool_calls}\n",
    "\n",
    "def get_input(prompt: str) -> str:\n",
    "    return input(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "@tool\n",
    "def confirm(topic: str) -> str:\n",
    "    \"\"\"ç¡®è®¤æ¥è‡ªLLMçš„Topicæˆ–Contentåˆ›ä½œç»“æœ\"\"\"\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "from typing import Any, Dict, Iterator, List, Optional, Union\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import HumanMessage, HumanMessageChunk, AIMessage, AIMessageChunk, BaseMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "\n",
    "class ChatWithUser(BaseChatModel):\n",
    "    \"\"\"å…è®¸ç”¨æˆ·å‚ä¸çš„å¤§æ¨¡å‹\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"chat-with-user\"\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        generations = [ChatGeneration(message=res) for res in self._ask(messages)]\n",
    "        return ChatResult(generations=generations)\n",
    "\n",
    "    def _ask(self, messages: List[BaseMessage]) -> List[BaseMessage]:\n",
    "        user_input = get_input(\"ğŸ‘¤: \")\n",
    "        if user_input == \"ok\":\n",
    "            tool_resp = _make_tools_invocation({\"confirm\": \"{'topic': 'my_topic'}\"})\n",
    "            response = AIMessage(content=\"\", additional_kwargs=tool_resp)\n",
    "        else:\n",
    "            response = AIMessage(user_input)            \n",
    "        return [response]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='hi', id='run-b4f0d6eb-04a9-4ad5-8bf8-0dfd50d0a0da-0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = ChatWithUser()\n",
    "u.invoke(\"hello?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph, MessageGraph, END\n",
    "import operator\n",
    "\n",
    "# å®šä¹‰ä¸šåŠ¡é€»è¾‘\n",
    "def nodeA(state):\n",
    "    return [\"AAA\"]\n",
    "\n",
    "from langchain.llms.fake import FakeStreamingListLLM\n",
    "llm = FakeStreamingListLLM(responses=[\"ä½ å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„å¤§è¯­è¨€æ¨¡å‹\"])\n",
    "\n",
    "# å®šä¹‰å›¾\n",
    "workflow = MessageGraph()\n",
    "\n",
    "workflow.add_node(\"a\", nodeA)\n",
    "workflow.add_node(\"b\", llm)\n",
    "\n",
    "workflow.add_edge(\"a\", \"b\")\n",
    "workflow.add_edge(\"b\", END)\n",
    "\n",
    "workflow.set_entry_point(\"a\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain_chinese import (\n",
    "    AskDocumentTool,\n",
    "    create_qa_chain,\n",
    "    create_reason_agent,\n",
    ")\n",
    "from langchain_chinese.langgraph import create_tools_calling_executor\n",
    "\n",
    "query_document = Document(page_content=\"langchain_chinese æ˜¯ä¸ºä¸­å›½å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–çš„langchainæ¨¡å—\")\n",
    "\n",
    "db = Chroma.from_documents([query_document], ZhipuAIEmbeddings())\n",
    "\n",
    "llm_zhipu = ChatZhipuAI(model=\"glm-4\")\n",
    "\n",
    "chain = create_qa_chain(llm_zhipu, db.as_retriever())\n",
    "qa_tool = AskDocumentTool(qa_chain=(chain | StrOutputParser()), name=\"qa_chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              +---------------------------------+          \n",
      "              | Parallel<context,question>Input |          \n",
      "              +---------------------------------+          \n",
      "                    ****               ****                \n",
      "                 ***                       ***             \n",
      "               **                             ***          \n",
      "    +-------------+                              **        \n",
      "    | Lambda(...) |                               *        \n",
      "    +-------------+                               *        \n",
      "            *                                     *        \n",
      "            *                                     *        \n",
      "            *                                     *        \n",
      "+----------------------+                          *        \n",
      "| VectorStoreRetriever |                          *        \n",
      "+----------------------+                          *        \n",
      "            *                                     *        \n",
      "            *                                     *        \n",
      "            *                                     *        \n",
      "+---------------------+                   +-------------+  \n",
      "| Lambda(format_docs) |                   | Lambda(...) |  \n",
      "+---------------------+                   +-------------+  \n",
      "                    ****               ****                \n",
      "                        ***         ***                    \n",
      "                           **     **                       \n",
      "             +----------------------------------+          \n",
      "             | Parallel<context,question>Output |          \n",
      "             +----------------------------------+          \n",
      "                               *                           \n",
      "                               *                           \n",
      "                               *                           \n",
      "                    +--------------------+                 \n",
      "                    | ChatPromptTemplate |                 \n",
      "                    +--------------------+                 \n",
      "                               *                           \n",
      "                               *                           \n",
      "                               *                           \n",
      "                        +-------------+                    \n",
      "                        | ChatZhipuAI |                    \n",
      "                        +-------------+                    \n",
      "                               *                           \n",
      "                               *                           \n",
      "                               *                           \n",
      "                    +-------------------+                  \n",
      "                    | ChatZhipuAIOutput |                  \n",
      "                    +-------------------+                  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='langchain_chinese æ˜¯ä¸“é—¨é’ˆå¯¹ä¸­å›½å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œä¼˜åŒ–çš„langchainæ¨¡å—ã€‚å®ƒå¯èƒ½æ˜¯ç”¨äºæå‡è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šåº”ç”¨åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œæ¯”å¦‚åœ¨ä¸­æ–‡è¯­å¢ƒä¸‹çš„ç†è§£ã€ç”Ÿæˆå’Œæ¨ç†èƒ½åŠ›ã€‚è¿™ä¸ªæ¨¡å—çš„è®¾è®¡ç›®çš„æ˜¯ä¸ºäº†æ›´å¥½åœ°é€‚åº”ä¸­æ–‡è¯­è¨€ç‰¹ç‚¹ï¼Œæé«˜è¯­è¨€æ¨¡å‹åœ¨ä¸­å›½å¸‚åœºçš„åº”ç”¨æ•ˆç‡å’Œè´¨é‡ã€‚')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke([\"langchain_chinese æ˜¯å•¥\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "import random\n",
    "\n",
    "# @tool(args_schema=WhereIsCatSchema)\n",
    "@tool(\"WhereIsCatHidding\")\n",
    "def where_is_cat_hiding(idea: str) -> str:\n",
    "    \"\"\"ä»è¿™äº›åœ°æ–¹é€‰æ‹©çŒ«èº²è—çš„åœ°æ–¹\"\"\"\n",
    "    return random.choice([\"åœ¨åºŠåº•ä¸‹\", \"åœ¨ä¹¦æ¶ä¸­\", \"åœ¨é˜³å°\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# llm_zhipu = ChatZhipuAI()\n",
    "tools = [qa_tool, where_is_cat_hiding]\n",
    "app = create_tools_calling_executor(llm_zhipu, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-chinese-gSQlHcwW-py3.9/lib/python3.9/site-packages/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " on_chat_model_start ChatZhipuAI ['seq:step:2']\n",
      "__runnable-name:  qa_chain\n",
      "log: call tool [qa_chain]\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk': [ToolMessage(content='langchain_chinese æ˜¯ä¸“é—¨é’ˆå¯¹ä¸­å›½å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œä¼˜åŒ–è®¾è®¡çš„langchainæ¨¡å—ã€‚è¿™ä¸ªæ¨¡å—å¯èƒ½æ˜¯ç”¨äºæå‡è¯­è¨€æ¨¡å‹åœ¨å„ç§åº”ç”¨åœºæ™¯ä¸‹çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œæ›´å¥½åœ°é€‚åº”ä¸­æ–‡è¯­è¨€ç¯å¢ƒçš„ç‰¹ç‚¹ã€‚', tool_call_id='call_8483741731860767639')]}\n",
      "{'input': ['è¯·æŸ¥è¯¢èµ„æ–™ï¼Œå‘Šè¯‰æˆ‘langchain_chineseæ˜¯ä»€ä¹ˆï¼Ÿ', AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_8483741731860767639', 'function': {'arguments': '{\"query\":\"langchain_chineseæ˜¯ä»€ä¹ˆ\"}', 'name': 'qa_chain'}, 'type': 'function'}]})], 'output': [ToolMessage(content='langchain_chinese æ˜¯ä¸“é—¨é’ˆå¯¹ä¸­å›½å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œä¼˜åŒ–è®¾è®¡çš„langchainæ¨¡å—ã€‚è¿™ä¸ªæ¨¡å—å¯èƒ½æ˜¯ç”¨äºæå‡è¯­è¨€æ¨¡å‹åœ¨å„ç§åº”ç”¨åœºæ™¯ä¸‹çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œæ›´å¥½åœ°é€‚åº”ä¸­æ–‡è¯­è¨€ç¯å¢ƒçš„ç‰¹ç‚¹ã€‚', tool_call_id='call_8483741731860767639')]}\n",
      "\n",
      " on_chat_model_start ChatZhipuAI ['seq:step:2']\n",
      "æ ¹æ®_æˆ‘çš„_æŸ¥è¯¢_ç»“æœ_ï¼Œ_lang_chain__ch_inese_ æ˜¯_ä¸“é—¨_é’ˆå¯¹_ä¸­å›½_å¤§_è¯­è¨€_æ¨¡å‹_è¿›è¡Œ_ä¼˜åŒ–_è®¾è®¡çš„_lang_chain_æ¨¡å—_ã€‚_è¿™ä¸ª_æ¨¡å—_å¯èƒ½æ˜¯_ç”¨äº_æå‡_è¯­è¨€_æ¨¡å‹_åœ¨å„ç§_åº”ç”¨_åœºæ™¯_ä¸‹çš„_æ€§èƒ½_å’Œ_æ•ˆç‡_ï¼Œ_æ›´å¥½åœ°_é€‚åº”_ä¸­æ–‡_è¯­è¨€_ç¯å¢ƒ_çš„ç‰¹ç‚¹_ã€‚__"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "app = create_tools_calling_executor(\n",
    "    llm_zhipu,\n",
    "    tools = tools,\n",
    "    verbose = True)\n",
    "\n",
    "inputs = [\"è¯·æŸ¥è¯¢èµ„æ–™ï¼Œå‘Šè¯‰æˆ‘langchain_chineseæ˜¯ä»€ä¹ˆï¼Ÿ\"]\n",
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name'] in [\"agent\", \"qa_chain\"]):\n",
    "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
    "    elif(chunk['name']==\"action\"):\n",
    "        print(chunk['data'])\n",
    "    elif(chunk['event']==\"on_chat_model_start\"):\n",
    "\t    print(\"\\n\", chunk['event'], chunk['name'], chunk['tags'])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    +-----------+             \n",
      "                    | __start__ |             \n",
      "                    +-----------+             \n",
      "                          *                   \n",
      "                          *                   \n",
      "                          *                   \n",
      "                      +-------+               \n",
      "                      | agent |               \n",
      "                     *+-------+*              \n",
      "                   **           ***           \n",
      "                 **                **         \n",
      "               **                    **       \n",
      "+-----------------------+              **     \n",
      "| agent_should_continue |               *     \n",
      "+-----------------------+               *     \n",
      "            *           *****           *     \n",
      "            *                ****       *     \n",
      "            *                    ***    *     \n",
      "       +---------+                +--------+  \n",
      "       | __end__ |                | action |  \n",
      "       +---------+                +--------+  \n"
     ]
    }
   ],
   "source": [
    "app.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " on_chat_model_start ChatZhipuAI ['seq:step:2']\n",
      "__runnable-name:  qa_chain\n",
      "log: call runnable [qa_chain]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " on_chat_model_start ChatZhipuAI ['seq:step:4']\n",
      "lang_chain__ch_inese_ æ˜¯_ä¸“é—¨_é’ˆå¯¹_ä¸­å›½_å¤§_è¯­è¨€_æ¨¡å‹_è¿›è¡Œ_ä¼˜_åŒ–çš„_ lang_chain_ æ¨¡_å—_ã€‚_å®ƒ_å¯èƒ½æ˜¯_ lang_chain_ åº“_çš„ä¸€ä¸ª_åˆ†æ”¯_æˆ–_æ‰©å±•_ï¼Œ_æ—¨åœ¨_æ›´å¥½åœ°_é€‚åº”_ä¸­æ–‡_è¯­å¢ƒ_å’Œ_ä¸­å›½çš„_ç›¸å…³_æŠ€æœ¯_éœ€æ±‚_ã€‚_å…·ä½“çš„_ç»†èŠ‚_éœ€è¦_æŸ¥é˜…_æ›´å¤š_å…³äº_ lang_chain__ch_inese_ çš„_å®˜æ–¹_æ–‡æ¡£_æˆ–_èµ„æ–™_æ‰èƒ½_äº†è§£_ã€‚__"
     ]
    }
   ],
   "source": [
    "app = create_tools_calling_executor(\n",
    "    llm_zhipu,\n",
    "    tools = tools,\n",
    "    runnables = {\"qa_chain\": chain},\n",
    "    verbose = True)\n",
    "\n",
    "inputs = [\"è¯·æŸ¥è¯¢èµ„æ–™ï¼Œå‘Šè¯‰æˆ‘langchain_chineseæ˜¯ä»€ä¹ˆï¼Ÿ\"]\n",
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name'] in [\"agent\", \"qa_chain\"]):\n",
    "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
    "    elif(chunk['name']==\"action\"):\n",
    "        print(chunk['data'])\n",
    "    elif(chunk['event']==\"on_chat_model_start\"):\n",
    "\t    print(\"\\n\", chunk['event'], chunk['name'], chunk['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        +-----------+                     \n",
      "                        | __start__ |                     \n",
      "                        +-----------+                     \n",
      "                              *                           \n",
      "                              *                           \n",
      "                              *                           \n",
      "                          +-------+                       \n",
      "                          | agent |*                      \n",
      "                         *+-------+ ****                  \n",
      "                       **               ***               \n",
      "                     **                    ***            \n",
      "                   **                         ****        \n",
      "    +-----------------------+                     **      \n",
      "    | agent_should_continue |                      *      \n",
      "    +-----------------------+**                    *      \n",
      "           **        **        *******             *      \n",
      "         **            **             *****        *      \n",
      "        *                **                ****    *      \n",
      "+----------+               *                  +--------+  \n",
      "| qa_chain |             **                   | action |  \n",
      "+----------+           **                     +--------+  \n",
      "           **        **                                   \n",
      "             **    **                                     \n",
      "               *  *                                       \n",
      "           +---------+                                    \n",
      "           | __end__ |                                    \n",
      "           +---------+                                    \n"
     ]
    }
   ],
   "source": [
    "app.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              +-----------+                          \n",
      "                              | __start__ |                          \n",
      "                              +-----------+                          \n",
      "                                     *                               \n",
      "                                     *                               \n",
      "                                     *                               \n",
      "                                +-------+                            \n",
      "                               *| agent |***                         \n",
      "                            *** +-------+   ***                      \n",
      "                       *****          *        *****                 \n",
      "                    ***                *            ***              \n",
      "                 ***                   *               *****         \n",
      "+-----------------------+               *                   **       \n",
      "| agent_should_continue |*             *                     *       \n",
      "+-----------------------+ ********     *                     *       \n",
      "               *      *****       *********                  *       \n",
      "               *           ***        *    ********          *       \n",
      "                *             ***    *             *****     *       \n",
      "           +---------+         +--------+              +----------+  \n",
      "           | __end__ |         | action |              | qa_chain |  \n",
      "           +---------+         +--------+              +----------+  \n"
     ]
    }
   ],
   "source": [
    "app = create_tools_calling_executor(\n",
    "    llm_zhipu,\n",
    "    tools = tools,\n",
    "    runnables = {\"qa_chain\": {\"node\": chain, \"to\": \"agent\"}},\n",
    "    verbose = True)\n",
    "app.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " on_chat_model_start ChatZhipuAI ['seq:step:2']\n",
      "__runnable-name:  qa_chain\n",
      "log: call runnable [qa_chain]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " on_chat_model_start ChatZhipuAI ['seq:step:4']\n",
      "lang_chain__ch_inese_ æ˜¯_ä¸“é—¨_é’ˆå¯¹_ä¸­å›½_å¤§_è¯­è¨€_æ¨¡å‹_è¿›è¡Œ_ä¼˜åŒ–_å’Œ_æ”¹è¿›_çš„_lang_chain_æ¨¡å—_ã€‚_å®ƒ_å¯èƒ½æ˜¯_lang_chain_è¿™ä¸€_æ¡†æ¶_çš„_æ‰©å±•_æˆ–_è¡ç”Ÿ_ç‰ˆæœ¬_ï¼Œ_æ—¨åœ¨_æ›´å¥½åœ°_é€‚åº”_ä¸­æ–‡_è¯­è¨€_ç¯å¢ƒ_ä¸‹çš„_éœ€æ±‚_ï¼Œ_æå‡_å¤§_è¯­è¨€_æ¨¡å‹_åœ¨_ä¸­æ–‡_å¤„ç†_æ–¹é¢çš„_æ€§èƒ½_å’Œ_æ•ˆæœ_ã€‚__\n",
      " on_chat_model_start ChatZhipuAI ['seq:step:2']\n",
      "æ ¹æ®_æˆ‘çš„_æŸ¥è¯¢_ç»“æœ_ï¼Œ_lang_chain__ch_inese_ æ˜¯_ä¸“é—¨_é’ˆå¯¹_ä¸­å›½_å¤§_è¯­è¨€_æ¨¡å‹_è¿›è¡Œ_ä¼˜åŒ–_å’Œ_æ”¹è¿›_çš„_ lang_chain_ æ¨¡_å—_ã€‚_å®ƒæ˜¯_ lang_chain_ è¿™ä¸€_æ¡†æ¶_çš„_æ‰©å±•_æˆ–_è¡ç”Ÿ_ç‰ˆæœ¬_ï¼Œ_æ—¨åœ¨_æ›´å¥½åœ°_é€‚åº”_ä¸­æ–‡_è¯­è¨€_ç¯å¢ƒ_ä¸‹çš„_éœ€æ±‚_ï¼Œ_æå‡_å¤§_è¯­è¨€_æ¨¡å‹_åœ¨_ä¸­æ–‡_å¤„ç†_æ–¹é¢çš„_æ€§èƒ½_å’Œ_æ•ˆæœ_ã€‚__"
     ]
    }
   ],
   "source": [
    "inputs = [\"è¯·æŸ¥è¯¢èµ„æ–™ï¼Œå‘Šè¯‰æˆ‘langchain_chineseæ˜¯ä»€ä¹ˆï¼Ÿ\"]\n",
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name'] in [\"agent\", \"qa_chain\"]):\n",
    "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
    "    elif(chunk['name']==\"action\"):\n",
    "        print(chunk['data'])\n",
    "    elif(chunk['event']==\"on_chat_model_start\"):\n",
    "\t    print(\"\\n\", chunk['event'], chunk['name'], chunk['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__runnable-name:  WhereIsCatHidding\n",
      "log: call tool [WhereIsCatHidding]\n",
      "{}\n",
      "{'chunk': [ToolMessage(content='åœ¨åºŠåº•ä¸‹', tool_call_id='call_8483752349020160968')]}\n",
      "{'input': [HumanMessage(content='çŒ«åœ¨å“ªé‡Œï¼Ÿ'), AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_8483752349020160968', 'function': {'arguments': '{\"idea\":\"çŒ«åœ¨å“ªé‡Œ\"}', 'name': 'WhereIsCatHidding'}, 'type': 'function'}]})], 'output': [ToolMessage(content='åœ¨åºŠåº•ä¸‹', tool_call_id='call_8483752349020160968')]}\n",
      "æ ¹æ®_æˆ‘çš„_è§‚å¯Ÿ_å’Œ_ç»éªŒ_ï¼Œ_çŒ«_é€šå¸¸ä¼š_èº²_åœ¨ä¸€äº›_ç‹­_å°_çš„åœ°æ–¹_ï¼Œ_æ¯”å¦‚_åºŠ_åº•ä¸‹_ã€_æ²™å‘_åé¢_æˆ–è€…_æŸœ_å­é‡Œ_ã€‚_æ‰€ä»¥_ï¼Œ_å¦‚æœä½ æƒ³_æ‰¾åˆ°_ä¸€åª_çŒ«_ï¼Œ_æœ€å¥½_åœ¨è¿™äº›_åœ°æ–¹_ä»”ç»†_å¯»æ‰¾_ã€‚__"
     ]
    }
   ],
   "source": [
    "inputs = [HumanMessage(content=\"çŒ«åœ¨å“ªé‡Œï¼Ÿ\")]\n",
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name'] in [\"agent\", \"qa_chain\"]):\n",
    "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
    "    elif(chunk['name']==\"action\"):\n",
    "        print(chunk['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__runnable-name:  qa_chain\n",
      "log: call runnable [qa_chain]\n"
     ]
    }
   ],
   "source": [
    "inputs =  [HumanMessage(content=\"éœé‡‘çš„ç”Ÿæ—¥æ˜¯å“ªä¸€å¤©ï¼Ÿ\")]\n",
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name']==\"agent\"):\n",
    "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
    "    elif(chunk['name']==\"action\"):\n",
    "        print(chunk['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¸ºä»€ä¹ˆ_ç¨‹åºå‘˜_æ€»æ˜¯_æºå¸¦_ç”µè„‘_ï¼Ÿ_å› ä¸ºä»–ä»¬_ä¸æƒ³_è¢«äºº_ç§°ä¸º_â€œ_è£¸_å¥”_è€…_â€ã€‚__"
     ]
    }
   ],
   "source": [
    "inputs =  [HumanMessage(content=\"å†™ä¸€ä¸ªå…³äºç¨‹åºå‘˜çš„ç¬‘è¯\")]\n",
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name']==\"agent\"):\n",
    "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
    "    elif(chunk['name']==\"action\"):\n",
    "        print(chunk['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-chinese-py3.9-ipkyernel",
   "language": "python",
   "name": "langchain-chinese-py3.9-ipkyernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
