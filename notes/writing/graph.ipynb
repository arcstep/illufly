{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "import os\n",
    "os.chdir('../..')\n",
    "from langchain_zhipu import ChatZhipuAI, ZhipuAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UserInputLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, Iterator, List, Optional, Union\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "def _make_tools_invocation(name_to_arguments: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    tool_calls = [\n",
    "        {\"function\": {\"name\": name, \"arguments\": json.dumps(arguments)}, \"id\": idx}\n",
    "        for idx, (name, arguments) in enumerate(name_to_arguments.items())\n",
    "    ]\n",
    "\n",
    "    return {\"tool_calls\": tool_calls}\n",
    "\n",
    "def get_input(prompt: str) -> str:\n",
    "    return input(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "@tool\n",
    "def confirm(topic: str) -> str:\n",
    "    \"\"\"Á°ÆËÆ§Êù•Ëá™LLMÁöÑTopicÊàñContentÂàõ‰ΩúÁªìÊûú\"\"\"\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "from typing import Any, Dict, Iterator, List, Optional, Union\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import HumanMessage, HumanMessageChunk, AIMessage, AIMessageChunk, BaseMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "\n",
    "class ChatWithUser(BaseChatModel):\n",
    "    \"\"\"ÂÖÅËÆ∏Áî®Êà∑ÂèÇ‰∏éÁöÑÂ§ßÊ®°Âûã\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"chat-with-user\"\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        generations = [ChatGeneration(message=res) for res in self._ask(messages)]\n",
    "        return ChatResult(generations=generations)\n",
    "\n",
    "    def _ask(self, messages: List[BaseMessage]) -> List[BaseMessage]:\n",
    "        user_input = get_input(\"üë§: \")\n",
    "        if user_input == \"ok\":\n",
    "            tool_resp = _make_tools_invocation({\"confirm\": \"{'topic': 'my_topic'}\"})\n",
    "            response = AIMessage(content=\"\", additional_kwargs=tool_resp)\n",
    "        else:\n",
    "            response = AIMessage(user_input)            \n",
    "        return [response]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='hi', id='run-b4f0d6eb-04a9-4ad5-8bf8-0dfd50d0a0da-0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = ChatWithUser()\n",
    "u.invoke(\"hello?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph, MessageGraph, END\n",
    "import operator\n",
    "\n",
    "# ÂÆö‰πâ‰∏öÂä°ÈÄªËæë\n",
    "def nodeA(state):\n",
    "    return [\"AAA\"]\n",
    "\n",
    "from langchain.llms.fake import FakeStreamingListLLM\n",
    "llm = FakeStreamingListLLM(responses=[\"‰Ω†Â•ΩÔºåÊàëÊòØ‰∏Ä‰∏™Ê®°ÊãüÁöÑÂ§ßËØ≠Ë®ÄÊ®°Âûã\"])\n",
    "\n",
    "# ÂÆö‰πâÂõæ\n",
    "workflow = MessageGraph()\n",
    "\n",
    "workflow.add_node(\"a\", nodeA)\n",
    "workflow.add_node(\"b\", llm)\n",
    "\n",
    "workflow.add_edge(\"a\", \"b\")\n",
    "workflow.add_edge(\"b\", END)\n",
    "\n",
    "workflow.set_entry_point(\"a\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain_chinese import (\n",
    "    AskDocumentTool,\n",
    "    create_qa_chain,\n",
    "    create_reason_agent,\n",
    ")\n",
    "from langchain_chinese.langgraph import create_tools_calling_executor\n",
    "\n",
    "query_document = Document(page_content=\"langchain_chinese ÊòØ‰∏∫‰∏≠ÂõΩÂ§ßËØ≠Ë®ÄÊ®°Âûã‰ºòÂåñÁöÑlangchainÊ®°Âùó\")\n",
    "\n",
    "db = Chroma.from_documents([query_document], ZhipuAIEmbeddings())\n",
    "\n",
    "llm_zhipu = ChatZhipuAI(model=\"glm-4\")\n",
    "\n",
    "chain = create_qa_chain(llm_zhipu, db.as_retriever())\n",
    "qa_tool = AskDocumentTool(qa_chain=(chain | StrOutputParser()), name=\"qa_chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              +---------------------------------+          \n",
      "              | Parallel<context,question>Input |          \n",
      "              +---------------------------------+          \n",
      "                    ****               ****                \n",
      "                 ***                       ***             \n",
      "               **                             ***          \n",
      "    +-------------+                              **        \n",
      "    | Lambda(...) |                               *        \n",
      "    +-------------+                               *        \n",
      "            *                                     *        \n",
      "            *                                     *        \n",
      "            *                                     *        \n",
      "+----------------------+                          *        \n",
      "| VectorStoreRetriever |                          *        \n",
      "+----------------------+                          *        \n",
      "            *                                     *        \n",
      "            *                                     *        \n",
      "            *                                     *        \n",
      "+---------------------+                   +-------------+  \n",
      "| Lambda(format_docs) |                   | Lambda(...) |  \n",
      "+---------------------+                   +-------------+  \n",
      "                    ****               ****                \n",
      "                        ***         ***                    \n",
      "                           **     **                       \n",
      "             +----------------------------------+          \n",
      "             | Parallel<context,question>Output |          \n",
      "             +----------------------------------+          \n",
      "                               *                           \n",
      "                               *                           \n",
      "                               *                           \n",
      "                    +--------------------+                 \n",
      "                    | ChatPromptTemplate |                 \n",
      "                    +--------------------+                 \n",
      "                               *                           \n",
      "                               *                           \n",
      "                               *                           \n",
      "                        +-------------+                    \n",
      "                        | ChatZhipuAI |                    \n",
      "                        +-------------+                    \n",
      "                               *                           \n",
      "                               *                           \n",
      "                               *                           \n",
      "                    +-------------------+                  \n",
      "                    | ChatZhipuAIOutput |                  \n",
      "                    +-------------------+                  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='langchain_chinese ÊòØ‰∏ìÈó®ÈíàÂØπ‰∏≠ÂõΩÂ§ßËØ≠Ë®ÄÊ®°ÂûãËøõË°å‰ºòÂåñÁöÑlangchainÊ®°Âùó„ÄÇÂÆÉÂèØËÉΩÊòØÁî®‰∫éÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÂú®ÁâπÂÆöÂ∫îÁî®Âú∫ÊôØ‰∏ãÁöÑË°®Áé∞ÔºåÊØîÂ¶ÇÂú®‰∏≠ÊñáËØ≠Â¢É‰∏ãÁöÑÁêÜËß£„ÄÅÁîüÊàêÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇËøô‰∏™Ê®°ÂùóÁöÑËÆæËÆ°ÁõÆÁöÑÊòØ‰∏∫‰∫ÜÊõ¥Â•ΩÂú∞ÈÄÇÂ∫î‰∏≠ÊñáËØ≠Ë®ÄÁâπÁÇπÔºåÊèêÈ´òËØ≠Ë®ÄÊ®°ÂûãÂú®‰∏≠ÂõΩÂ∏ÇÂú∫ÁöÑÂ∫îÁî®ÊïàÁéáÂíåË¥®Èáè„ÄÇ')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke([\"langchain_chinese ÊòØÂï•\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "import random\n",
    "\n",
    "# @tool(args_schema=WhereIsCatSchema)\n",
    "@tool(\"WhereIsCatHidding\")\n",
    "def where_is_cat_hiding(idea: str) -> str:\n",
    "    \"\"\"‰ªéËøô‰∫õÂú∞ÊñπÈÄâÊã©Áå´Ë∫≤ËóèÁöÑÂú∞Êñπ\"\"\"\n",
    "    return random.choice([\"Âú®Â∫äÂ∫ï‰∏ã\", \"Âú®‰π¶Êû∂‰∏≠\", \"Âú®Èò≥Âè∞\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# llm_zhipu = ChatZhipuAI()\n",
    "tools = [qa_tool, where_is_cat_hiding]\n",
    "app = create_tools_calling_executor(llm_zhipu, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuehongwei/Library/Caches/pypoetry/virtualenvs/langchain-chinese-gSQlHcwW-py3.9/lib/python3.9/site-packages/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " on_chat_model_start ChatZhipuAI ['seq:step:2']\n",
      "__runnable-name:  qa_chain\n",
      "log: call tool [qa_chain]\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk': [ToolMessage(content='langchain_chinese ÊòØ‰∏ìÈó®ÈíàÂØπ‰∏≠ÂõΩÂ§ßËØ≠Ë®ÄÊ®°ÂûãËøõË°å‰ºòÂåñËÆæËÆ°ÁöÑlangchainÊ®°Âùó„ÄÇËøô‰∏™Ê®°ÂùóÂèØËÉΩÊòØÁî®‰∫éÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÂú®ÂêÑÁßçÂ∫îÁî®Âú∫ÊôØ‰∏ãÁöÑÊÄßËÉΩÂíåÊïàÁéáÔºåÊõ¥Â•ΩÂú∞ÈÄÇÂ∫î‰∏≠ÊñáËØ≠Ë®ÄÁéØÂ¢ÉÁöÑÁâπÁÇπ„ÄÇ', tool_call_id='call_8483741731860767639')]}\n",
      "{'input': ['ËØ∑Êü•ËØ¢ËµÑÊñôÔºåÂëäËØâÊàëlangchain_chineseÊòØ‰ªÄ‰πàÔºü', AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_8483741731860767639', 'function': {'arguments': '{\"query\":\"langchain_chineseÊòØ‰ªÄ‰πà\"}', 'name': 'qa_chain'}, 'type': 'function'}]})], 'output': [ToolMessage(content='langchain_chinese ÊòØ‰∏ìÈó®ÈíàÂØπ‰∏≠ÂõΩÂ§ßËØ≠Ë®ÄÊ®°ÂûãËøõË°å‰ºòÂåñËÆæËÆ°ÁöÑlangchainÊ®°Âùó„ÄÇËøô‰∏™Ê®°ÂùóÂèØËÉΩÊòØÁî®‰∫éÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÂú®ÂêÑÁßçÂ∫îÁî®Âú∫ÊôØ‰∏ãÁöÑÊÄßËÉΩÂíåÊïàÁéáÔºåÊõ¥Â•ΩÂú∞ÈÄÇÂ∫î‰∏≠ÊñáËØ≠Ë®ÄÁéØÂ¢ÉÁöÑÁâπÁÇπ„ÄÇ', tool_call_id='call_8483741731860767639')]}\n",
      "\n",
      " on_chat_model_start ChatZhipuAI ['seq:step:2']\n",
      "Ê†πÊçÆ_ÊàëÁöÑ_Êü•ËØ¢_ÁªìÊûú_Ôºå_lang_chain__ch_inese_ ÊòØ_‰∏ìÈó®_ÈíàÂØπ_‰∏≠ÂõΩ_Â§ß_ËØ≠Ë®Ä_Ê®°Âûã_ËøõË°å_‰ºòÂåñ_ËÆæËÆ°ÁöÑ_lang_chain_Ê®°Âùó_„ÄÇ_Ëøô‰∏™_Ê®°Âùó_ÂèØËÉΩÊòØ_Áî®‰∫é_ÊèêÂçá_ËØ≠Ë®Ä_Ê®°Âûã_Âú®ÂêÑÁßç_Â∫îÁî®_Âú∫ÊôØ_‰∏ãÁöÑ_ÊÄßËÉΩ_Âíå_ÊïàÁéá_Ôºå_Êõ¥Â•ΩÂú∞_ÈÄÇÂ∫î_‰∏≠Êñá_ËØ≠Ë®Ä_ÁéØÂ¢É_ÁöÑÁâπÁÇπ_„ÄÇ__"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "app = create_tools_calling_executor(\n",
    "    llm_zhipu,\n",
    "    tools = tools,\n",
    "    verbose = True)\n",
    "\n",
    "inputs = [\"ËØ∑Êü•ËØ¢ËµÑÊñôÔºåÂëäËØâÊàëlangchain_chineseÊòØ‰ªÄ‰πàÔºü\"]\n",
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name'] in [\"agent\", \"qa_chain\"]):\n",
    "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
    "    elif(chunk['name']==\"action\"):\n",
    "        print(chunk['data'])\n",
    "    elif(chunk['event']==\"on_chat_model_start\"):\n",
    "\t    print(\"\\n\", chunk['event'], chunk['name'], chunk['tags'])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    +-----------+             \n",
      "                    | __start__ |             \n",
      "                    +-----------+             \n",
      "                          *                   \n",
      "                          *                   \n",
      "                          *                   \n",
      "                      +-------+               \n",
      "                      | agent |               \n",
      "                     *+-------+*              \n",
      "                   **           ***           \n",
      "                 **                **         \n",
      "               **                    **       \n",
      "+-----------------------+              **     \n",
      "| agent_should_continue |               *     \n",
      "+-----------------------+               *     \n",
      "            *           *****           *     \n",
      "            *                ****       *     \n",
      "            *                    ***    *     \n",
      "       +---------+                +--------+  \n",
      "       | __end__ |                | action |  \n",
      "       +---------+                +--------+  \n"
     ]
    }
   ],
   "source": [
    "app.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " on_chat_model_start ChatZhipuAI ['seq:step:2']\n",
      "__runnable-name:  qa_chain\n",
      "log: call runnable [qa_chain]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " on_chat_model_start ChatZhipuAI ['seq:step:4']\n",
      "lang_chain__ch_inese_ ÊòØ_‰∏ìÈó®_ÈíàÂØπ_‰∏≠ÂõΩ_Â§ß_ËØ≠Ë®Ä_Ê®°Âûã_ËøõË°å_‰ºò_ÂåñÁöÑ_ lang_chain_ Ê®°_Âùó_„ÄÇ_ÂÆÉ_ÂèØËÉΩÊòØ_ lang_chain_ Â∫ì_ÁöÑ‰∏Ä‰∏™_ÂàÜÊîØ_Êàñ_Êâ©Â±ï_Ôºå_Êó®Âú®_Êõ¥Â•ΩÂú∞_ÈÄÇÂ∫î_‰∏≠Êñá_ËØ≠Â¢É_Âíå_‰∏≠ÂõΩÁöÑ_Áõ∏ÂÖ≥_ÊäÄÊúØ_ÈúÄÊ±Ç_„ÄÇ_ÂÖ∑‰ΩìÁöÑ_ÁªÜËäÇ_ÈúÄË¶Å_Êü•ÈòÖ_Êõ¥Â§ö_ÂÖ≥‰∫é_ lang_chain__ch_inese_ ÁöÑ_ÂÆòÊñπ_ÊñáÊ°£_Êàñ_ËµÑÊñô_ÊâçËÉΩ_‰∫ÜËß£_„ÄÇ__"
     ]
    }
   ],
   "source": [
    "app = create_tools_calling_executor(\n",
    "    llm_zhipu,\n",
    "    tools = tools,\n",
    "    runnables = {\"qa_chain\": chain},\n",
    "    verbose = True)\n",
    "\n",
    "inputs = [\"ËØ∑Êü•ËØ¢ËµÑÊñôÔºåÂëäËØâÊàëlangchain_chineseÊòØ‰ªÄ‰πàÔºü\"]\n",
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name'] in [\"agent\", \"qa_chain\"]):\n",
    "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
    "    elif(chunk['name']==\"action\"):\n",
    "        print(chunk['data'])\n",
    "    elif(chunk['event']==\"on_chat_model_start\"):\n",
    "\t    print(\"\\n\", chunk['event'], chunk['name'], chunk['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        +-----------+                     \n",
      "                        | __start__ |                     \n",
      "                        +-----------+                     \n",
      "                              *                           \n",
      "                              *                           \n",
      "                              *                           \n",
      "                          +-------+                       \n",
      "                          | agent |*                      \n",
      "                         *+-------+ ****                  \n",
      "                       **               ***               \n",
      "                     **                    ***            \n",
      "                   **                         ****        \n",
      "    +-----------------------+                     **      \n",
      "    | agent_should_continue |                      *      \n",
      "    +-----------------------+**                    *      \n",
      "           **        **        *******             *      \n",
      "         **            **             *****        *      \n",
      "        *                **                ****    *      \n",
      "+----------+               *                  +--------+  \n",
      "| qa_chain |             **                   | action |  \n",
      "+----------+           **                     +--------+  \n",
      "           **        **                                   \n",
      "             **    **                                     \n",
      "               *  *                                       \n",
      "           +---------+                                    \n",
      "           | __end__ |                                    \n",
      "           +---------+                                    \n"
     ]
    }
   ],
   "source": [
    "app.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              +-----------+                          \n",
      "                              | __start__ |                          \n",
      "                              +-----------+                          \n",
      "                                     *                               \n",
      "                                     *                               \n",
      "                                     *                               \n",
      "                                +-------+                            \n",
      "                               *| agent |***                         \n",
      "                            *** +-------+   ***                      \n",
      "                       *****          *        *****                 \n",
      "                    ***                *            ***              \n",
      "                 ***                   *               *****         \n",
      "+-----------------------+               *                   **       \n",
      "| agent_should_continue |*             *                     *       \n",
      "+-----------------------+ ********     *                     *       \n",
      "               *      *****       *********                  *       \n",
      "               *           ***        *    ********          *       \n",
      "                *             ***    *             *****     *       \n",
      "           +---------+         +--------+              +----------+  \n",
      "           | __end__ |         | action |              | qa_chain |  \n",
      "           +---------+         +--------+              +----------+  \n"
     ]
    }
   ],
   "source": [
    "app = create_tools_calling_executor(\n",
    "    llm_zhipu,\n",
    "    tools = tools,\n",
    "    runnables = {\"qa_chain\": {\"node\": chain, \"to\": \"agent\"}},\n",
    "    verbose = True)\n",
    "app.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " on_chat_model_start ChatZhipuAI ['seq:step:2']\n",
      "__runnable-name:  qa_chain\n",
      "log: call runnable [qa_chain]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " on_chat_model_start ChatZhipuAI ['seq:step:4']\n",
      "lang_chain__ch_inese_ ÊòØ_‰∏ìÈó®_ÈíàÂØπ_‰∏≠ÂõΩ_Â§ß_ËØ≠Ë®Ä_Ê®°Âûã_ËøõË°å_‰ºòÂåñ_Âíå_ÊîπËøõ_ÁöÑ_lang_chain_Ê®°Âùó_„ÄÇ_ÂÆÉ_ÂèØËÉΩÊòØ_lang_chain_Ëøô‰∏Ä_Ê°ÜÊû∂_ÁöÑ_Êâ©Â±ï_Êàñ_Ë°çÁîü_ÁâàÊú¨_Ôºå_Êó®Âú®_Êõ¥Â•ΩÂú∞_ÈÄÇÂ∫î_‰∏≠Êñá_ËØ≠Ë®Ä_ÁéØÂ¢É_‰∏ãÁöÑ_ÈúÄÊ±Ç_Ôºå_ÊèêÂçá_Â§ß_ËØ≠Ë®Ä_Ê®°Âûã_Âú®_‰∏≠Êñá_Â§ÑÁêÜ_ÊñπÈù¢ÁöÑ_ÊÄßËÉΩ_Âíå_ÊïàÊûú_„ÄÇ__\n",
      " on_chat_model_start ChatZhipuAI ['seq:step:2']\n",
      "Ê†πÊçÆ_ÊàëÁöÑ_Êü•ËØ¢_ÁªìÊûú_Ôºå_lang_chain__ch_inese_ ÊòØ_‰∏ìÈó®_ÈíàÂØπ_‰∏≠ÂõΩ_Â§ß_ËØ≠Ë®Ä_Ê®°Âûã_ËøõË°å_‰ºòÂåñ_Âíå_ÊîπËøõ_ÁöÑ_ lang_chain_ Ê®°_Âùó_„ÄÇ_ÂÆÉÊòØ_ lang_chain_ Ëøô‰∏Ä_Ê°ÜÊû∂_ÁöÑ_Êâ©Â±ï_Êàñ_Ë°çÁîü_ÁâàÊú¨_Ôºå_Êó®Âú®_Êõ¥Â•ΩÂú∞_ÈÄÇÂ∫î_‰∏≠Êñá_ËØ≠Ë®Ä_ÁéØÂ¢É_‰∏ãÁöÑ_ÈúÄÊ±Ç_Ôºå_ÊèêÂçá_Â§ß_ËØ≠Ë®Ä_Ê®°Âûã_Âú®_‰∏≠Êñá_Â§ÑÁêÜ_ÊñπÈù¢ÁöÑ_ÊÄßËÉΩ_Âíå_ÊïàÊûú_„ÄÇ__"
     ]
    }
   ],
   "source": [
    "inputs = [\"ËØ∑Êü•ËØ¢ËµÑÊñôÔºåÂëäËØâÊàëlangchain_chineseÊòØ‰ªÄ‰πàÔºü\"]\n",
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name'] in [\"agent\", \"qa_chain\"]):\n",
    "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
    "    elif(chunk['name']==\"action\"):\n",
    "        print(chunk['data'])\n",
    "    elif(chunk['event']==\"on_chat_model_start\"):\n",
    "\t    print(\"\\n\", chunk['event'], chunk['name'], chunk['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__runnable-name:  WhereIsCatHidding\n",
      "log: call tool [WhereIsCatHidding]\n",
      "{}\n",
      "{'chunk': [ToolMessage(content='Âú®Â∫äÂ∫ï‰∏ã', tool_call_id='call_8483752349020160968')]}\n",
      "{'input': [HumanMessage(content='Áå´Âú®Âì™ÈáåÔºü'), AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_8483752349020160968', 'function': {'arguments': '{\"idea\":\"Áå´Âú®Âì™Èáå\"}', 'name': 'WhereIsCatHidding'}, 'type': 'function'}]})], 'output': [ToolMessage(content='Âú®Â∫äÂ∫ï‰∏ã', tool_call_id='call_8483752349020160968')]}\n",
      "Ê†πÊçÆ_ÊàëÁöÑ_ËßÇÂØü_Âíå_ÁªèÈ™å_Ôºå_Áå´_ÈÄöÂ∏∏‰ºö_Ë∫≤_Âú®‰∏Ä‰∫õ_Áã≠_Â∞è_ÁöÑÂú∞Êñπ_Ôºå_ÊØîÂ¶Ç_Â∫ä_Â∫ï‰∏ã_„ÄÅ_Ê≤ôÂèë_ÂêéÈù¢_ÊàñËÄÖ_Êüú_Â≠êÈáå_„ÄÇ_ÊâÄ‰ª•_Ôºå_Â¶ÇÊûú‰Ω†ÊÉ≥_ÊâæÂà∞_‰∏ÄÂè™_Áå´_Ôºå_ÊúÄÂ•Ω_Âú®Ëøô‰∫õ_Âú∞Êñπ_‰ªîÁªÜ_ÂØªÊâæ_„ÄÇ__"
     ]
    }
   ],
   "source": [
    "inputs = [HumanMessage(content=\"Áå´Âú®Âì™ÈáåÔºü\")]\n",
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name'] in [\"agent\", \"qa_chain\"]):\n",
    "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
    "    elif(chunk['name']==\"action\"):\n",
    "        print(chunk['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__runnable-name:  qa_chain\n",
      "log: call runnable [qa_chain]\n"
     ]
    }
   ],
   "source": [
    "inputs =  [HumanMessage(content=\"ÈúçÈáëÁöÑÁîüÊó•ÊòØÂì™‰∏ÄÂ§©Ôºü\")]\n",
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name']==\"agent\"):\n",
    "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
    "    elif(chunk['name']==\"action\"):\n",
    "        print(chunk['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‰∏∫‰ªÄ‰πà_Á®ãÂ∫èÂëò_ÊÄªÊòØ_Êê∫Â∏¶_ÁîµËÑë_Ôºü_Âõ†‰∏∫‰ªñ‰ª¨_‰∏çÊÉ≥_Ë¢´‰∫∫_Áß∞‰∏∫_‚Äú_Ë£∏_Â•î_ËÄÖ_‚Äù„ÄÇ__"
     ]
    }
   ],
   "source": [
    "inputs =  [HumanMessage(content=\"ÂÜô‰∏Ä‰∏™ÂÖ≥‰∫éÁ®ãÂ∫èÂëòÁöÑÁ¨ëËØù\")]\n",
    "async for chunk in app.astream_events(inputs, version=\"v1\"):\n",
    "    # print(\" \"*10, chunk['event'], chunk['name'], chunk['tags'])\n",
    "    if(chunk['event']==\"on_chain_stream\" and chunk['name']==\"agent\"):\n",
    "        print(chunk['data']['chunk'].content, end=\"_\", flush=True)\n",
    "    elif(chunk['name']==\"action\"):\n",
    "        print(chunk['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-chinese-py3.9-ipkyernel",
   "language": "python",
   "name": "langchain-chinese-py3.9-ipkyernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
